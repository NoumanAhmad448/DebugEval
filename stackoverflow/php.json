[
    {
        "instruction": "How to split a binary file into different frames",
        "input": "",
        "output": "<p>OK, so we know a few things given your comments:</p>\n\n<ol>\n<li>Each frame is 424 x 512</li>\n<li>The precision of each element in your video is <code>uint8</code></li>\n<li>This is a grayscale video, so colour is not going to be considered</li>\n<li>I'm going to assume your data is read in row major format.  Bear in mind that MATLAB reads in data in column major format if you want to read in the data as a matrix, so you'll have to do a transpose operation, and ensure that you read the data into a transposed matrix first.  </li>\n</ol>\n\n<p>What I'm going to do is put each frame into a <a href=\"http://www.mathworks.com/help/matlab/ref/cell.html\" rel=\"nofollow\"><code>cell</code></a> entry inside a cell array.  The reason why I'm doing this is because we don't know how many frames are really in your sequence.  You said \"70 or so\" frames, and since we don't know the exact number, we will dynamically populate the cell array as we go.</p>\n\n<p>The code you have written will predominantly be the same, but we're going to run a <code>while</code> loop until what we read in from file is empty.  In the <code>while</code> loop, we will read one frame at a time and store it into the cell array.  I'll also make a counter that counts the number of frames we have to for later, and you'll see why. As such:</p>\n\n<pre><code>%// Open the file\nfid = fopen('C:\\KinectData\\Depth\\Depth_Raw_0.binary');\ncol = 424; %// Change if the dimensions are not proper\nrow = 512;\n\nframes = {}; %// Empty cell array - Put frames in here\nnumFrames = 0; %// Let's record the number of frames too\nwhile (true) %// Until we reach the end of the file:\n    B = fread(fin, [col row],'uint8=&gt;uint8'); %// Read in one frame at a time\n\n    if (isempty(B)) %// If there are no more frames, get out\n        break;\n    end\n\n    frames{end+1} = B.'; %// Transpose to make row major and place in cell array\n    numFrames = numFrames + 1; %// Count frame\nend\n\n%// Close the file    \nfclose(fid);\n</code></pre>\n\n<p>As such, if you want to access the i<sup>th</sup> frame, you would simply do:</p>\n\n<pre><code>frm = frames{i};\n</code></pre>\n\n<p>As an additional bonus, you can play this as a <a href=\"http://www.mathworks.com/help/matlab/ref/movie.html\" rel=\"nofollow\"><code>movie</code></a> in MATLAB.  What we can do is preallocate a movie structure that is compatible to play the movie, then when you run the <code>movie</code> command, this will play the frames for you.  Let's specify a frame rate of 10 fps, so that it will play slow enough to see the results.  Therefore, when you're done do something like this:</p>\n\n<pre><code>%// Preallocate the movie structure array \nmovieFrames(numFrames) = struct('cdata',[],'colormap',[]);\n\nfor idx = 1 : numFrames\n    img = frames{idx};\n    movieFrames(idx) = im2frame(cat(3, img, img, img));\nend\nfigure;\nimshow(movieFrames(1).cdata); %// Show first frame to establish figure size\nmovie(movieFrames, 1, 10); %// Play the movie once at 10 FPS\n</code></pre>\n\n<p>Note that movies can only be played in colour, and so you would have to make your grayscale image artificially into colour.  A colour frame is a 3D matrix, where each 2D slice tells you the amount of red, green or blue.  For grayscale, each of these slices are the same, and so you would just replicate each grayscale frame for each slice.  I used <a href=\"http://www.mathworks.com/help/matlab/ref/cat.html\" rel=\"nofollow\"><code>cat</code></a> to make the grayscale frame into a 3D matrix.  I also use <a href=\"http://www.mathworks.com/help/matlab/ref/im2frame.html\" rel=\"nofollow\"><code>im2frame</code></a> to convert each image into a movie frame structure compatible to be played with <code>movie</code>.</p>\n\n<h1>Small caveat</h1>\n\n<p>I don't have access to your binary file, so this code is untested.  Use at your own risk!</p>\n",
        "system": ""
    },
    {
        "instruction": "Processing Kinect Depth data in MATLAB",
        "input": "",
        "output": "<p>looks like your are reading an indexed image with <code>imread()</code></p>\n\n<p>try this:</p>\n\n<pre><code>[idx, map] = imread('yourImage');\nRGB = ind2rgb(idx, map);\n</code></pre>\n\n<p>and see if RGB contains the correct value.</p>\n",
        "system": ""
    },
    {
        "instruction": "Image rectification algorithm in Matlab",
        "input": "",
        "output": "<p>As <a href=\"https://stackoverflow.com/questions/25745809/image-rectification-algorithm-in-matlab#comment40256852_25745809\">Shai says</a>, <code>T1</code> and <code>T2</code> are projective transformation matrices, not filter kernels. You should be using <code>imwarp</code>, rather than <code>conv2</code>:</p>\n\n<pre><code>imnoua = imwarp(im1, projective2d(T1));\nimnoua2 = imwarp(im2, projective2d(T2));\n</code></pre>\n\n<p>Better yet, use <code>rectifyStereoImages</code> from the Computer Vision System Toolbox.  Check out this <a href=\"http://www.mathworks.com/help/vision/ug/stereo-vision.html\" rel=\"nofollow noreferrer\">example</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Matching photographed image with screenshot (or generated image based on data model)",
        "input": "",
        "output": "<p>Your initial idea of using SURF features was actually very good, just try to understand how the parameters for this algorithm work and you should be able to register your images. A good starting point for your parameters would be varying only the Hessian treshold, and being fearles while doing so: your features are quite well defined, so try to use tresholds around 2000 and above (increasing in steps of 500-1000 till you get good results is totally ok).</p>\n\n<p>Alternatively you can try to detect your ellipses and calculate an affine warp that normalizes them and run a cross-correlation to register them. This alternative does imply much more work, but is quite fascinating. Some ideas on that normalization using the covariance matrix and its choletsky decomposition <a href=\"http://cmp.felk.cvut.cz/~xobdrzal/publications/matas02icpr.pdf\" rel=\"nofollow\">here</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Augmented Reality - Rendering 3D objects using projection matrix",
        "input": "",
        "output": "<p>Define \"easiest\"? In opengl you can just set you projection matrix as the modelview, and start drawing. Caveats, the usual ones: make sure your viewport / clipping planes make sense.</p>\n",
        "system": ""
    },
    {
        "instruction": "suggestion on implementation of visual odometry using monocular camera",
        "input": "",
        "output": "<p>Visual-inertial fusion is the most recent approach used for monocular odometry. You need to calculate the scale between world units and meters.</p>\n\n<p>You can use an Extended Kalman Filter approach or optimization as you like. Try with this paper: <a href=\"http://www.ee.ucr.edu/~mourikis/papers/Li2012-ICRA.pdf\" rel=\"nofollow\">http://www.ee.ucr.edu/~mourikis/papers/Li2012-ICRA.pdf</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Python OpenCV detect circles from black and white mask",
        "input": "",
        "output": "<p>First of all, if I'm not mistaken, Hough Transform for circles expects a hollow circle, not a full one. This means you need to extract only the boundary/perimeter of the circles before applying the Hough Transform. OpenCV's <code>findContours</code> and <code>arcLength</code> functions will help you find the perimeters.</p>\n\n<p>Second of all, unfortunately, in my experience Hough Transform is very sensitive to variations of the circle shape, meaning that if the shape you're trying to detect is \"almost\" a circle, it might not be able to detect it.</p>\n\n<p>My advice is you should try to make your objects \"rounder\" by applying the <a href=\"http://en.wikipedia.org/wiki/Closing_(morphology)\" rel=\"nofollow noreferrer\">Closing morphological operation</a> on your binary image, with a disk-shaped structuring element. Then extract the perimeter of the objects in the image, and only then apply the Hough Transform. Hopefully, this will give you good enough results.</p>\n\n<hr>\n\n<p>Alternatively, you can try to detect circles using the <a href=\"http://en.wikipedia.org/wiki/RANSAC\" rel=\"nofollow noreferrer\">RANSAC</a> algorithm. <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/30809-ransac-algorithm-with-example-of-finding-homography\" rel=\"nofollow noreferrer\">Here</a> is an implementation for detecting lines, but you can adjust it for circles - just choose 3 points randomly (instead of 2), and define a circle that goes through them. Next, find all the points that lie near that circle (those are called the inlier points). These will be the points that satisfy the inequality:\n<img src=\"https://i.sstatic.net/LcqoY.png\" alt=\"enter image description here\"></p>\n\n<p>where <code>(x,y)</code> is the point, <code>(x0,y0)</code> is the center of the circle, <code>r</code> is its radius, and <code>margin</code> is a parameter you'll have to tune. \nThe rest of the algorithm is the same.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Applying Haar-like features to an image / defining the features",
        "input": "",
        "output": "<p>A feature for the haar-like feature algorithm is a single shape located in the selected window. \nSo each feature is binary valued and includes both the 'shape' of the feature and its relative position in the detection window. </p>\n\n<p>The image is processed by selecting many subwindows. The goal then is to discard any subwindows, not representing the desired object, as soon as possible. \nThis is accomplished by applying above features to each subwindow. From this feature set a classifier is learned. </p>\n\n<p>In case of the Viola-Jones detection framework a chain of classifiers is used, where the first classifiers use less features, therefore being faster to compute. \nIf a classifier in the chain discards the subwindow further computation on this window is stopped. </p>\n\n<p>The paper by Paul Viola and Michael Jones can be found <a href=\"https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf\" rel=\"nofollow\">here</a>. Another useful paper on haar-like feature detection was published by Sri-Kaushik Pavani, David Delgadoand  Alejandro F. Frangi under the name <a href=\"http://www.cistib.org/gestpub/attachments/Haar-like%20features%20with%20optimally%20weighted%20rectangles%20for%20rapid%20object%20detection.pdf-7d6f1eb720e3ab5937b69a479b89ae0a.pdf\" rel=\"nofollow\">Haar-like features with optimally weighted rectangles for rapid object detection</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can i fix this error with my UpdateMotionhistory function in opencv?",
        "input": "",
        "output": "<p>Let's take a look at the <a href=\"http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html#updatemotionhistory\" rel=\"nofollow\">documentation</a>: </p>\n\n<pre><code>void updateMotionHistory(InputArray silhouette, \n                         InputOutputArray mhi, \n                         double timestamp, \n                         double duration)\n</code></pre>\n\n<p>The <code>mhi</code> is a</p>\n\n<blockquote>\n  <p>single-channel, 32-bit floating-point</p>\n</blockquote>\n\n<p>Therefore, declare your <code>tst</code> with CV_32FC1, e.g. <code>Mat tst(frameHeight, frameWidth CV_32FC1);</code>,  can solve the OpenCV error.</p>\n\n<p>The<code>timestamp</code> and <code>duration</code> define the \"history\" of your application. The <code>timestamp</code> means \"now\", which is a number grows over time; and <code>duration</code> means the \"length of history you want to store in <code>mhi</code> (motion history image)\", which is a constant.</p>\n\n<p>Let's look at a simple one-dimension moving ball (B):</p>\n\n<pre><code>   B\n---|---|---|---|---|---&gt;\n   0   1   2   3   4    x\n</code></pre>\n\n<p>Assume the ball is moving to right by one each time. At time 1, the ball is at: </p>\n\n<pre><code>       B\n---|---|---|---|---|---&gt;\n   0   1   2   3   4    x\n</code></pre>\n\n<p>The silhouette (absdiff, same as in your code) at time(stamp) 1 is [True, True, False, False, False]. Assume the the initial value of <code>mhi</code> is [0, 0, 0, 0, 0] and <code>duration</code> is 2:</p>\n\n<pre><code>timestamp     silhouette             mhi\n 1          [T, T, F, F, F]  --&gt;  [1, 1, 0, 0, 0]\n 2          [F, T, T, F, F]  --&gt;  [1, 2, 2, 0, 0]\n 3          [F, F, T, T, F]  --&gt;  [1, 2, 3, 3, 0]\n 4          [F, F, F, T, T]  --&gt;  [0, 2, 3, 4, 4]\n ...\n</code></pre>\n\n<p>The <code>mhi</code> stores the history of timestamps. For example, <code>mhi[x] == t</code> means \"the ball was moving at position x when the timestamp was t.\"</p>\n\n<p>You can define your own <code>timestamp</code> and <code>duration</code>, according to your definition of \"history\" in the application. If you don't know how to determine the two parameters, here are two simple examples:</p>\n\n<ul>\n<li><code>timestamp = clock(); duration = 5000 ms;</code> Use real time as timestamp, stores the history of the last 5 seconds.</li>\n<li><code>timestamp = frameNumber(); duration = 50 frames;</code> Use video frame number as timestamp, stores the history of the last 50 frames. The <code>frameNumber()</code> simply returns <code>timestamp + 1.0</code> in your <code>while(1)</code> loop.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "vision.OpticalFlow is not showing the video nor the value of the flow",
        "input": "",
        "output": "<p>Your problem is in these two lines:</p>\n\n<pre><code>step(viewThresh,y&gt;mean(y(:)));\nstep(videoPlayer)\n</code></pre>\n\n<p>Try replacing them with these:</p>\n\n<pre><code>viewThresh = y;\nviewThresh(y &lt; mean(y(:))) = 0;\nstep(videoPlayer, viewThresh);\n</code></pre>\n\n<p>You do not need the <code>step</code> method for thresholding <code>y</code>, because you are not using any objects. And when you call <code>step</code> on the <code>videoPlayer</code> object, you have to pass in the video frame you wish to display. </p>\n",
        "system": ""
    },
    {
        "instruction": "How to estimate 3D pose using 2D tracking and initial 3D pose",
        "input": "",
        "output": "<p>When you know the 3D location of your feature points in some fixed co-ordinate system, then upon moving your camera all you really have to do is to estimate the pose of the camera and apply the reverse transformation to know the current pose of the object in current reference frame. Such problems are known as Pn-P problems. There is a huge body of work around this, but one of the recent papers on this topic that promises an efficient algorithm to estimate pose is <a href=\"http://cvlabwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf\" rel=\"nofollow\">http://cvlabwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf</a></p>\n\n<p>You can also use <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#solvepnp\" rel=\"nofollow\">cv::solvePnP</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Generalized Hough Transform for shape detection",
        "input": "",
        "output": "<p>There is an implementation by Jguillon on github, which is OpenCV implementation of the generalized Hough transform as described in [Ballard1981].</p>\n\n<p>The link is <a href=\"https://github.com/jguillon/generalized-hough-tranform\" rel=\"nofollow\">https://github.com/jguillon/generalized-hough-tranform</a>.  I build it with Visual Studio 2010 and it works.</p>\n",
        "system": ""
    },
    {
        "instruction": "Android OpenCV views other that CameraBridgeViewBase",
        "input": "",
        "output": "<p>I was able to achieve the required functionality by setting background on OpenCvCameraView.</p>\n",
        "system": ""
    },
    {
        "instruction": "In ExtractHogFeatures Matlab a line that I can&#39;t understand",
        "input": "",
        "output": "<p>It is an internal function.  Unfortunately, you cannot see the source, because it is compiled code.</p>\n",
        "system": ""
    },
    {
        "instruction": "Linear Color Gradient in openCV",
        "input": "",
        "output": "<p><em>I corrected my first code</em></p>\n\n<p>It seems to be a really complex code for something that should be easier.\nI would do something like that.</p>\n\n<pre><code>int taille = 500;    \nMat image(taille,taille,CV_8UC3);\nfor(int y = 0; y &lt; taille; y++){\n   Vec3b val;\n   val[0] = 0; val[1] = (y*255)/taille; val[2] = (taille-y)*255/taille;\n   for(int x = 0; x &lt; taille; x++)\n      image.at&lt;Vec3b&gt;(y,x) = val;\n}\n</code></pre>\n\n<p>On Micka's advice, I add a picture of the result with taille = 400;\n<img src=\"https://i.sstatic.net/XpEWJ.png\" alt=\"Result of the previous code.\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Neural network topology for object recognition on aerial photos (computer vision)",
        "input": "",
        "output": "<p>I think we need a better definition of \"buildings\". If you want to do building \"detection\", that is detect the presence of a building of any shape/size, this is difficult for a cascade classifier. You can try the following, though:</p>\n\n<ol>\n<li>Partition a set of known images to fixed-size blocks.</li>\n<li>Label each block as \"building\", \"not building\", or\n\"boundary(includes portions\n    of both)\"</li>\n<li><p>Extract basic features like intensity histograms, edges,\n    hough lines, HOG, etc.</p></li>\n<li><p>Train SVM classifiers based on these features (you can try others, too, but I recommend SVM by experience).</p></li>\n</ol>\n\n<p>Now you can partition your images again and use the trained classifier to get the results. The results will have to be combined to identify buildings.</p>\n\n<p>This will still need some testing to get the parameters(size of histograms, parameters of SVM classifier etc.) right. </p>\n\n<p>I have used this approach to detect \"food\" regions on images. The accuracy was below 70%, but my guess is that it will be better for buildings. </p>\n",
        "system": ""
    },
    {
        "instruction": "Differentiating between regular and irregular shaped blobs",
        "input": "",
        "output": "<p>One idea that came into my mind was that, you can use <code>number of corners</code> per <code>size of blob</code> to determine the <code>regularity/irregularity</code>. The test results seem to comply with our hypothesis too. Here's the code -</p>\n\n<pre><code>im = imread(input_image_path);\nbw= im2bw(im);\n\n%// Parameter for cutting into four slices into the third dimsensions \n%// corresponding to the four objects\ncommon_width = 270; \n\n%// Threshold to decide between regular and irregular ones\nfactor1_th = 0.01;\n\nbw1 = bw(:,1:common_width*floor(size(bw,2)/common_width)); %// Cropped image\nobjs =reshape(bw1,size(bw1,1),common_width,[]);%//Objects stored as dim3 slices\nfor objc=1:size(objs,3) %// Object counter\n    disp(['-------------- Processing Obj #' num2str(objc)]);\n    obj = objs(:,:,objc);\n    corners = corner(obj);\n    factor1 = size(corners,1)/nnz(obj)\n    if factor1 &gt; factor1_th\n        disp('This is an irregular one.'); %//'\n    else\n        disp('This is a regular one.'); %//'\n    end\nend\n</code></pre>\n\n<p>Output -</p>\n\n<pre><code>-------------- Processing Obj #1\nfactor1 =\n    0.0050\nThis is a regular one.\n-------------- Processing Obj #2\nfactor1 =\n    0.0109\nThis is an irregular one.\n-------------- Processing Obj #3\nfactor1 =\n    0.0052\nThis is a regular one.\n-------------- Processing Obj #4\nfactor1 =\n    0.0078\nThis is a regular one.\n</code></pre>\n\n<p>If anyone is interested in running the code, here's the input image that has the symbols a,b,c,d removed -</p>\n\n<p><img src=\"https://i.sstatic.net/uPpUU.jpg\" alt=\"enter image description here\"></p>\n\n<p>Link - <a href=\"https://i.sstatic.net/uPpUU.jpg\" rel=\"nofollow noreferrer\">https://i.sstatic.net/uPpUU.jpg</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Adding more hard negatives while training a classifier leads to more false positives",
        "input": "",
        "output": "<p>When there are 20000 negatives and 4000 positives, are you adjusting the weights accordingly? In my opinion, the weights should sum to one while maintaining the proportion. The weight of each +ve examples should be <code>0.5/4000</code> and accordingly determine the -ve weights too. If you are not adjusting the weights, then it explains why you are getting better performance for 5000 since the weight imbalance becomes less severe.</p>\n",
        "system": ""
    },
    {
        "instruction": "collect negative samples of adaboost algorithm for face detection",
        "input": "",
        "output": "<p>You have asked many questions inside your thread. </p>\n\n<ol>\n<li><strong>Amount of samples</strong>. As a rule of thumbs: When you train a detector you need roughly few thousands positive and negative examples per stage. Typical detector has 10-20 stages. Each stage reduces the amount of negative by a factor of 2. So you will need roughly 3,000 - 10,000 positive examples and ~5,000,000 to 100,000,000 negative examples. </li>\n<li><strong>Which negatives to take</strong>. A rule of thumb: You need to find a face in a given environment. So you need to take that environment as negative examples. For instance, if you try to detect faces of students sitting in a classroom than take as negative examples images from the classroom (walls, windows, human body, clothes etc). Taking images of the moon or of the sky will probably not help you. If you don't know your environment than just take as much as possible different natural images (under different light conditions).</li>\n<li><strong>Should you take facial parts</strong> (like an eye, or a nose) as negative? You can but this is definitely not enough (to take only those negatives). The real strength of the detector will come from the negative images which represent the typical background of the faces </li>\n<li><strong>How to collect/generate negative samples</strong> - You don't actually need many negative images. You can take 1000 images and generate 10,000,000 negative samples from them. Here is how you do it. Suppose you take a photo of a car of 1 mega pixel resolution 1000x1000 pixels. Suppose than you want to train face detector to work on resolution of 20x20 pixels (like openCV did). So you take your 1000x1000 big image and cut it to pieces of 20x20. You can get 2,500 pieces (50x50). So this is how from a single big image you generated 2,500 negative examples. Now you can take the same big image and cut it to pieces of size 10x10 pixels. You will now have additional 10,000 negative examples. Each example is of size 10x10 pixels and you can enlarge it by factor of 2 to force all the sample to have the same size. You can repeat this process as much as you want (cutting the input image to pieces of different size). Mathematically speaking, if your image is of size NxN - You can generate O(N^4) negative examples from it by taking each possible rectangle inside it.</li>\n<li>In step 4, I described how to take a single big image and cut it to a large amount of negative examples. I must warn you that negative examples should not have high co-variance so I don't recommend taking only one image and generating 1 million negative examples from it. As a rule of thumb - create a library of 1000 images (or download random images from Google). Verify than none of the images contains faces. Crop about 10,000 negative examples from each image and now you have got a decent 10,000,000 negative examples. Train your detector. In the next step you can cut each image to ~50,000 (partially overlapping pieces) and thus enlarge your amount of negatives to 50 millions. You will start having very good results with it. </li>\n<li><strong>Final enhancement step of the detector</strong>. When you already have a rather good detector, run it on many images. It will produce false detections (detect face where there is no face). Gather all those false detections and add them to your negative set. Now retrain the detector once again. The more such iterations you do the better your detector becomes</li>\n<li><strong>Real numbers</strong> - The best face detectors today (like Facebooks) use hundreds of millions of positive examples and billions of negatives. As positive examples they take not only frontal faces but faces in many orientations, different facial expressions (smiling, shouting, angry,...), different age groups, different genders, different races (Caucasians, blacks, Thai, Chinese,....), with or without glasses/hat/sunglasses/make-up etc. You will not be able to compete with the best, so don't get angry if your detector misses some faces.<br>\nGood luck</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How can i prevent my object detection program from detecting multiple objects of different sizes?",
        "input": "",
        "output": "<p>I think you can try out the following procedure:</p>\n\n<ul>\n<li>obtain a circular kernel having roughly the same area as your object of interest. You can do it like: <strong>Mat kernel = getStructuringElement(MORPH_ELLIPSE, Size(d, d));</strong> \nwhere d is the diameter of the disk.</li>\n<li>perform normalized-cross-correlation or convolution of the filtered regions image with this kernel (I think normalized-cross-correlation would be better. And add an empty boarder around the kernel).</li>\n<li>the peak of the resulting image should give you the location of the circular region in your filtered image (if you are using normalized-cross-correlation, you'll have to add the shift).</li>\n</ul>\n\n<p>To speed things up, you can perform this at a reduced resolution. </p>\n",
        "system": ""
    },
    {
        "instruction": "Paper currency recognition by image processing",
        "input": "",
        "output": "<p>Here is my rather botched attempt at determining whether your bank note is fake or real.  One thing I have noticed between the notes is that the real note has its thin strip to be more or less continuous while the fake strip has fragmented thin lines in the strip.  One could say that the fake note has <strong>more than one line</strong> in the thin strip while the real note <strong>only has one line</strong>.  Let's try and get our image so that we detect the strip itself (as you have also have tried) and we count how many lines we see.  If we see just one line, it's real but if we see more than one line it's fake.  I'll walk you through how I did it with intermediate images in between.</p>\n\n<h1>Step #1 - Read in the image (of course)</h1>\n\n<p>I'm going to directly read your images in from StackOverflow.  <code>imread</code> is great for reading images from online URLs:</p>\n\n<pre><code>%%//Read in image\nclear all;\nclose all;\nIreal = imread('https://i.sstatic.net/SqbnIm.jpg'); %//Real\nIfake = imread('https://i.sstatic.net/2U3DEm.jpg'); %//Fake\n</code></pre>\n\n<h1>Step #2 - Decompose image into HSV and analyse</h1>\n\n<p>One thing I have noticed is that the strips are very dark while the bank note is predominantly green in colour.  I used some basic colour image processing as a pre-processing step.  I transformed the image into HSV (<a href=\"http://en.wikipedia.org/wiki/HSL_and_HSV\" rel=\"noreferrer\">Hue-Saturation-Value</a>) and took a look at each component separately:</p>\n\n<pre><code>%% Pre-analysis\nhsvImageReal = rgb2hsv(Ireal);\nhsvImageFake = rgb2hsv(Ifake);\nfigure;\nimshow([hsvImageReal(:,:,1) hsvImageReal(:,:,2) hsvImageReal(:,:,3)]);\ntitle('Real');\nfigure;\nimshow([hsvImageFake(:,:,1) hsvImageFake(:,:,2) hsvImageFake(:,:,3)]);\ntitle('Fake');\n</code></pre>\n\n<p>Here are what the images look like:</p>\n\n<p><img src=\"https://i.sstatic.net/crc55.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/KYMgb.png\" alt=\"enter image description here\"></p>\n\n<p>In this code, I'm displaying each of the components side by side each other for the hue, saturation and value respectively.  You'll notice something very peculiar.  The black thin strip has a saturation that is quite high, which makes sense as black could be considered as a \"colour\" that is pure saturation (devoid of white light).  The value component has its strip with values to be very low, which also makes sense as value captures the lightness / intensity of the colour.  </p>\n\n<h1>Step #3 - Threshold the saturation and value planes to create a binary image</h1>\n\n<p>With the observations I made above, I'm going to threshold the image looking at the saturation and value planes.  Any points that have a combined saturation that is rather high and values that are rather low are candidates that are part of the black strip.  I am going to crop out just the strips by themselves to make things easier (as you have already did too).  Take note that the position of the strips in each image differ, so I had to adjust accordingly.  I simply extracted out the right columns, while leaving the rows and slices the same.  These saturation and value thresholds are rather ad-hoc, so I had to play around with these to get good results.</p>\n\n<pre><code>%%//Initial segmentation\ncroppedImageReal = hsvImageReal(:,90:95,:);\ncroppedImageFake = hsvImageFake(:,93:98,:);\nsatThresh = 0.4;\nvalThresh = 0.3;\nBWImageReal = (croppedImageReal(:,:,2) &gt; satThresh &amp; croppedImageReal(:,:,3) &lt; valThresh);\nfigure;\nsubplot(1,2,1);\nimshow(BWImageReal);\ntitle('Real');\nBWImageFake = (croppedImageFake(:,:,2) &gt; satThresh &amp; croppedImageFake(:,:,3) &lt; valThresh);\nsubplot(1,2,2);\nimshow(BWImageFake);\ntitle('Fake');\n</code></pre>\n\n<p>These are what the images look like:</p>\n\n<p><img src=\"https://i.sstatic.net/8FBSF.png\" alt=\"enter image description here\"></p>\n\n<p>You can see that the real strip more or less has more connectivity than the fake strip.  Let's do a bit more processing to clean this up</p>\n\n<h1>Step #4 - Do some minor closings</h1>\n\n<p>If you take a look at the thin black strip of the fake note, you'll see that each black line is separated by quite a few pixels while the real note really has no separation.  However, you'll see that in the real strip above, there are still some parts of the line that are disconnected.  As such, let's try and connect the line together.  This is safe because if we were to do this on the fake image, the parts of the strip are so far apart that closing shouldn't make a difference, but it'll help in our real image analysis.  As such, I closed these images by a 6 pixel line that is vertical.  Here's the code to do that:</p>\n\n<pre><code>%%//Post-process\nse = strel('line', 6, 90);\nBWImageCloseReal = imclose(BWImageReal, se);\nBWImageCloseFake = imclose(BWImageFake, se);\nfigure;\nsubplot(1,2,1);\nimshow(BWImageCloseReal);\ntitle('Real');\nsubplot(1,2,2);\nimshow(BWImageCloseFake);\ntitle('Fake');\n</code></pre>\n\n<p>These are what the images look like:</p>\n\n<p><img src=\"https://i.sstatic.net/bduHf.png\" alt=\"enter image description here\"></p>\n\n<h1>Step #5 - Final cleanup</h1>\n\n<p>You'll notice that for each of the images, there are some noisy pixels on the edges.  As such, let's use an area opening through <a href=\"http://www.mathworks.com/help/images/ref/bwareaopen.html\" rel=\"noreferrer\"><code>bwareaopen</code></a>.  This function removes pixel areas in a black and white image that have less than a certain area.  I'm going to choose 15 to get rid of the pixels along the edges that don't belong to the strip.  As such:</p>\n\n<pre><code>%%//Area open the image\nfigure;\nareaopenReal = bwareaopen(BWImageCloseReal, 15);\nimshow(areaopenReal);\ntitle('Real');\nfigure;\nareaopenFake = bwareaopen(BWImageCloseFake, 15);\nimshow(areaopenFake);\ntitle('Fake');\n</code></pre>\n\n<p>Here's what the images look like:</p>\n\n<p><img src=\"https://i.sstatic.net/Pq9Z5.png\" alt=\"enter image description here\"></p>\n\n<h1>Step #6 - Count the number of black lines</h1>\n\n<p>The last step is to simply count the number of black lines in each image.  If there is just 1, this denotes that the bank note is real, while if there is more than 1, this denotes that the bank note is fake.  We can use <a href=\"http://www.mathworks.com/help/images/ref/bwlabel.html\" rel=\"noreferrer\"><code>bwlabel</code></a> and use the second parameter to count how many objects there are.  In other words:</p>\n\n<pre><code>%%//Count how many objects there are\n[~,countReal] = bwlabel(areaopenReal);\n[~,countFake] = bwlabel(areaopenFake);\ndisp(['The total number of black lines for the real note is: ' num2str(countReal)]);\ndisp(['The total number of black lines for the fake note is: ' num2str(countFake)]);\n</code></pre>\n\n<p>We get the following output in MATLAB:</p>\n\n<pre><code>The total number of black lines for the real note is: 1\nThe total number of black lines for the fake note is: 4\n</code></pre>\n\n<p>As you can see, the real note has just one line while the fake note has more than one.  You'll have to play with this code depending on what bank note you have to get this to work, but this is somewhere to start.</p>\n\n<hr>\n\n<h1>Full code</h1>\n\n<p>Just for completeness, here is the full code so you can copy and paste and run in MATLAB yourself.</p>\n\n<pre><code>%%//Read in image\nclear all;\nclose all;\nIreal = imread('https://i.sstatic.net/SqbnIm.jpg'); % Real\nIfake = imread('https://i.sstatic.net/2U3DEm.jpg'); % Fake\n\n%%//Pre-analysis\nhsvImageReal = rgb2hsv(Ireal);\nhsvImageFake = rgb2hsv(Ifake);\nfigure;\nimshow([hsvImageReal(:,:,1) hsvImageReal(:,:,2) hsvImageReal(:,:,3)]);\ntitle('Real');\nfigure;\nimshow([hsvImageFake(:,:,1) hsvImageFake(:,:,2) hsvImageFake(:,:,3)]);\ntitle('Fake');\n\n%%//Initial segmentation\ncroppedImageReal = hsvImageReal(:,90:95,:);\ncroppedImageFake = hsvImageFake(:,93:98,:);\nsatThresh = 0.4;\nvalThresh = 0.3;\nBWImageReal = (croppedImageReal(:,:,2) &gt; satThresh &amp; croppedImageReal(:,:,3) &lt; valThresh);\nfigure;\nsubplot(1,2,1);\nimshow(BWImageReal);\ntitle('Real');\nBWImageFake = (croppedImageFake(:,:,2) &gt; satThresh &amp; croppedImageFake(:,:,3) &lt; valThresh);\nsubplot(1,2,2);\nimshow(BWImageFake);\ntitle('Fake');\n\n%%//Post-process\nse = strel('line', 6, 90);\nBWImageCloseReal = imclose(BWImageReal, se);\nBWImageCloseFake = imclose(BWImageFake, se);\nfigure;\nsubplot(1,2,1);\nimshow(BWImageCloseReal);\ntitle('Real');\nsubplot(1,2,2);\nimshow(BWImageCloseFake);\ntitle('Fake');\n\n%%//Area open the image\nfigure;\nareaopenReal = bwareaopen(BWImageCloseReal, 15);\nsubplot(1,2,1);\nimshow(areaopenReal);\ntitle('Real');\nsubplot(1,2,2);\nareaopenFake = bwareaopen(BWImageCloseFake, 15);\nimshow(areaopenFake);\ntitle('Fake');\n\n%%//Count how many objects there are\n[~,countReal] = bwlabel(areaopenReal);\n[~,countFake] = bwlabel(areaopenFake);\ndisp(['The total number of black lines for the real note is: ' num2str(countReal)]);\ndisp(['The total number of black lines for the fake note is: ' num2str(countFake)]);\n</code></pre>\n\n<hr>\n\n<h1>Edit - September 4th, 2014</h1>\n\n<p>You contacted me and wanted to know how to detect the large black strip that is to the right of each note.  This is actually not so bad to do.  The image that you posted that is the other fake note is of a different size than the others.  As such, I'm going to resize this image so that this image is roughly the same size as the others you have shown.  This is the image you have posted in the comments:</p>\n\n<p><img src=\"https://i.sstatic.net/NpJ4q.jpg\" alt=\"\"></p>\n\n<p>By looking at all of the notes, they hover between the 195th column to the 215th column.  This is assuming that each image has 320 columns.  Now, the process behind how I detect whether the bank note is fake is by looking at the overall intensity of the black strip itself.  You'll notice that the fake notes either do not have a black strip, or the strip is rather dull and faded.  We can certainly use this to our advantage.  Here is a quick list of what I did to detect the black strip:</p>\n\n<ul>\n<li>Read in the images and resize them to all the same columns when necessary</li>\n<li>Extract the 195th to 215th columns for all of the images</li>\n<li>Convert the images to grayscale using <a href=\"http://www.mathworks.com/help/images/ref/rgb2gray.html\" rel=\"noreferrer\"><code>rgb2gray</code></a></li>\n<li>Threshold the images using about intensity level 30. I used 30 heuristically as this was predominantly the intensity that I saw that the black strip consisted of.  You then invert the images so that black becomes white.  I need the black strip to become white for further analysis.  I use <a href=\"http://www.mathworks.com/help/images/ref/im2bw.html\" rel=\"noreferrer\"><code>im2bw</code></a> to do this.</li>\n<li>I area open the image like I did before, but specifying a larger area of about 100 to ensure I get rid of any spurious noisy and isolated pixels.</li>\n<li>I do a closing using a square structuring element of 5 x 5 to ensure that any disconnected regions that are near larger regions get connected to each other.</li>\n<li>I then count the total number of objects that are left.  If the count is <strong>not equal to 1</strong>, then it's fake note.  If it's just 1, then it's a real note.</li>\n</ul>\n\n<hr>\n\n<p>Here is the full code:</p>\n\n<pre><code>%% //Read in images\nclear all;\nclose all;\nIreal = imread('https://i.sstatic.net/SqbnIm.jpg'); % Real\nIfake = imread('https://i.sstatic.net/2U3DEm.jpg'); % Fake\nIfake2 = imread('https://i.sstatic.net/NpJ4q.jpg'); % Fake #2\n% //Resize so that we have the same dimensions as the other images\nIfake2 = imresize(Ifake2, [160 320], 'bilinear');\n\n%% //Extract the black strips for each image\nblackStripReal = Ireal(:,195:215,:);\nblackStripFake = Ifake(:,195:215,:);\nblackStripFake2 = Ifake2(:,195:215,:);\n\nfigure(1);\nsubplot(1,3,1);\nimshow(blackStripReal);\ntitle('Real');\nsubplot(1,3,2);\nimshow(blackStripFake);\ntitle('Fake');\nsubplot(1,3,3);\nimshow(blackStripFake2);\ntitle('Fake #2');\n\n%% //Convert into grayscale then threshold\nblackStripReal = rgb2gray(blackStripReal);\nblackStripFake = rgb2gray(blackStripFake);\nblackStripFake2 = rgb2gray(blackStripFake2);\n\nfigure(2);\nsubplot(1,3,1);\nimshow(blackStripReal);\ntitle('Real');\nsubplot(1,3,2);\nimshow(blackStripFake);\ntitle('Fake');\nsubplot(1,3,3);\nimshow(blackStripFake2);\ntitle('Fake #2');\n\n%% //Threshold using about intensity 30\nblackStripRealBW = ~im2bw(blackStripReal, 30/255);\nblackStripFakeBW = ~im2bw(blackStripFake, 30/255);\nblackStripFake2BW = ~im2bw(blackStripFake2, 30/255);\n\nfigure(3);\nsubplot(1,3,1);\nimshow(blackStripRealBW);\ntitle('Real');\nsubplot(1,3,2);\nimshow(blackStripFakeBW);\ntitle('Fake');\nsubplot(1,3,3);\nimshow(blackStripFake2BW);\ntitle('Fake #2');\n\n%% //Area open the image\nfigure(4);\nareaopenReal = bwareaopen(blackStripRealBW, 100);\nsubplot(1,3,1);\nimshow(areaopenReal);\ntitle('Real');\nsubplot(1,3,2);\nareaopenFake = bwareaopen(blackStripFakeBW, 100);\nimshow(areaopenFake);\ntitle('Fake');\nsubplot(1,3,3);\nareaopenFake2 = bwareaopen(blackStripFake2BW, 100);\nimshow(areaopenFake2);\ntitle('Fake #2');\n\n%% //Post-process\nse = strel('square', 5);\nBWImageCloseReal = imclose(areaopenReal, se);\nBWImageCloseFake = imclose(areaopenFake, se);\nBWImageCloseFake2 = imclose(areaopenFake2, se);\nfigure(5);\nsubplot(1,3,1);\nimshow(BWImageCloseReal);\ntitle('Real');\nsubplot(1,3,2);\nimshow(BWImageCloseFake);\ntitle('Fake');\nsubplot(1,3,3);\nimshow(BWImageCloseFake2);\ntitle('Fake #2');\n\n%% //Count the total number of objects in this strip\n[~,countReal] = bwlabel(BWImageCloseReal);\n[~,countFake] = bwlabel(BWImageCloseFake);\n[~,countFake2] = bwlabel(BWImageCloseFake2);\ndisp(['The total number of black lines for the real note is: ' num2str(countReal)]);\ndisp(['The total number of black lines for the fake note is: ' num2str(countFake)]);\ndisp(['The total number of black lines for the second fake note is: ' num2str(countFake2)]);\n</code></pre>\n\n<hr>\n\n<p>Bear in mind that you'll have to play around with these parameters to suit your purposes.    Here are what the figures look like at each step:</p>\n\n<p><img src=\"https://i.sstatic.net/anh26.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/xrOhP.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/79Ze7.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/Yqi4H.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/jiI1q.png\" alt=\"enter image description here\"></p>\n\n<p>... and finally the object count:</p>\n\n<pre><code>The total number of black lines for the real note is: 1\nThe total number of black lines for the fake note is: 2\nThe total number of black lines for the second fake note is: 0\n</code></pre>\n\n<hr>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "How can i make my Colored object detection and tracking program more efficient?",
        "input": "",
        "output": "<p>Assuming you have got a binary image as output of your color filtering, then:</p>\n\n<ol>\n<li>Use <code>findContours()</code>-function (<a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours\" rel=\"nofollow\">http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours</a>) to extract your objects. You get the contours of all red objects inclusive your coke cap.</li>\n<li><p>Now you have to distinguish between your coke cap and the rest of the red colored objects. In your case it would be appropriate to use morphologic features like the area (as you mentioned) and the circularity of the objects (caps are circular!):</p>\n\n<ul>\n<li><p>Area can be calculated by <code>contourArea()</code>-function (<a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#contourarea\" rel=\"nofollow\">http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#contourarea</a>) and compared with a threshold.</p></li>\n<li><p>The circularity of an object can be calculated by <code>circularity = (4*PI*A)/P^2</code>, where <em>A</em> is the area (calculated by the above function) and <em>P</em> is the perimeter of the object. The perimeter is the output of <code>arcLength()</code>-function (<a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#arclength\" rel=\"nofollow\">http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#arclength</a>). A circle has got a circularity of 1. You can compare the values of your objects with it.</p></li>\n</ul></li>\n</ol>\n\n<p>To sum up, use the features <em>object's area</em> and <em>object's circularity</em> to differ coke caps from other red objects.</p>\n",
        "system": ""
    },
    {
        "instruction": "How exactly does the dp parameter in the houghcircle function in the opencv library work?",
        "input": "",
        "output": "<p><em>Verba docent,<br>\nExempla trahunt.</em></p>\n\n<h2>Best seen on an interactive GUI-Demo</h2>\n\n<p>A configurable UI-panel allows to adjust parameter(s) by moving a few sliders</p>\n\n<hr>\n\n<pre><code>import sys\nimport cv2\nimport math\nimport numpy\nfrom scipy.ndimage import label\npi_4 = 4*math.pi\n\ndef nothing_asCallback(x):\n    pass\n\ndef GUI_openCV_circles():\n    # --------------------------------------------------------------------------------GUI-&lt;image&gt;\n    frame = cv2.imread(  \"openCV_CircleDetection_IMG_LASSO_AREA.JPG\" )\n    demo  = frame[:800,:800,:]\n    # --------------------------------------------------------------------------------GUI-&lt;window&gt;-s\n    cv2.namedWindow( \"DEMO.IN\",             cv2.cv.CV_WINDOW_AUTOSIZE )\n    cv2.namedWindow( \"DEMO.Canny\",          cv2.cv.CV_WINDOW_AUTOSIZE )\n    cv2.namedWindow( \"DEMO.Canny.Circles\",  cv2.cv.CV_WINDOW_AUTOSIZE )\n    # --------------------------------------------------------------------------------GUI-&lt;state&gt;-initial-value(s)\n    aKeyPRESSED                                     = None              # .init\n\n    aCanny_LoTreshold                               = 127\n    aCanny_LoTreshold_PREVIOUS                      =  -1\n    aCanny_HiTreshold                               = 255\n    aCanny_HiTreshold_PREVIOUS                      =  -1\n\n    aHough_dp                                       =   1\n    aHough_dp_PREVIOUS                              =  -1\n    aHough_minDistance                              =  10\n    aHough_minDistance_PREVIOUS                     =  -1\n    aHough_param1_aCannyHiTreshold                  = 255\n    aHough_param1_aCannyHiTreshold_PREVIOUS         =  -1\n    aHough_param2_aCentreDetectTreshold             =  20\n    aHough_param2_aCentreDetectTreshold_PREVIOUS    =  -1\n    aHough_minRadius                                =  10\n    aHough_minRadius_PREVIOUS                       =  -1\n    aHough_maxRadius                                =  30\n    aHough_maxRadius_PREVIOUS                       =  -1\n    # --------------------------------------------------------------------------------GUI-&lt;ACTOR&gt;-s\n    cv2.createTrackbar( \"Lo_Treshold\",          \"DEMO.Canny\",          aCanny_LoTreshold,                      255, nothing_asCallback )\n    cv2.createTrackbar( \"Hi_Treshold\",          \"DEMO.Canny\",          aCanny_HiTreshold,                      255, nothing_asCallback )\n\n    cv2.createTrackbar( \"dp\",                   \"DEMO.Canny.Circles\",  aHough_dp,                              255, nothing_asCallback )\n    cv2.createTrackbar( \"minDistance\",          \"DEMO.Canny.Circles\",  aHough_minDistance,                     255, nothing_asCallback )\n    cv2.createTrackbar( \"param1_HiTreshold\",    \"DEMO.Canny.Circles\",  aHough_param1_aCannyHiTreshold,         255, nothing_asCallback )\n    cv2.createTrackbar( \"param2_CentreDetect\",  \"DEMO.Canny.Circles\",  aHough_param2_aCentreDetectTreshold,    255, nothing_asCallback )\n    cv2.createTrackbar( \"minRadius\",            \"DEMO.Canny.Circles\",  aHough_minRadius,                       255, nothing_asCallback )\n    cv2.createTrackbar( \"maxRadius\",            \"DEMO.Canny.Circles\",  aHough_maxRadius,                       255, nothing_asCallback )\n\n    cv2.imshow( \"DEMO.IN\",          demo )                              # static ...\n    # --------------------------------------------------------------------------------GUI-mainloop()\n    print \" --------------------------------------------------------------------------- press [ESC] to exit \"\n    while( True ):\n        # --------------------------------------------------------------------------------GUI-[ESCAPE]?\n        if aKeyPRESSED == 27:\n            break\n        # --------------------------------------------------------------------------------&lt;vars&gt;-DETECT-delta(s)\n        aCanny_LoTreshold = cv2.getTrackbarPos( \"Lo_Treshold\", \"DEMO.Canny\" )\n        aCanny_HiTreshold = cv2.getTrackbarPos( \"Hi_Treshold\", \"DEMO.Canny\" )\n\n        if (    aCanny_LoTreshold      != aCanny_LoTreshold_PREVIOUS\n            or  aCanny_HiTreshold      != aCanny_HiTreshold_PREVIOUS\n            ):\n            # --------------------------= FLAG\n            aCannyRefreshFLAG           = True\n            # --------------------------= RE-SYNC\n            aCanny_LoTreshold_PREVIOUS  = aCanny_LoTreshold\n            aCanny_HiTreshold_PREVIOUS  = aCanny_HiTreshold\n        else:\n            # --------------------------= Un-FLAG\n            aCannyRefreshFLAG           = False\n\n        aHough_dp                           = cv2.getTrackbarPos( \"dp\",                 \"DEMO.Canny.Circles\" )\n        aHough_minDistance                  = cv2.getTrackbarPos( \"minDistance\",        \"DEMO.Canny.Circles\" )\n        aHough_param1_aCannyHiTreshold      = cv2.getTrackbarPos( \"param1_HiTreshold\",  \"DEMO.Canny.Circles\" )\n        aHough_param2_aCentreDetectTreshold = cv2.getTrackbarPos( \"param2_CentreDetect\",\"DEMO.Canny.Circles\" )\n        aHough_minRadius                    = cv2.getTrackbarPos( \"minRadius\",          \"DEMO.Canny.Circles\" )\n        aHough_maxRadius                    = cv2.getTrackbarPos( \"maxRadius\",          \"DEMO.Canny.Circles\" )\n\n        if (    aHough_dp                            != aHough_dp_PREVIOUS\n            or  aHough_minDistance                   != aHough_minDistance_PREVIOUS\n            or  aHough_param1_aCannyHiTreshold       != aHough_param1_aCannyHiTreshold_PREVIOUS\n            or  aHough_param2_aCentreDetectTreshold  != aHough_param2_aCentreDetectTreshold_PREVIOUS    \n            or  aHough_minRadius                     != aHough_minRadius_PREVIOUS\n            or  aHough_maxRadius                     != aHough_maxRadius_PREVIOUS\n            ):\n            # --------------------------= FLAG\n            aHoughRefreshFLAG           = True                  \n            # ----------------------------------------------= RE-SYNC\n            aHough_dp_PREVIOUS                              =  aHough_dp                          \n            aHough_minDistance_PREVIOUS                     =  aHough_minDistance                 \n            aHough_param1_aCannyHiTreshold_PREVIOUS         =  aHough_param1_aCannyHiTreshold     \n            aHough_param2_aCentreDetectTreshold_PREVIOUS    =  aHough_param2_aCentreDetectTreshold\n            aHough_minRadius_PREVIOUS                       =  aHough_minRadius                   \n            aHough_maxRadius_PREVIOUS                       =  aHough_maxRadius                   \n        else:\n            # --------------------------= Un-FLAG\n            aHoughRefreshFLAG           = False\n        # --------------------------------------------------------------------------------REFRESH-process-pipe-line ( with recent &lt;state&gt; &lt;vars&gt; )\n        if ( aCannyRefreshFLAG ):\n\n            edges   = cv2.Canny(        demo,   aCanny_LoTreshold,\n                                                aCanny_HiTreshold\n                                        )\n            # --------------------------------------------------------------------------------GUI-SHOW-Canny()-&lt;edges&gt;-onRefreshFLAG\n            cv2.imshow( \"DEMO.Canny\",   edges )\n            pass\n\n        if ( aCannyRefreshFLAG or aHoughRefreshFLAG ):\n\n            circles = cv2.HoughCircles( edges,  cv2.cv.CV_HOUGH_GRADIENT,\n                                                aHough_dp,\n                                                aHough_minDistance,\n                                                param1      = aHough_param1_aCannyHiTreshold,\n                                                param2      = aHough_param2_aCentreDetectTreshold,\n                                                minRadius   = aHough_minRadius,\n                                                maxRadius   = aHough_maxRadius\n                                        )\n            # --------------------------------------------------------------------------------GUI-SHOW-HoughCircles()-&lt;edges&gt;-onRefreshFLAG\n            demoWithCircles = cv2.cvtColor( demo,            cv2.COLOR_BGR2RGB )                          # .re-init &lt;&lt;&lt; src\n            demoWithCircles = cv2.cvtColor( demoWithCircles, cv2.COLOR_RGB2BGR )\n\n            for aCircle in circles[0]:\n                cv2.circle( demoWithCircles,    ( int( aCircle[0] ), int( aCircle[1] ) ),\n                                                aCircle[2],\n                                                (0,255,0),\n                                                1\n                            )\n                pass\n            pass\n            cv2.imshow( \"DEMO.Canny.Circles\", demoWithCircles )\n        pass        \n        # --------------------------------------------------------------------------------&lt;vars&gt;-UPDATE-&lt;state&gt;\n        # ref. above in .onRefreshFLAG RE-SYNC sections\n        # --------------------------------------------------------------------------------GUI-INPUT ? [ESCAPE]\n        aKeyPRESSED = cv2.waitKey(1) &amp; 0xFF\n    pass\n    # --------------------------------------------------------------------------------GUI-&lt;window&gt;-s / DESTROY\n    cv2.destroyWindow( \"DEMO.IN\" )\n    cv2.destroyWindow( \"DEMO.Canny\" )\n    cv2.destroyWindow( \"DEMO.Canny.Circles\" )\n    # --------------------------------------------------------------------------------GUI-&lt;window&gt;-s\n    pass\n\ndef main():\n    GUI_openCV_circles()\n    return 0\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Screen Capture APIs",
        "input": "",
        "output": "<p>most likely others are faster because other parties use faster compression while sending you the data. Or try to send less data, for example divide the screen with 8 x 8 blocks, and only send those blocks that have changed in between.. things like that are often used in such apps.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to apply the camera pose transformation computed using EPnP to the VTK camera?",
        "input": "",
        "output": "<p>Yeah things get complicated when applying those parameters to the vtk camera.\nHere is how I did it (just excerpts of the important code passages, the whole code would be way too much to paste here and would be useless for you anyway). Other points to consider: </p>\n\n<ol>\n<li>I am rendering the endoscope image as background texture in my vtkRenderWindow. </li>\n<li>I am using a mix of VTK, ITK (vnl), OpenCV functions but they should be interchangeable (e.g. cvRound could also be replaced by vtkMath::Round() etc.)</li>\n</ol>\n\n<p>First of all, i use the active camera from my vtkRenderer:</p>\n\n<pre><code>d-&gt;m_Renderer-&gt;GetActiveCamera()\n</code></pre>\n\n<p>The next step is to continously update the active camera by applying your transform. <strong>Depending on whether your render window is resizeable or not, you have to initialize or also continously update two further parameters: 1. ViewAngle, 2. WindowCenter</strong> (EXTREMELY important, not documented at all by vtk. But in the end you have to apply your principal point here which you found by calibration or you will have your surfaces rendered with an offset. Took me 3 months to find this two line solution).</p>\n\n<p><strong>Calculation of the view angle:</strong></p>\n\n<pre><code>  double focalLengthY = _CameraIntrinsics-&gt;GetFocalLengthY();\n  if( _WindowSize.height != _ImageSize.height )\n  {\n    double factor = static_cast&lt;double&gt;(_WindowSize.height)/static_cast&lt;double&gt;(_ImageSize.height);\n    focalLengthY = _CameraIntrinsics-&gt;GetFocalLengthY() * factor;\n  }\n\n  _ViewAngle = 2 * atan( ( _WindowSize.height / 2 ) / focalLengthY ) * 180 / vnl_math::pi;\n</code></pre>\n\n<p><strong>Apply the view angle:</strong></p>\n\n<pre><code>d-&gt;m_Renderer-&gt;GetActiveCamera()-&gt;SetViewAngle(viewAngle);\n</code></pre>\n\n<p><strong>Calculation of the WindowCenter:</strong></p>\n\n<pre><code>  double px = 0;\n  double width = 0;\n\n  double py = 0;\n  double height = 0;\n\n  if( _ImageSize.width != _WindowSize.width || _ImageSize.height != _WindowSize.height )\n  {\n    double factor = static_cast&lt;double&gt;(_WindowSize.height)/static_cast&lt;double&gt;(_ImageSize.height);\n\n    px = factor * _CameraIntrinsics-&gt;GetPrincipalPointX();\n    width = _WindowSize.width;\n    int expectedWindowSize = cvRound(factor * static_cast&lt;double&gt;(_ImageSize.width));\n    if( expectedWindowSize != _WindowSize.width )\n    {\n      int diffX = (_WindowSize.width - expectedWindowSize) / 2;\n      px = px + diffX;\n    }\n\n    py = factor * _CameraIntrinsics-&gt;GetPrincipalPointY();\n    height = _WindowSize.height;\n  }\n  else\n  {\n    px = _CameraIntrinsics-&gt;GetPrincipalPointX();\n    width = _ImageSize.width;\n\n    py = _CameraIntrinsics-&gt;GetPrincipalPointY();\n    height = _ImageSize.height;\n  }\n\n  double cx = width - px;\n  double cy = py;\n\n  _WindowCenter.x = cx / ( ( width-1)/2 ) - 1 ;\n  _WindowCenter.y = cy / ( ( height-1)/2 ) - 1;\n</code></pre>\n\n<p><strong>Setting the Window Center:</strong></p>\n\n<pre><code> d-&gt;m_Renderer-&gt;GetActiveCamera()-&gt;SetWindowCenter(_WindowCenter.x, _WindowCenter.y);\n</code></pre>\n\n<p><strong>Applying the extrinsic matrix to the camera</strong>:</p>\n\n<pre><code>// create a scaling matrix (THE CLASS TRANSFORM IS A WRAPPER FOR A 4x4 Matrix, methods should be self-documenting)\nd-&gt;m_ScaledTransform = Transform::New();\nd-&gt;m_ScaleMat.set_identity();\nd-&gt;m_ScaleMat(1,1) = -d-&gt;m_ScaleMat(1,1);\nd-&gt;m_ScaleMat(2,2) = -d-&gt;m_ScaleMat(2,2);\n\n// scale the matrix appropriately (m_VnlMat is a VNL 4x4 Matrix)\nd-&gt;m_VnlMat = d-&gt;m_CameraExtrinsicMatrix-&gt;GetMatrix();\nd-&gt;m_VnlMat = d-&gt;m_ScaleMat * d-&gt;m_VnlMat;\nd-&gt;m_ScaledTransform-&gt;SetMatrix( d-&gt;m_VnlMat );\n\nd-&gt;m_VnlRotation = d-&gt;m_ScaledTransform-&gt;GetVnlRotationMatrix();\nd-&gt;m_VnlRotation.normalize_rows();\nd-&gt;m_VnlInverseRotation = vnl_matrix_inverse&lt;mitk::ScalarType&gt;( d-&gt;m_VnlRotation );\n\n// rotate translation vector by inverse rotation P = P'\nd-&gt;m_VnlTranslation = d-&gt;m_ScaledTransform-&gt;GetVnlTranslation();\nd-&gt;m_VnlTranslation = d-&gt;m_VnlInverseRotation * d-&gt;m_VnlTranslation;\nd-&gt;m_VnlTranslation *= -1;  // save -P'\n\n// from here proceed as normal\n// focalPoint = P-viewPlaneNormal, viewPlaneNormal is rotation[2]\nd-&gt;m_ViewPlaneNormal[0] = d-&gt;m_VnlRotation(2,0);\nd-&gt;m_ViewPlaneNormal[1] = d-&gt;m_VnlRotation(2,1);\nd-&gt;m_ViewPlaneNormal[2] = d-&gt;m_VnlRotation(2,2);\n\nd-&gt;m_vtkCamera-&gt;SetPosition(d-&gt;m_VnlTranslation[0], d-&gt;m_VnlTranslation[1], d-&gt;m_VnlTranslation[2]);\n\nd-&gt;m_vtkCamera-&gt;SetFocalPoint( d-&gt;m_VnlTranslation[0] - d-&gt;m_ViewPlaneNormal[0],\n                               d-&gt;m_VnlTranslation[1] - d-&gt;m_ViewPlaneNormal[1],\n                               d-&gt;m_VnlTranslation[2] - d-&gt;m_ViewPlaneNormal[2] );\nd-&gt;m_vtkCamera-&gt;SetViewUp( d-&gt;m_VnlRotation(1,0), d-&gt;m_VnlRotation(1,1), d-&gt;m_VnlRotation(1,2) );\n</code></pre>\n\n<p><strong>And finally do a clipping range reset</strong>:</p>\n\n<pre><code>d-&gt;m_Renderer-&gt;ResetCameraClippingRange();\n</code></pre>\n\n<p>Hope this helps. I don't have the time to explain more details. Especially the last code (applying the extrinsics to the camera) has some implications which are connect to the coordinate system orientation. But that worked for me.</p>\n\n<p>Best Michael</p>\n",
        "system": ""
    },
    {
        "instruction": "32 bit RGBA numpy array from PIL image",
        "input": "",
        "output": "<h3>1. Indexing</h3>\n\n<p>You're misunderstanding the way that NumPy indexes images. NumPy prefers <a href=\"http://en.wikipedia.org/wiki/Row-major_order\" rel=\"noreferrer\">row-major indexing</a> (<em>y</em>, <em>x</em>, <em>c</em>) for images <a href=\"http://docs.scipy.org/doc/numpy/reference/internals.html#multidimensional-array-indexing-order-issues\" rel=\"noreferrer\">for the reasons described here</a>:</p>\n\n<blockquote>\n  <p>The drawback of [column-major indexing] is potential performance penalties. It\u2019s common to access the data sequentially, either implicitly in array operations or explicitly by looping over rows of an image. When that is done, then the data will be accessed in non-optimal order. As the first index is incremented, what is actually happening is that elements spaced far apart in memory are being sequentially accessed, with usually poor memory access speeds.</p>\n</blockquote>\n\n<p>If you prefer to use column-major indexing (<em>x</em>, <em>y</em>, <em>c</em>) and don't mind the potential performance penalty, then use <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html\" rel=\"noreferrer\"><code>numpy.transpose</code></a> to permute the indices:</p>\n\n<pre><code>np.array(image).transpose((1, 0, 2))\n</code></pre>\n\n<p>But the NumPy documentation recommends that you just get used to it:</p>\n\n<blockquote>\n  <p>We recommend simply learning to reverse the usual order of indices when accessing elements of an array. Granted, it goes against the grain, but it is more in line with Python semantics and the natural order of the data.</p>\n</blockquote>\n\n<h3>2. Colour channels</h3>\n\n<p>The third axis of the array gives you the 4 colour channels in each pixel, here the values (<em>red</em>, <em>green</em>, <em>blue</em>, <em>alpha</em>). This is more useful for most applications than a single 32-bit number: for example, you can extract the alpha channel by writing <code>image[...,3]</code>.</p>\n\n<p>If you really want the 32-bit colour values, then you can use the <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.view.html#numpy.ndarray.view\" rel=\"noreferrer\"><code>ndarray.view</code> method</a> to get a view of the same image data with a different <code>dtype</code> and then use <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape\" rel=\"noreferrer\"><code>numpy.reshape</code></a> to drop the last axis (which is now redundant):</p>\n\n<pre><code>a = np.array(image)\na.view(dtype=np.uint32).reshape(a.shape[:-1])\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Compute edge for intensity decreases only",
        "input": "",
        "output": "<p>You can write a custom filter and use cvFilter2D (2D convolution). </p>\n\n<p>To give a very simple example, the convolution kernel {1 0 -1;1 0 -1; 1 0 -1} is a 3x3 filter that can highlight intensity decreases going from left to right. You can threshold the result to get the edges.</p>\n\n<p>You will have to select the right size of the kernel, and also the right values, to suit your images.</p>\n\n<p><a href=\"http://hasanaga.info/tag/cvfilter2d-opencv-example/\" rel=\"nofollow\">Here</a> is a good link that shows how to use cvFilter2d:</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV drawMatches do nothing",
        "input": "",
        "output": "<p>You have not matched any points. Look at this example: <a href=\"http://docs.opencv.org/doc/user_guide/ug_features2d.html\" rel=\"nofollow\">http://docs.opencv.org/doc/user_guide/ug_features2d.html</a>.</p>\n\n<p>You need to extract descriptors and then match them with FLANN for instance. Then you can draw your matches ;)</p>\n",
        "system": ""
    },
    {
        "instruction": "How can i match gestures and compare them?",
        "input": "",
        "output": "<p>You can use a classification machine learning algorithm like <a href=\"http://en.wikipedia.org/wiki/Logistic_regression\" rel=\"nofollow\">logistic regression</a>.<br>\nThis algorithm tries to minimize the cost function to predict a picture input similarity to all classes (all gestures in your case) and it'll pick the most similar class and give you that. for pictures you should use each pixel as a feature for your data.<br>\nAfter feeding your algorithm with enough training set it can classify your picture into one of the gestures, and as you said you are working with webcam images the running time wouldn't be that much.<br>\nHere is a great <a href=\"https://www.youtube.com/watch?v=LLx4diIP83I\" rel=\"nofollow\">video</a> for learning logistic regression by professor Andrew Ng of Stanford.</p>\n",
        "system": ""
    },
    {
        "instruction": "Feature Extraction : SPTS and CAPP",
        "input": "",
        "output": "<p>The paper is indeed not very good on providing the relevant  information in an easily accessible way. </p>\n\n<p>SPTS: \"similarity normalized shape features\"</p>\n\n<p>CAPP: \"Canonical normalized appearance\"</p>\n\n<p>CAPP: They cite 3: ( A. Ashraf, S. Lucey, J. Cohn, T. Chen, Z. Ambadar, K. Prkachin,\nP. . Solomon, and B.-J. Theobald. The painful face: pain expression\nrecognition using active appearance models. In Proceedings of the\n9th international conference on Multimodal interfaces, pages 9\u201314,\nNagoya, Aichi, Japan, 2007. ACM. 5)</p>\n",
        "system": ""
    },
    {
        "instruction": "How can I improve my sobel operator edge detection?",
        "input": "",
        "output": "<p>Your kernel in the y direction seems to be incorrect, it should be</p>\n\n<p>[  1, 2, 1;\n   0, 0, 0;\n  -1, -2, -1];</p>\n\n<p>Further, if you want to improve edge detection, you can look into Hysteresis, its an easy way to complete some obvious contours in an image which might be missed out</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Canny_edge_detector#Tracing_edges_through_the_image_and_hysteresis_thresholding\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Canny_edge_detector#Tracing_edges_through_the_image_and_hysteresis_thresholding</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Result showing lining on image",
        "input": "",
        "output": "<p>Check if your both images are of same size's if not then first <code>resize</code> both images and then perform operation , you need to perform <code>resize</code> operation on java end rather than on <code>jni</code> side . Resultant image should be of size of input image but second image size should also be of input image size , all images size matter's in this case.</p>\n",
        "system": ""
    },
    {
        "instruction": "Camera Calibration on MATLAB",
        "input": "",
        "output": "<p>I assume you are working with 1 just camera, so only intrinsic parameters of the camera are in the game.</p>\n\n<p>(1),(2). Once your camera is calibrated, you need to use this parameters to <strong>undistort</strong> the image. Cameras dont take the images as they are in reality as the lenses distort it a bit, and the calibration parameters are for fixing the images. More in <a href=\"http://en.wikipedia.org/wiki/Distortion_(optics)\" rel=\"nofollow\">wikipedia</a>.</p>\n\n<p>About when you need to recalibrate the camera (3): if you set up the camera and don't change its focus, then you can use the same calibration parameters, but once you change the focal distance a recalibration is necessary.</p>\n\n<p>(4) As long as you dont change the focal distance and you are not using a stereo camera sistem you can change your camera freely.</p>\n",
        "system": ""
    },
    {
        "instruction": "What to include for cv::meanShift function?",
        "input": "",
        "output": "<p>it's in the <a href=\"http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html#meanshift\" rel=\"nofollow\">video</a> module, so:</p>\n\n<pre><code>#include &lt;opencv2/video/video.hpp&gt;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Detect and plot all blobs larger than 100 pixel",
        "input": "",
        "output": "<p><strong>Approach #1</strong> No-loop <code>bwlabel</code> based -</p>\n\n<pre><code>threshold = 100;\n[L,num] = bwlabel( Iobr );\ncounts = sum(bsxfun(@eq,L(:),1:num));\nB1 = bsxfun(@eq,L,permute(find(counts&gt;threshold),[1 3 2]));\nIobr = sum(B1,3)&gt;0;\nfigure, imshow(Iobr);\n</code></pre>\n\n<hr>\n\n<p><strong>Approach #2</strong> <code>bwconncomp</code> based -</p>\n\n<pre><code>threshold = 100;\nCC = bwconncomp(Iobr);\ncount_pixels = cellfun(@numel,CC.PixelIdxList);\nfor k = 1:numel(count_pixels)\n    if count_pixels(k)&lt;=threshold\n        Iobr(CC.PixelIdxList{k}) = 0;\n    end\nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "how to use Vlfeat in Octave in window 7?",
        "input": "",
        "output": "<p>The code in the second box is meant to be executed by the command line. </p>\n\n<p>Under Windows you would have to replace <code>cd</code> with <code>chdir</code> and you would need <code>make</code> installed.</p>\n\n<p>You also need to find where <code>mkoctfile</code> is located on Windows and provide that information to make like it is done in the example.</p>\n\n<h1>Compiling on Windows</h1>\n\n<p>There is also good information on how to compile on Windows on the VLfeat website <a href=\"http://www.vlfeat.org/compiling-windows.html\" rel=\"nofollow noreferrer\">here</a>. Try that first, and add the <code>MKOCTFILE</code> parameter as well. (You might have to edit it into the NMake file. )</p>\n\n<h1>Conclusion</h1>\n\n<p>It doesn't seem like there is a proper option. Compilation under Windows is done using NMake. The configuration of this is in <code>Makefile.mak</code>, the configuration for UNIX is in <code>Makefile</code> which in turn loads (among others) <code>make/octave.mak</code>. There is no provision for Octave in the Windows <code>Makefile.mak</code>. </p>\n\n<p>Options:</p>\n\n<ol>\n<li>Fix it yourself. It shouldn't be too hard. Just compare what is done to Matlab under Windows, see what is done to Octave under UNIX and change the Windows makefiles to compile the Octave API. (Ideally, open a pull request on VLfeat's Github with the working code.)</li>\n<li>Ask the main author of VLFeat for some help. I've contacted him in the past with bug fixes and he was responsive to that.</li>\n</ol>\n\n<p>Duplicate (unanswered): <a href=\"https://stackoverflow.com/questions/25391974/running-vlfeat-setup-using-octave-os-windows-7?rq=1\">Running VLFeat setup using OCTAVE (OS: Windows 7)</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Make mask from edge detection on image?",
        "input": "",
        "output": "<p>If floodfill does not provide you with a sufficient mask, another way could be to take the edge image from figure 1 and apply a dilation operator and then a closing operator. The mask will be slightly larger than the original due to the dilation although the dilation helps in closing black spots when applying the closing operator.</p>\n\n<p>This is the result I obtained (I do not have a high enough rep to post the image in the answer. Here is the link):\n<a href=\"http://tinypic.com/view.php?pic=33jmpao&amp;s=8#.U_cHm_mSz9s\" rel=\"nofollow\">http://tinypic.com/view.php?pic=33jmpao&amp;s=8#.U_cHm_mSz9s</a></p>\n\n<p>The link below may also be useful to you.\n<a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\" rel=\"nofollow\">http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html</a></p>\n\n<p>The code I used:</p>\n\n<pre><code>// Dilation\nMat se = getStructuringElement(CV_SHAPE_ELLIPSE, Size(9, 9));\ndilate(edge_image, dst, se, Point(-1,-1), 1);\n\n// Closing\nMat closed;\nMat element = getStructuringElement(MORPH_ELLIPSE, Size(19, 19));\nmorphologyEx(dst, closed, MORPH_CLOSE, element, Point(-1,-1), 3);\n</code></pre>\n\n<p>This is my first answer on stackoverflow. I hope it helps and good luck! :)</p>\n",
        "system": ""
    },
    {
        "instruction": "Iris detection in still images",
        "input": "",
        "output": "<p>I think the problem can be split into two parts:</p>\n\n<ol>\n<li>Localisation of the iris regions</li>\n<li>Estimating the colour</li>\n</ol>\n\n<p>Step one is time consuming, but I have done this at my workplace. You can train a Haar-cascade classifier for iris images (grayscale), and localise the iris within the eye-region returned be the cascade classifier for the eyes. If you already have a collection of face images, you can use them. Otherwise, try to collect as many samples as possible, with the same image quality as the images you want to use.</p>\n\n<p>Step two is relatively easy, but might not be \"very easy\" because of auto white balance etc.</p>\n\n<p>If you want a simpler approach, try detecting the white regions of the eye and using them to loc</p>\n",
        "system": ""
    },
    {
        "instruction": "How to do simple ball tracking?",
        "input": "",
        "output": "<p>This looks like a problem for the <a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_meanshift/py_meanshift.html\" rel=\"nofollow noreferrer\">CamShift algorithm</a>.</p>\n\n<p>CamShift is an algorithm for tracking objects. One common use of the algorithm is tracking an object with a unique color.</p>\n\n<p>You will first need to initialize the algorithm by taking an image of the ball, converting the image to HSV color-space, and tuning thresholds on the channels in a way that only the ball will be in the ranges (between the lower and upper threshold of each channel), while the background will not.</p>\n\n<p>The CamShift algorithm will search, in each frame, the location in which the most neighbor pixels are in those ranges. The locations are searched based on the location of the object in the previous frame (thus called tracking).</p>\n\n<p>Second, you can use <a href=\"http://en.wikipedia.org/wiki/Hough_transform\" rel=\"nofollow noreferrer\">Hough Transform</a> for circles in order to detect the round shape of the ball. This should give you an initial location of the ball, in the first frame, before tracking it through the next frames. Also, it will give you the exact boundary of the ball.</p>\n\n<p>Both of those algorithms are implemented in <a href=\"http://opencv.org/\" rel=\"nofollow noreferrer\">OpenCV</a>, which fortunatelly for you, has a few wrapper-libraries for <code>.net</code> - <a href=\"https://stackoverflow.com/questions/6910536/computer-vision-library\">See this thread</a>.</p>\n\n<p>Here's a guy who did this in <code>Python</code>, and it looks good: <a href=\"http://www.lirtex.com/robotics/fast-object-tracking-robot-computer-vision\" rel=\"nofollow noreferrer\">Fast Object Tracking \u2013 Robot Computer Vision</a></p>\n\n<p>Hope this is enough to get you on your way. Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Local Histogram Separation Energy Implementation",
        "input": "",
        "output": "<p>I tried this - no idea if its correct - its has no histogram smoothing (I dont think it is necessary)</p>\n\n<pre><code>if type==3  % Set up for bhatt\n\nF=zeros(size(idx,1),2);\n\nfor i = 1:numel(idx)  \n\nimg2 = img(yneg(i):ypos(i),xneg(i):xpos(i)); \nP = phi(yneg(i):ypos(i),xneg(i):xpos(i)); \n\nupts = find(P&lt;=0);            %local interior\nAin = length(upts)+eps;\n[u,~] = hist(img2(upts),1:256);\n\nvpts = find(P&gt;0);             %local exterior\nAout = length(vpts)+eps;\n [v,~] = hist(img2(vpts),1:256); \n\n   Ap = Ain;\n   Aq = Aout;\n   In=Ap;\n   Out=Aq;\n    try\n     p = ((u))  ./ Ap + eps;\n     q = ((v)) ./ Aq + eps;\n     catch\n     g  \n    end\n\n\n B = sum(sqrt(p .* q));\n\n F(i)=B.*((1/numel(In))-(1/numel(Out)))+(0.5.*(1/numel(In)*(q(img(idx(i))+1)...      /p(img(idx(i))+1))))-(0.5.*(1/numel(Out)*(p(img(idx(i))+1)/q(img(idx(i))+1))));\nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "The implementation of HOG Descriptor",
        "input": "",
        "output": "<p>Well,</p>\n\n<ol>\n<li>I haven't used HoG specifically but judging from other descriptors, usually they are not the same. The actual feature is the descriptor while the detector as you can guess it is used to detect (to locate) the feature.</li>\n<li>There is no point in finding interesting points and then extract features from the whole image. (Again) I don't know how HoG exactly work but usually either you extract features from the whole image or first detect interesting points and then extract from them. (Some features cannot be extracted from the whole image though).</li>\n<li>Judging from the answers in <a href=\"http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\" rel=\"nofollow\">wikipedia</a> the usual approach involves the calculation of the local features and then use the blocks to <code>group the cells together into larger, spatially connected blocks</code> to improve illumination etc robustness, so first you calculate HoG on cell level and then use the block to groug them together.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Detecting blobs of uniform colour with openCV",
        "input": "",
        "output": "<p>1) Convert to HSV and take H or take gray-scaled form. Apply median filter to smooth the fields :P if images are high-resolution.</p>\n\n<p>2) Extract histogram and find all the peaks. These peaks indicate the different colored fields.</p>\n\n<p>3) (A) Now you can use simple thresholding around these peaks-value and then find canny edges for trapezium or similar shapes.</p>\n\n<p>--OR--</p>\n\n<p>(B) Find canny-edges around the peak value ie for peak with maxima value x, find edge for range of (x - dx) to (x + dx) where dx is a small value to be find experimentally.</p>\n\n<p>4) Now you can extract count of contours at different levels/peaks.</p>\n\n<p>I haven't added code because language is not specified and all these constructs are readily available in OpenCV. Its fun to learn. Feel free to ask further. Happy Coding.</p>\n",
        "system": ""
    },
    {
        "instruction": "Isn&#39;t BMP in opencv lossless?",
        "input": "",
        "output": "<p>You're simply misinterpreting the binary data of the bitmaps. The source bitmap is 24 bits per pixel, whereas the new bitmap is 8 bits per pixel. Notice how on the source, each value is repeated three times. (3 times 8 bit bytes = 24 bits) If you just combine each set of three bytes into a single byte, it matches the target bitmap. You also appear to be truncating 1/3 of the binary data in the source bitmap you posted.</p>\n\n<p>So this translation is lossless <strong>only if</strong> the source truly is completely greyscale. If OpenCV is indeed implicitly converting it from 24bpp to 8bpp, then it likely is a lossless operation.</p>\n",
        "system": ""
    },
    {
        "instruction": "image shows result in VS2010 but not showing output in Android using OpenCV",
        "input": "",
        "output": "<p><strong>Part 1</strong></p>\n\n<p>I think your problem is setting alpha to <code>0</code> (transparent) in this line:</p>\n\n<pre><code>img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 0));\n</code></pre>\n\n<p>Change it to</p>\n\n<pre><code>img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 255));\n</code></pre>\n\n<hr>\n\n<p>Since the only difference is the alpha channel, that is the first place to look. If either img1 or img2 has 0 alpha your result will be completely transparent!\nThe suggested way to handle this, according to <a href=\"https://stackoverflow.com/questions/4689096/how-to-handle-alpha-in-a-manual-overlay-blend-operation\">this question</a>, is\n<code>result.alpha = 1 - (1 - target.alpha) * (1 - blend.alpha)</code></p>\n\n<p>Try this:</p>\n\n<pre><code> void sow(Mat&amp; img1, Mat&amp; img2, Mat&amp; out)\n{\n    Mat result(img1.size(), CV_32FC4);                           \n\n    img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 255));\n\n    for (int i = 0; i &lt; img1.rows; ++i)\n    {\n        Vec4b* i1 = img1.ptr&lt;Vec4b&gt;(i);\n        Vec4b* i2 = img2.ptr&lt;Vec4b&gt;(i);\n        Vec4f* r  = result.ptr&lt;Vec4f&gt;(i);\n\n        for (int j = 0; j &lt; img1.cols; ++j)\n        {\n            Vec4b p1 = i1[j];\n            Vec4b p2 = i2[j];\n            Vec4f&amp; pr = r[j];\n\n            // Blend overlay color channels\n            for (int c = 0; c &lt; 3; ++c)\n            { \n                //Formula \n                float target = (float) p1[c] / 255.0f;\n                float blend = (float) p2[c] / 255.0f;\n                if (blend &gt; 0.5)\n                {\n                    pr[c] = 1 - (1 - target) * (1 - 2 * (blend - 0.5f));\n                }\n                else\n                {\n                    pr[c] = target * 2 * blend;\n                }\n            }\n            // Alpha channel\n            float target_alpha  = (float) p1[3] / 255.0f;\n            float blend_alpha = (float) p2[3] / 255.0f;\n            pr[3] = 1 - (1 - target_alpha) * (1 - blend_alpha);\n        }\n    }\n    result.convertTo(out, CV_8UC4, 255);\n}\n</code></pre>\n\n<p>With your input image this generates your desired result:</p>\n\n<p><img src=\"https://i.sstatic.net/5pnhD.jpg\" alt=\"enter image description here\"></p>\n\n<hr>\n\n<p>Also, the problem might be with your JNI code, check that the code calling <code>sow()</code> works correctly, just return <code>img1</code> &amp; <code>img2</code>:</p>\n\n<pre><code>void sow(Mat&amp; img1, Mat&amp; img2, Mat&amp; out)\n{\n    img1.copyTo(out);\n}\n</code></pre>\n\n<p>BTW, except for <code>img2</code> the call by reference on those Mats is superfluous. E.g.</p>\n\n<pre><code>void sow(Mat img1, Mat&amp; img2, Mat out)\n{\n    img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 255));\n    img2.copyTo(out);\n}\n</code></pre>\n\n<hr>\n\n<p><strong>Part 2</strong>\nThe reason that </p>\n\n<pre><code>void BL(Mat&amp; img1, Mat&amp; img2, Mat&amp; out)\n{\n    img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 0));\n    min(img1, img2, out);\n}\n</code></pre>\n\n<p>doesn't display anything is that the alpha channel in <code>out</code> is always set to <code>0</code> (completely transparent). Change the alpha to 255 in img2 and it should work:</p>\n\n<pre><code>void BL(Mat&amp; img1, Mat&amp; img2, Mat&amp; out)\n{\n    img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 255));\n    min(img1, img2, out);\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Calculating percentage of Bounding box overlap, for image detector evaluation",
        "input": "",
        "output": "<p>For axis-aligned bounding boxes it is relatively simple. \"Axis-aligned\" means that the bounding box isn't rotated; or in other words that the boxes lines are parallel to the axes. Here's how to calculate the IoU of two axis-aligned bounding boxes.</p>\n\n<pre><code>def get_iou(bb1, bb2):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) of two bounding boxes.\n\n    Parameters\n    ----------\n    bb1 : dict\n        Keys: {'x1', 'x2', 'y1', 'y2'}\n        The (x1, y1) position is at the top left corner,\n        the (x2, y2) position is at the bottom right corner\n    bb2 : dict\n        Keys: {'x1', 'x2', 'y1', 'y2'}\n        The (x, y) position is at the top left corner,\n        the (x2, y2) position is at the bottom right corner\n\n    Returns\n    -------\n    float\n        in [0, 1]\n    \"\"\"\n    assert bb1['x1'] &lt; bb1['x2']\n    assert bb1['y1'] &lt; bb1['y2']\n    assert bb2['x1'] &lt; bb2['x2']\n    assert bb2['y1'] &lt; bb2['y2']\n\n    # determine the coordinates of the intersection rectangle\n    x_left = max(bb1['x1'], bb2['x1'])\n    y_top = max(bb1['y1'], bb2['y1'])\n    x_right = min(bb1['x2'], bb2['x2'])\n    y_bottom = min(bb1['y2'], bb2['y2'])\n\n    if x_right &lt; x_left or y_bottom &lt; y_top:\n        return 0.0\n\n    # The intersection of two axis-aligned bounding boxes is always an\n    # axis-aligned bounding box\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n\n    # compute the area of both AABBs\n    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n    assert iou &gt;= 0.0\n    assert iou &lt;= 1.0\n    return iou\n</code></pre>\n\n<h2>Explanation</h2>\n\n<p><img src=\"https://i.sstatic.net/QU04v.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/n1AZj.png\" alt=\"enter image description here\"></p>\n\n<p>Images are from <a href=\"https://stackoverflow.com/a/41660682/562769\">this answer</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Accessing IP cameras with OpenCV",
        "input": "",
        "output": "<p>If you can use local network, then Yes, you should use fixed IP address....if the Camera SDK is compatiable with OpenCV this you dont have have to worry about this and you can call <code>VideoCapture</code> directly. Or, you could use the camera SDK to get frames directly and then copy these frames to opencv image format and use opencv. This should not be too complicated.  </p>\n",
        "system": ""
    },
    {
        "instruction": "Scale domains of SURF and BRISK points in MATLAB",
        "input": "",
        "output": "<p>The scales are represented differently between BRISK and SURF. The reported BRISK scale is based on the radius of the BRISK sampling pattern. The SURF scale is represented by the detection scale \"s\" of the box filters used for keypoint detection.  </p>\n\n<p>Details about this can be found in the original references:</p>\n\n<p>(BRISK) <a href=\"http://www.robots.ox.ac.uk/~vgg/rg/papers/brisk.pdf\" rel=\"nofollow\">http://www.robots.ox.ac.uk/~vgg/rg/papers/brisk.pdf</a></p>\n\n<p>(SURF) <a href=\"http://www.vision.ee.ethz.ch/~surf/eccv06.pdf\" rel=\"nofollow\">http://www.vision.ee.ethz.ch/~surf/eccv06.pdf</a></p>\n\n<p>SURF estimates the orientation from a region with radius of 6s. So when converting BRISK scale information into SURF, the BRISK scale is divided by 6 to make sure the orientation is estimated over a similar sized region. When SURF is converted to BRISK, the 10 comes from the fact that SURF descriptors are extracted over a 20s window so multiplying by 10 makes sure the BRISK sampling pattern covers most of the 20s window so the descriptor captures most of the same information.</p>\n\n<p>These conversions can be done in other ways too. Just create a SURFPoints object from the information in the BRISKPoints object.  </p>\n\n<p>Having said that, BRISK is a corner based detector/descriptor while SURF is blob based detector/descriptor, so you can't really get a \"correct\" mapping that always makes sense. </p>\n",
        "system": ""
    },
    {
        "instruction": "vImage has same performance as normal loop with dispatch_apply",
        "input": "",
        "output": "<p>10x10 is a very small image.  You easily could be spending most of your time in overhead / malloc. Instruments time trace should help determine where the time is going.  </p>\n\n<p>The vector ALU on 4s is also half the width of a 5 or 5s, so doesn't provide as much of a win over scalar.</p>\n",
        "system": ""
    },
    {
        "instruction": "BRISK and FREAK descriptors in MATLAB",
        "input": "",
        "output": "<p>Unfortunately the current MATLAB implementation only computes the 512 bit BRISK descriptor.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to keep a drawn rectangle in the same place as the camera moves around",
        "input": "",
        "output": "<p>What what you are trying to achieve is possible, but its going to take some research and work on your part. One possible solution to your problem is to use Optical Flow (<a href=\"http://en.wikipedia.org/wiki/Optical_flow\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Optical_flow</a>) analysis to monitor the apparent motion of objects in your cameras view. You could use the resulting optical flow field to apply a \"correction\" to the positions of the corners of your rectangle between each frame. Here is a link to the OpenCV documentation for their optical flow functions: </p>\n\n<p><a href=\"http://docs.opencv.org/modules/gpu/doc/video.html\" rel=\"nofollow\">http://docs.opencv.org/modules/gpu/doc/video.html</a></p>\n\n<p>If the particular device you are using has a gyroscope, and gps/ins you might also use this data to supplement the optical flow data. Let me know how it goes, sounds like a really fun project!</p>\n",
        "system": ""
    },
    {
        "instruction": "How to convert 3D point cloud (extracted from 3D sparse reconstruction) to millimeters?",
        "input": "",
        "output": "<p>The formula you wrote is valid only in the special case when the image planes of the two cameras are on the same geometrical plane, and the motion from one to the other is a translation parallel to one of the image axes. </p>\n\n<p>In the general case you'll need to triangulate actual rays in 3D space, using one of the techniques described in that book (it has a whole chapter on reconstruction). The reconstruction will be metrical if your calibration is. In particular, if the coordinate transform between the cameras has a translation vector whose units are meters (or millimeters, or inches, ...). </p>\n",
        "system": ""
    },
    {
        "instruction": "Choosing Lines From Hough Lines",
        "input": "",
        "output": "<p>OpenCVs hough transform really could use some better Non-Maximum Suppression. Without that, you get this phenomenon of duplicate lines. Unfortunately I know of no easy way to tune that, besides reimplementing your own hough transform. (Which is a valid option. Hough transform is fairly simple)</p>\n\n<p>Fortunately it is easy to fix in post-processing:</p>\n\n<p>For the non-probabilistic hough transform, OpenCv will return the lines in order of their confidence, with the strongest line first. So simply take the first four lines that differ strongly in either rho or theta.</p>\n\n<ul>\n<li>so, add the first line found by HoughLines into a new List: <em>strong_lines</em></li>\n<li>for each line found by HoughLines:\n<ul>\n<li>test whether its rho <em>and</em> theta are close to any <em>strong_line</em> (e.g. rho is within 50 pixels and theta is within 10\u00b0 of the other line)</li>\n<li>if not, put it into the list of <em>strong_lines</em></li>\n<li>if you have found 4 <em>strong_lines</em>, break</li>\n</ul></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: what is the difference between these 2 haar cascade data sets?",
        "input": "",
        "output": "<p>I think this web site have the answer : <a href=\"http://answers.opencv.org/question/39437/opencv-what-is-the-difference-between-these-2-haar/\" rel=\"nofollow\">OpenCV</a></p>\n\n<p>The diference is eepending on there train data, so that, if you want to select a suit classifier, I prefer you try both two to find a better result.</p>\n",
        "system": ""
    },
    {
        "instruction": "Halide Tuple usage",
        "input": "",
        "output": "<p>Currently all outputs (elements of a Realization) must have the same number of dimensions, mins, and extents. Only the type of the elements can vary. Andrew has a plan to remove this restriction, but I would not depend on it for anything less than six to nine months out.</p>\n",
        "system": ""
    },
    {
        "instruction": "Point/Area of gaze with C++ and Opencv",
        "input": "",
        "output": "<p>I think my Optimeyes project here:</p>\n\n<p><a href=\"https://github.com/LukeAllen/optimeyes\" rel=\"noreferrer\">https://github.com/LukeAllen/optimeyes</a></p>\n\n<p>does what you're looking for: pupil detection and gaze tracking. Its included \"Theory Paper\" pdf discusses the principles of operation, and has references to other papers. The project was written using the Python version of OpenCV but you're welcome to port it to C++!</p>\n",
        "system": ""
    },
    {
        "instruction": "human activity recognition in a long unsegmented video sequence",
        "input": "",
        "output": "<p>Activity recognition in single activity sequence is done using deep learning. Multiple action detection in video sequence is also done. All these come under the Activity-net challenge which is hosted almost every year. In the github repos given as references, you can find all the classes that the model is able to recognise, if the class that you are looking for(bricklaying, et al) is not there and if you have proper training dataset, code for retraining the network is also given. One can use that to include those required classes. </p>\n\n<p>References: </p>\n\n<p><a href=\"https://github.com/yjxiong/temporal-segment-networks\" rel=\"nofollow noreferrer\">Temporal Segment networks - For single action recognition</a></p>\n\n<p><a href=\"https://github.com/imatge-upc/activitynet-2016-cvprw\" rel=\"nofollow noreferrer\">Multiple Activity Detection</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How to implement integral image on sliding window detection?",
        "input": "",
        "output": "<p>The integral image is most suited for the Haar-like features. Using it for HOG or LBP would be tricky.  I would suggest to first get your algorithm working, and then think about optimizing it.</p>\n\n<p>By the way, the Computer Vision System Toolbox includes the <code>extractHOGFeatures</code> function, which would be helpful.  Here's an <a href=\"http://www.mathworks.com/help/vision/examples/digit-classification-using-hog-features.html\" rel=\"nofollow\">example of training a HOG-SVM classifier to recognize hand-written digits</a>.  Also there is a <code>vision.PeopleDetector</code> object, which uses a HOG-SVM classifier to detect people.  You could either use it directly for your project, or use it to evaluate performance of your own algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "Align blurred and sharp rectangles",
        "input": "",
        "output": "<p>you can try affine registration, resize the bigger image and use DROP <a href=\"http://www.mrf-registration.net/deformable/index.html\" rel=\"nofollow\">http://www.mrf-registration.net/deformable/index.html</a>, it does more sophisticated stuff like discrete optimization using patch based matching, followed by b-spline interpolation to deform images</p>\n",
        "system": ""
    },
    {
        "instruction": "Open Cv filter results black lining on image",
        "input": "",
        "output": "<p>Your images are four channel. That means your indexing is wrong. In <code>blending_Lighten()</code></p>\n\n<pre><code>            float target = float(img1.at&lt;uchar&gt;(i, 3 * j + c)) / 255.;\n            float blend = float(img2.at&lt;uchar&gt;(i, 3 * j + c)) / 255.;\n            result.at&lt;float&gt;(i, 3 * j + c) = max(target, blend);\n</code></pre>\n\n<p>should be</p>\n\n<pre><code>            float target = float(img1.at&lt;uchar&gt;(i, 4 * j + c)) / 255.;\n            float blend = float(img2.at&lt;uchar&gt;(i, 4 * j + c)) / 255.;\n            result.at&lt;float&gt;(i, 4 * j + c) = max(target, blend);\n</code></pre>\n\n<p>BTW, you don't set the alpha channel in colors.</p>\n\n<p>And, it seems <code>blending_Lighten()</code> could be great simplified:</p>\n\n<pre><code>void blending_Lighten(Mat&amp; img1, Mat&amp; img2, Mat&amp; out)\n{\n    img2 = Mat(img1.size(), img1.type(), Scalar(186, 44, 28, 0));\n    max(img1, img2, out);\n}\n</code></pre>\n\n<p>Lastly, before returning from <code>Java_com_example_opencvfilters_MainActivity_filter()</code>, you should call  <code>env-&gt;ReleaseIntArrayElements()</code> for all things for which you called <code>GetIntArrayElements()</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to determine number of frames using vision.VideoFileReader",
        "input": "",
        "output": "<pre><code>videoFReader = vision.VideoFileReader(FILENAME) \nFrames = 0;\nwhile ~isDone(videoFReader)\n  I = step(videoFReader);\n  Frames = Frames+1;\nend\n</code></pre>\n\n<p>Update : There is an alternative solution which does not require iteration over all frames in the video .</p>\n\n<pre><code>videoSource2=VideoReader(FILENAME);\nframes=read(videoSource2);\ntotalFrameNumber=size(frames,4);\n</code></pre>\n\n<p>Hope this helps..</p>\n",
        "system": ""
    },
    {
        "instruction": "how to register face by landmark points",
        "input": "",
        "output": "<p>If you have a lot of points, say 68..then you can perform delaunay triangulation and then perform piecewise affine warp. </p>\n\n<p>If you have much fewer than 68, say 5 or 6, then you can try least square fitting of affine or perspective transform. I believe you can use the <code>findhomography</code> function of opencv and then use <code>perspectivetransform</code> function to perform this step.</p>\n",
        "system": ""
    },
    {
        "instruction": "Can a Kalman Filter predict where a tracked object will be after colliding with a wall?",
        "input": "",
        "output": "<p>A Kalman filter is able to handle linear process functions only, so it probably won't be a good choice since billard-ball mechanics don't fit in that criterion.</p>\n\n<p>For such a task I would probably try to use a particle filter (or recursive Monte-carlo) with a process variable (position, velocity). This is a common model used for robot navigation for ex. It will be able to predict several steps into the future adjusting the uncertainty depending on collisions and such. </p>\n",
        "system": ""
    },
    {
        "instruction": "Basler Pylon 4 SDK and OPENCV 2.4.8, Linux simple viewer",
        "input": "",
        "output": "<pre><code>// Grab.cpp\n/*\nNote: Before getting started, Basler recommends reading the Programmer's Guide topic\nin the pylon C++ API documentation that gets installed with pylon.\nIf you are upgrading to a higher major version of pylon, Basler also\nstrongly recommends reading the Migration topic in the pylon C++ API documentation.\n\nThis sample illustrates how to grab and process images using the CInstantCamera class.\nThe images are grabbed and processed asynchronously, i.e.,\nwhile the application is processing a buffer, the acquisition of the next buffer is done\nin parallel.\n\nThe CInstantCamera class uses a pool of buffers to retrieve image data\nfrom the camera device. Once a buffer is filled and ready,\nthe buffer can be retrieved from the camera object for processing. The buffer\nand additional image data are collected in a grab result. The grab result is\nheld by a smart pointer after retrieval. The buffer is automatically reused\nwhen explicitly released or when the smart pointer object is destroyed.\n*/\n\n#include &lt;pylon/PylonIncludes.h&gt;\n#ifdef PYLON_WIN_BUILD\n#include &lt;pylon/PylonGUI.h&gt;\n#endif\n\n#include \"opencv2/highgui/highgui.hpp\"\n#include \"opencv2/imgproc/imgproc.hpp\"\n#include \"opencv2/core/core.hpp\"\n\n\n  using namespace cv;\n\n  // Namespace for using pylon objects.\n  using namespace Pylon;\n\n\n  // Namespace for using cout.\n  using namespace std;\n\n// Number of images to be grabbed.\nstatic const uint32_t c_countOfImagesToGrab = 100;\n\nint main(int argc, char* argv[])\n{\n// The exit code of the sample application.\nint exitCode = 0;\n\n// Automagically call PylonInitialize and PylonTerminate to ensure \n// the pylon runtime system is initialized during the lifetime of this object.\nPylon::PylonAutoInitTerm autoInitTerm;\n\n\nCGrabResultPtr ptrGrabResult;\nnamedWindow(\"CV_Image\",WINDOW_AUTOSIZE);\ntry\n{\n\n    CInstantCamera camera( CTlFactory::GetInstance().CreateFirstDevice());\n    cout &lt;&lt; \"Using device \" &lt;&lt; camera.GetDeviceInfo().GetModelName() &lt;&lt; endl;\n     camera.Open();\n\n    GenApi::CIntegerPtr width(camera.GetNodeMap().GetNode(\"Width\"));\n     GenApi::CIntegerPtr height(camera.GetNodeMap().GetNode(\"Height\"));\n     Mat cv_img(width-&gt;GetValue(), height-&gt;GetValue(), CV_8UC3);\n\n    camera.StartGrabbing();\n    CPylonImage image;\n    CImageFormatConverter fc;\n     fc.OutputPixelFormat = PixelType_RGB8packed;\n\n    while(camera.IsGrabbing()){\n        camera.RetrieveResult( 5000, ptrGrabResult, TimeoutHandling_ThrowException);\n        if (ptrGrabResult-&gt;GrabSucceeded()){\n                 fc.Convert(image, ptrGrabResult);\n\n                cv_img = cv::Mat(ptrGrabResult-&gt;GetHeight(),     ptrGrabResult-&gt;GetWidth(), CV_8UC3,(uint8_t*)image.GetBuffer());\n                imshow(\"CV_Image\",cv_img);\n                  waitKey(1);\n                if(waitKey(30)==27){\n                      camera.StopGrabbing();\n                }\n        }\n    }\n\n}\n\ncatch (GenICam::GenericException &amp;e)\n{\n    // Error handling.\n    cerr &lt;&lt; \"An exception occurred.\" &lt;&lt; endl\n    &lt;&lt; e.GetDescription() &lt;&lt; endl;\n    exitCode = 1;\n}\n\n// Comment the following two lines to disable waiting on exit\ncerr &lt;&lt; endl &lt;&lt; \"Press Enter to exit.\" &lt;&lt; endl;\nwhile( cin.get() != '\\n');\n\nreturn exitCode;\n }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "ROC curves/classification accuracy, what types of evaluation metric to use when?",
        "input": "",
        "output": "<p>Second sometimes also called \"Localization\".</p>\n\n<p>You have ROC curves, when you have some parameter to change - to draw it. \nOtherwise you simply have classification error and confusion matrix.</p>\n\n<p>You can take an example from results of PASCAL VOC <a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/results/index.html\" rel=\"nofollow\">http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/results/index.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab - Creating a generic matching template for pattern detection with the available set of images",
        "input": "",
        "output": "<p>You can train a SVM model (<a href=\"http://www.mathworks.com/help/stats/support-vector-machines-svm.html\" rel=\"nofollow\">http://www.mathworks.com/help/stats/support-vector-machines-svm.html</a>) on the images you have with the matched images as positive samples and the unmatched images as negative samples. Finally you will get ONE template.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can I deal or track only one face while I have detected all the faces in front of camera?",
        "input": "",
        "output": "<p>Here's an <a href=\"http://www.mathworks.com/help/vision/examples/face-detection-and-tracking-using-the-klt-algorithm.html\" rel=\"nofollow\">example of how you can track one face</a>.  However, if you detect multiple faces, you would need some way for your program to decide, which face to track.</p>\n\n<p>Alternatively, you can <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/47105-detect-and-track-multiple-faces\" rel=\"nofollow\">track all faces</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect non-closed contour on opencv",
        "input": "",
        "output": "<p>There are multiple possible solutions.</p>\n\n<p>The simplest one may be:</p>\n\n<ul>\n<li>if FindContours does not find a closed contour, repeat the canny filter with a slightly decreased <code>low_threshold</code>, until you find a closed contour. If the closed contour has roughly the right size and shape, it is a card. The <a href=\"https://stackoverflow.com/a/22242203/145999\">answer linked by Haris</a> explains how to check whether a contour is closed</li>\n</ul>\n\n<p>Another rather simple solution:</p>\n\n<ul>\n<li>Don't apply Canny to the image at all. Execute findContours on the otsu thresholded image. Optionally use morphological opening and closing on the thresholded image to remove noise before findContours</li>\n</ul>\n\n<p>FindContours does not need an edge image, it is usually executed with a thresholded image. I don't know your source image, so I cannot say how good this would work, but you would definitely avoid the problem of holes in the shape.</p>\n\n<p>If the source image does not allow this, then the following may help:</p>\n\n<ul>\n<li>use <a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_watershed/py_watershed.html\" rel=\"noreferrer\">watershed</a> to separate the card from the background. Use a high threshold to get some seed pixels that are definitely foreground and a low threshold to get pixels that are definitely background, then grow those two seeds using <code>cv:watershed()</code>.</li>\n</ul>\n\n<p>If the background in that image is the same color as the card, then the previous two methods may not work so well. In that case, your best bet may be the solution suggested by Micka:</p>\n\n<ul>\n<li>use <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html\" rel=\"noreferrer\">hough transform</a> to find the 4 most prominent lines in the image. Form a rectangle with these 4 lines.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Measure similar information using Kullback-Leibler (KL) distance matlab code",
        "input": "",
        "output": "<p>Let answer one by one </p>\n\n<ol>\n<li>Log function in paper is log or log2 in matlab?</li>\n</ol>\n\n<p>Ans: This is natural log. In matlab, you just call <code>log()</code></p>\n\n<ol start=\"2\">\n<li>Log(0) is infinite, But we know that distribution result will return many 0 values. How to ignore it? </li>\n</ol>\n\n<p>Ans: To ignore the log of zero. You need add some small value as <code>log(x+eps)</code> or <code>log(x+(x==0)*eps)</code>, where <code>x</code> is your values</p>\n\n<ol start=\"3\">\n<li>Could you see my code? Is it correct? I am not sure about my implementation.</li>\n</ol>\n\n<p>Ans: Your code looks fine. You can base on my suggestion to improve your code. Good luck</p>\n",
        "system": ""
    },
    {
        "instruction": "Using Pickle vs database for loading large amount of data?",
        "input": "",
        "output": "<p>Use a database because it allows you to query faster. I've done this before. I would suggest against using cPickle. What specific implementation are you using? </p>\n",
        "system": ""
    },
    {
        "instruction": "HOG features for different image sizes and different aspect ratios",
        "input": "",
        "output": "<p>In your case, the images are of different size and aspect ratios, and since same class images have same aspect ratios, resizing is not a good option as it will stretch or shrink your specific class images changing their shape.</p>\n\n<p><strong>I would suggest you to pad your images with zeros</strong> in such a way that they all follow same size and same aspect ratio. </p>\n\n<p>For e.g: image1 : 320X240, image2 : 160X120, in this case aspect ratio is same but size is different, hence pad second image with extra 160 columns and 120 rows which will make it of same size. </p>\n\n<p>For e.g: image1: 320X240, image2: 320X200, in this case the aspect ratio has changed, hence pad image2 with extra 40 rows to make aspect ration the same.</p>\n\n<p>In this way the aspect ratio of a particular class image remains the same. </p>\n",
        "system": ""
    },
    {
        "instruction": "Laplacian of Gaussian in openCV, how to find zero crossings?",
        "input": "",
        "output": "<p>A zero crossing will occur when</p>\n\n<ul>\n<li>a value equals 0, or</li>\n<li>two adjacent values have opposite signs</li>\n</ul>\n\n<p>If you have something like</p>\n\n<pre><code>Laplacian(src, dst, CV_16S);\n</code></pre>\n\n<p>You just need to scan <code>dst</code> looking for those two cases</p>\n",
        "system": ""
    },
    {
        "instruction": "Accessing android phone Camera thru USB from my java openCV application in real time",
        "input": "",
        "output": "<p>If you only want to experiment and learn to use OpenCV, you can get an <a href=\"https://play.google.com/store/apps/details?id=com.pas.webcam&amp;hl=en\" rel=\"nofollow\">IP Camera app</a> for your phone and then read the image stream from your computer (<a href=\"http://robocv.blogspot.de/2012/01/using-your-ip-camera-with-opencv.html\" rel=\"nofollow\">here's</a> an example of how to do that). That is the easiest \"real time-ish\" solution I can think of.</p>\n",
        "system": ""
    },
    {
        "instruction": "Using Opencv how to detect a box in image while eliminating objects printed inside box?",
        "input": "",
        "output": "<p>I would suggest the following steps:</p>\n\n<p>1: Make a mask image by using <code>cv::inRange()</code> (<a href=\"http://docs.opencv.org/modules/core/doc/operations_on_arrays.html#inrange\" rel=\"nofollow\">documentation</a>) to select the background color. Then use <code>cv::not()</code> to invert this mask. This will give you only the box.</p>\n\n<p>2: If you're not concerned about shadow, depth effects making your measurment inaccurate you can proceed right away with trying to use <code>cv::findContours()</code> again. You select the biggest contour and store it's <code>cv::rotatedRect</code>.</p>\n\n<p>3: This <code>cv::rotatedRect</code> will give you a rotatedRect.size that defines the width en the height of your box in pixels</p>\n",
        "system": ""
    },
    {
        "instruction": "pymouse not running with opencv in python",
        "input": "",
        "output": "<p>From the <a href=\"http://docs.opencv.org/modules/highgui/doc/user_interface.html\" rel=\"nofollow\">official documentation of OpenCV</a>:</p>\n\n<blockquote>\n  <p>The function waitKey waits for a key event infinitely (when \\texttt{delay}\\leq 0 ) or for delay milliseconds, when it is positive.</p>\n</blockquote>\n\n<p>So when you put in your code:</p>\n\n<pre><code>cv2.waitKey(0)\n</code></pre>\n\n<p>It waits indefinitely for a user input i.e. if you don't type something the following lines won't be executed (that's why when you comment out the line the move action is executed)</p>\n",
        "system": ""
    },
    {
        "instruction": "Truncated gaussian kernel implementation Matlab,right?",
        "input": "",
        "output": "<p>Your definition of truncated gaussian kernel is different than how MATLAB truncates filter kernels, though it generally won't matter in practice for sizable <code>d</code>.</p>\n\n<p><code>fspecial</code> already returns truncated AND normalized filter, so the second case is redundant, because it generates exactly the same result as case 1.</p>\n\n<p>From MATLAB help:</p>\n\n<pre><code>H = fspecial('gaussian',HSIZE,SIGMA) returns a rotationally\nsymmetric Gaussian lowpass filter  of size HSIZE with standard\ndeviation SIGMA (positive). HSIZE can be a vector specifying the\nnumber of rows and columns in H or a scalar, in which case H is a\nsquare matrix.\nThe default HSIZE is [3 3], the default SIGMA is 0.5.\n</code></pre>\n\n<p>You can use <code>fspecial('gaussian',1,sigma)</code> to generate a 1x1 filter and see that it is indeed normalized.</p>\n\n<p>To generate a filter kernel that fits your definition, you need to make <code>B</code> in your second case a matrix that has ones in a circular area. A less strict (but nonetheless redundant in practice) solution is to use <code>fspecial('disk',size)</code> to truncate your gaussian kernel. Don't forget to normalize it in either case.</p>\n",
        "system": ""
    },
    {
        "instruction": "3 channels - Joint color histogram",
        "input": "",
        "output": "<p>Joint histograms are just histograms in higher dimensional space. <a href=\"http://en.wikipedia.org/wiki/Color_histogram\" rel=\"nofollow\">Color histogram</a> and joint color histogram are synonymous when referring to (binned) distribution of colors, with \"joint\" only emphasizing the polychromatic or multidimensional nature of the color bins, as opposed to one dimensional histograms whose sole spatial dimension being intensity.</p>\n",
        "system": ""
    },
    {
        "instruction": "Use calibrated camera get matched points for 3D reconstruction",
        "input": "",
        "output": "<p>If you only have the fundamental matrix and the intrinsics, you can only get a reconstruction up to scale. That is your translation vector t is in some unknown units. You can get the 3D points in real units in several ways:</p>\n\n<ul>\n<li>You need to have some reference points in the world with known distances between them. This way you can compute their coordinates in your unknown units and calculate the scale factor to convert your unknown units into real units.</li>\n<li>You need to know the extrinsics of each camera relative to a common coordinate system. For example, you can have a checkerboard calibration pattern somewhere in your scene that you can detect and compute extrinsics from. See this <a href=\"http://www.mathworks.com/help/vision/ug/sparse-3-d-reconstruction-from-multiple-views.html\" rel=\"nofollow\">example</a>. By the way, if you know the extrinsics, you can compute the Fundamental matrix and the camera projection matrices directly, without having to match points.</li>\n<li>You can do stereo calibration to estimate the R and the t between the cameras, which would also give you the Fundamental and the Essential matrices. See this <a href=\"http://www.mathworks.com/help/vision/ug/stereo-vision.html\" rel=\"nofollow\">example</a>.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Best Image Processing technique for detecting smudges/residues",
        "input": "",
        "output": "<p>Unfortunately (or fortunately, for most people), recovering oil smudge patterns from a smartphone screen using visible light photography is not going to be very reliable, and OpenCV isn't a silver bullet that will help you with that. You may be able to adjust the contrast, brightness, or amplify certain channels of the photograph to highlight features that weren't visible before. However, blindly applying Canny edge detection or any other operator available in OpenCV is unlikely to reveal some \"invisible ink\" to you unless you have some goal in mind and some understanding of what the operator is intended to do. </p>\n\n<p>More information can be found in the papers referenced by the Wikipedia article, but I imagine the most important factor in seeing smudges is going to be the lighting setup when the photo was taken. It might also be possible to detect smudges with less optimal lighting if your photo sensor was detecting some wavelength other than visible light, though that is pure speculation on my part. Ultimately, if the smudge information is not present in the photo because of specific lighting or wavelengths captured, though, you will not be able to extract it with any sort of image processing.</p>\n\n<p>Your best bet is to read the OpenCV docs, and <a href=\"http://docs.opencv.org/doc/tutorials/tutorials.html\" rel=\"nofollow\">follow some of their tutorials</a>, which are excellent. There is also <a href=\"https://opencv-python-tutroals.readthedocs.org/en/latest/\" rel=\"nofollow\">great support for the Python bindings</a>. This will be good for you as a Masters student and also give you a much better idea of what OpenCV can, and cannot do for you.</p>\n",
        "system": ""
    },
    {
        "instruction": "Is the official MATLAB computer vision toolbox actually OpenCV?",
        "input": "",
        "output": "<p>MATLAB for the most part uses OpenCV libraries underneath the hood for their CV toolbox.  However, I would like to note that the Computer Vision Toolbox also implements some functionality that is independent of the OpenCV libraries but a good majority of what the toolbox provides uses OpenCV functionality.  As quoted by Amro in his comment below:</p>\n\n<blockquote>\n  <p><em>Like many other areas, MATLAB wraps well-known libraries in an easy to use format (think BLAS/LAPACK, FFTW, SparseSuite, just to name a few!). So while MATLAB does make use of OpenCV in its CVST toolbox, it adds many other algorithms not found in OpenCV (either implemented in M-code or a lower-level language).</em></p>\n</blockquote>\n\n<p>In addition, you are certainly able to interface OpenCV code with MATLAB if you have code already written in this platform and would like to interface that with MATLAB if you are developing MATLAB products and want to take full advantage of OpenCV.  See this link for more details (courtesy of Amro): <a href=\"http://www.mathworks.com/discovery/matlab-opencv.html\" rel=\"nofollow\">http://www.mathworks.com/discovery/matlab-opencv.html</a></p>\n\n<p>Some of the MEX functions that called in the Computer Vision toolbox that ultimately get run call OpenCV C++ methods in the end.  Inside the folder of where MATLAB is installed, if you actually take a look at the <code>bin/os/</code> folder where <code>os</code> is the operating system you're using (for me, it's <code>maci64</code>), you'll see a <code>libopencv</code> folder.  In this folder, you will see a lot of dynamically linked libraries that are basically those from OpenCV to which the MATLAB MEX functions that are part of the CV toolbox access in the end.  </p>\n\n<p>To navigate here in MATLAB, type this into the MATLAB command prompt:</p>\n\n<pre><code>&gt;&gt; cd (matlabroot)/bin/\n</code></pre>\n\n<p>.. then go into the folder that is for your operating system, then finally go to the <code>libopencv</code> folder. </p>\n\n<p>You will also see what version of OpenCV is being used when calling the functions in the OpenCV library and the version number is appended at the end of each of the files. As such, if you want to use OpenCV for any of your MEX functions, perhaps the easiest thing would be to use the version that is pre-loaded into MATLAB to escape any compilation / setup headaches. With this, make sure you access the appropriate documentation for this version of OpenCV. </p>\n\n<p>As such, for those functions in the toolbox that do use OpenCV, it's really a three step process:</p>\n\n<ol>\n<li>The Computer Vision Toolbox provides MATLAB wrappers for you to call your functions. </li>\n<li>Inside these MATLAB wrappers, there may be some pre-processing steps which then get passed to a MEX function.  There may be functions in the CV toolbox in MATLAB where you're just calling a MEX function directly and the MATLAB wrapper may just include the doc string of what function you are calling. </li>\n<li>Inside the MEX code, they are calling OpenCV functions from the OpenCV libraries which then spit out the results you need.  Now, I'm not quite sure <strong>which</strong> MATLAB CV functions are calling OpenCV functions themselves, but I do know that a good majority of the MATLAB CV toolbox does call OpenCV under the hood.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How to apply CLAHE on RGB color images?",
        "input": "",
        "output": "<p>Conversion of RGB to LAB(L for lightness and a and b for the color opponents green\u2013red and blue\u2013yellow) will do the work. Apply CLAHE to the converted image in LAB format to only Lightness component and convert back the image to RGB.\nHere is the snippet.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import cv2\n\nbgr = cv2.imread(image_path)\n\nlab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n\nlab_planes = cv2.split(lab)\n\nclahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n\nlab_planes[0] = clahe.apply(lab_planes[0])\n\nlab = cv2.merge(lab_planes)\n\nbgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n</code></pre>\n<p><code>bgr</code> is the final RGB image obtained after applying CLAHE.</p>\n",
        "system": ""
    },
    {
        "instruction": "Performing computer vision and machine learning on images saved on a database",
        "input": "",
        "output": "<p>You probably need to get familiar with one of the Computer Vision libraries such as <a href=\"http://opencv.org\" rel=\"nofollow\">OpenCV</a>.  It has what you need, including the implementation of machine learning algorithms.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to efficiently digitize a color scale?",
        "input": "",
        "output": "<p>This color scale seems to be fairly simple to reproduce without need to use HSV or HSL.</p>\n\n<p>Supposing <code>x</code> is a <code>float</code> and can range from <code>-1.0f</code> to <code>1.0f</code>:</p>\n\n<pre><code>r = std::max&lt;int&gt;(0, std::min&lt;int&gt;(255,  512 * x));\ng = std::max&lt;int&gt;(0, std::min&lt;int&gt;(255,  512 * (std::abs&lt;float&gt;(x) - 0.5f)));\nb = std::max&lt;int&gt;(0, std::min&lt;int&gt;(255, -512 * x));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Installing C++ OpenCV on Mac",
        "input": "",
        "output": "<pre><code>brew tap homebrew/science\nbrew info opencv\nbrew install opencv\n</code></pre>\n\n<p>The <code>info</code> command was added as a suggestion so that you review the options you have when installing OpenCV. I'm using <a href=\"http://brew.sh/\" rel=\"nofollow\">homebrew</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: Improving the speed of Cascades detection",
        "input": "",
        "output": "<p>I'm not sure what you mean by \"speed\" in your video example since it's hard to make out what \"speed\" the detections are done at there. In computer vision, when we talk about the \"speed\" of detections, we generally mean the frames per second (FPS) or the millisecond run-time of the algorithm for a single or set of videos. If the FPS achieved by the algorithm is same as the FPS of the input video, this is called real-time or 1x processing speed. If processing FPS is greater than the input FPS, you have faster than real-time processing and if it is smaller, then you have slower than real-time. I will <em>assume</em> you meant the same thing when you said \"speed\".</p>\n\n<p>Given this, let me give you two ways to speed up the detections. I really suggest reading these two papers that have really set the bar in pedestrian detection in the past several years: <a href=\"http://www.robots.ox.ac.uk/~vgg/rg/papers/DollarBMVC10FPDW.pdf\" rel=\"noreferrer\">The Fastest Pedestrian Detector in the West</a> and <a href=\"http://lars.mec.ua.pt/public/LAR%20Projects/Perception/2013_PedroSilva/Disserta%C3%A7%C3%A3o/Papers/2012_cvpr_pedestrian_detection_at_100_frames_per_second.pdf\" rel=\"noreferrer\">Pedestrian detection at 100 frames per second</a>, both optimizing on the computation bottleneck of performing detection at multiple scales in the traditional detection setting. The latter has publicly available code <a href=\"https://bitbucket.org/rodrigob/doppia\" rel=\"noreferrer\">here</a> and <a href=\"http://www.vision.rwth-aachen.de/projects/groundhog\" rel=\"noreferrer\">here</a>. But so this is one of the areas to gain improvement: <em>scale sizes</em>.</p>\n\n<p>The method implemented natively in OpenCV is based on a <a href=\"http://www.lienhart.de/Prof._Dr._Rainer_Lienhart/Source_Code_files/ICIP2002.pdf\" rel=\"noreferrer\">variant</a> of the Viola-Jones method that extends the Haar-like feature set used in detection. Another area of improvement to consider is called <em>windowing</em>. Traditional detection methods, including the one implemented natively in OpenCV, require that you slide windows at scale across the image, usually row-wise from the upper-left to the bottom-right. A classic way to get around this is called <a href=\"https://sites.google.com/site/christophlampert/software\" rel=\"noreferrer\">Efficient Subwindow Search</a>  (ESS) which performs <a href=\"https://en.wikipedia.org/wiki/Branch_and_bound\" rel=\"noreferrer\">branch-and-bound</a> optimization. There have been many extensions building from this, but it's an excellent place to start and understand the basics of object detection.</p>\n\n<p>Now, of course, one very obvious way to speed-up the detection process is to parallelize your code, e.g. multi-threading or GPU. There are several GPU implementations that are publicly available, e.g. <a href=\"http://www.emgu.com/wiki/index.php/Pedestrian_Detection_in_CSharp\" rel=\"noreferrer\">here</a> using a support vector machine-based detector.</p>\n",
        "system": ""
    },
    {
        "instruction": "Python OpenCV: Rubik&#39;s cube solver color extraction",
        "input": "",
        "output": "<p>Here's a simple approach:</p>\n<ul>\n<li>Convert image to HSV format</li>\n<li>Use color thresholding to detect the squares with <a href=\"https://docs.opencv.org/3.4.15/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981\" rel=\"nofollow noreferrer\"><code>cv2.inRange()</code></a></li>\n<li>Perform <a href=\"https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\" rel=\"nofollow noreferrer\">morphological operations</a> and draw squares onto a mask</li>\n<li><a href=\"https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours\" rel=\"nofollow noreferrer\">Find contours</a> on mask and sort from top-bottom or bottom-top</li>\n<li>Take each row of three squares and sort from left-right or right-left</li>\n</ul>\n<hr />\n<p>After converting to HSV format, we perform color thresholding using <code>cv2.inRange()</code> to detect the squares. We draw the detected squares onto a mask</p>\n<img src=\"https://i.sstatic.net/fdxfb.png\" width=\"300\">\n<p>From here we find contours on the mask and utilize <a href=\"https://github.com/jrosebr1/imutils#sorting-contours\" rel=\"nofollow noreferrer\"><code>imutils.contours.sort_contours()</code></a> to sort the contours from top-to-bottom or bottom-to-top. Next we take each row of 3 squares and sort this row from left-to-right or right-to-left. Here's a visualization of the sorting (top-bottom, left) or (bottom-top, right)</p>\n<img src=\"https://i.sstatic.net/H8F5p.gif\" width=\"300\">\n<img src=\"https://i.sstatic.net/hFdyH.gif\" width=\"300\">\n<p>Now that we have the contours sorted, we simply draw the rectangles onto our image. Here's the results</p>\n<p>Left-to-right and top-to-bottom (left), right-to-left and top-to-bottom</p>\n<img src=\"https://i.sstatic.net/76jOd.png\" width=\"300\">\n<img src=\"https://i.sstatic.net/A6qZ8.png\" width=\"300\">\n<p>Left-to-right and bottom-to-top (left), right-to-left and bottom-to-top</p>\n<img src=\"https://i.sstatic.net/D5LsP.png\" width=\"300\">\n<img src=\"https://i.sstatic.net/QnoMG.png\" width=\"300\">\n<pre><code>import cv2\nimport numpy as np\nfrom imutils import contours\n\nimage = cv2.imread('1.png')\noriginal = image.copy()\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\nmask = np.zeros(image.shape, dtype=np.uint8)\n\ncolors = {\n    'gray': ([76, 0, 41], [179, 255, 70]),        # Gray\n    'blue': ([69, 120, 100], [179, 255, 255]),    # Blue\n    'yellow': ([21, 110, 117], [45, 255, 255]),   # Yellow\n    'orange': ([0, 110, 125], [17, 255, 255])     # Orange\n    }\n\n# Color threshold to find the squares\nopen_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))\nclose_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\nfor color, (lower, upper) in colors.items():\n    lower = np.array(lower, dtype=np.uint8)\n    upper = np.array(upper, dtype=np.uint8)\n    color_mask = cv2.inRange(image, lower, upper)\n    color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_OPEN, open_kernel, iterations=1)\n    color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_CLOSE, close_kernel, iterations=5)\n\n    color_mask = cv2.merge([color_mask, color_mask, color_mask])\n    mask = cv2.bitwise_or(mask, color_mask)\n\ngray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\ncnts = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\n# Sort all contours from top-to-bottom or bottom-to-top\n(cnts, _) = contours.sort_contours(cnts, method=&quot;top-to-bottom&quot;)\n\n# Take each row of 3 and sort from left-to-right or right-to-left\ncube_rows = []\nrow = []\nfor (i, c) in enumerate(cnts, 1):\n    row.append(c)\n    if i % 3 == 0:  \n        (cnts, _) = contours.sort_contours(row, method=&quot;left-to-right&quot;)\n        cube_rows.append(cnts)\n        row = []\n\n# Draw text\nnumber = 0\nfor row in cube_rows:\n    for c in row:\n        x,y,w,h = cv2.boundingRect(c)\n        cv2.rectangle(original, (x, y), (x + w, y + h), (36,255,12), 2)\n\n        cv2.putText(original, &quot;#{}&quot;.format(number + 1), (x,y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n        number += 1\n\ncv2.imshow('mask', mask)\ncv2.imwrite('mask.png', mask)\ncv2.imshow('original', original)\ncv2.waitKey()\n</code></pre>\n<p>To get the HSV color ranges, you can use this simple HSV color thresholder script to determine the lower/upper color ranges. Change the image path in <code>cv2.imread()</code></p>\n<pre><code>import cv2\nimport numpy as np\n\ndef nothing(x):\n    pass\n\n# Load image\nimage = cv2.imread('1.jpg')\n\n# Create a window\ncv2.namedWindow('image')\n\n# Create trackbars for color change\n# Hue is from 0-179 for Opencv\ncv2.createTrackbar('HMin', 'image', 0, 179, nothing)\ncv2.createTrackbar('SMin', 'image', 0, 255, nothing)\ncv2.createTrackbar('VMin', 'image', 0, 255, nothing)\ncv2.createTrackbar('HMax', 'image', 0, 179, nothing)\ncv2.createTrackbar('SMax', 'image', 0, 255, nothing)\ncv2.createTrackbar('VMax', 'image', 0, 255, nothing)\n\n# Set default value for Max HSV trackbars\ncv2.setTrackbarPos('HMax', 'image', 179)\ncv2.setTrackbarPos('SMax', 'image', 255)\ncv2.setTrackbarPos('VMax', 'image', 255)\n\n# Initialize HSV min/max values\nhMin = sMin = vMin = hMax = sMax = vMax = 0\nphMin = psMin = pvMin = phMax = psMax = pvMax = 0\n\nwhile(1):\n    # Get current positions of all trackbars\n    hMin = cv2.getTrackbarPos('HMin', 'image')\n    sMin = cv2.getTrackbarPos('SMin', 'image')\n    vMin = cv2.getTrackbarPos('VMin', 'image')\n    hMax = cv2.getTrackbarPos('HMax', 'image')\n    sMax = cv2.getTrackbarPos('SMax', 'image')\n    vMax = cv2.getTrackbarPos('VMax', 'image')\n\n    # Set minimum and maximum HSV values to display\n    lower = np.array([hMin, sMin, vMin])\n    upper = np.array([hMax, sMax, vMax])\n\n    # Convert to HSV format and color threshold\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    mask = cv2.inRange(hsv, lower, upper)\n    result = cv2.bitwise_and(image, image, mask=mask)\n\n    # Print if there is a change in HSV value\n    if((phMin != hMin) | (psMin != sMin) | (pvMin != vMin) | (phMax != hMax) | (psMax != sMax) | (pvMax != vMax) ):\n        print(&quot;(hMin = %d , sMin = %d, vMin = %d), (hMax = %d , sMax = %d, vMax = %d)&quot; % (hMin , sMin , vMin, hMax, sMax , vMax))\n        phMin = hMin\n        psMin = sMin\n        pvMin = vMin\n        phMax = hMax\n        psMax = sMax\n        pvMax = vMax\n\n    # Display result image\n    cv2.imshow('image', result)\n    if cv2.waitKey(10) &amp; 0xFF == ord('q'):\n        break\n\ncv2.destroyAllWindows()\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Function doesn&#39;t work properly (OpenCV Python)",
        "input": "",
        "output": "<p>Your threshold of <code>128</code> on the <code>mask</code> in <code>replace</code> need not necessarily hold. Since the <code>segmentation</code> function will return a matrix of only two possible values, try this instead:</p>\n\n<pre><code>  low, high = np.unique(mask)\n  mask[mask == low] = 0\n  mask[mask == high] = 1\n</code></pre>\n\n<p>Everything else looks fine after that.</p>\n",
        "system": ""
    },
    {
        "instruction": "Is there a camera calibration matrix database?",
        "input": "",
        "output": "<p>there is the web based calibration service at <a href=\"https://www.calibdb.net/\" rel=\"nofollow noreferrer\">https://www.calibdb.net/</a>. It remembers finished calibrations, so it also acts as a database.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to improve keypoints detection and matching",
        "input": "",
        "output": "<p>Compensating the difference between image output and the world coordinates:</p>\n\n<p>You should use good old camera calibration approach for calibrating the camera response and possibly generating a correction matrix for the camera output (in order to convert them into real scales).</p>\n\n<p>It's not that complicated once you have printed out a checkerboard template and capture various shots. (For this application you don't need to worry about rotation invariance. Just calibrate the world view with the image array.)</p>\n\n<p>You can find more information here: <a href=\"http://www.vision.caltech.edu/bouguetj/calib_doc/htmls/own_calib.html\" rel=\"nofollow\">http://www.vision.caltech.edu/bouguetj/calib_doc/htmls/own_calib.html</a></p>\n\n<p>--</p>\n\n<p>Now since I can't seem to comment on the question, I'd like to ask if your specific application requires the machine to \"find out\" the shape of the hole on the fly. If there are finite amount of hole shapes, you may then model them mathematically and look for the pixels that support the predefined models on the B/W edge image.</p>\n\n<p>Such as (x)^2+(y)^2-r^2=0 for a circle with radius r, whereas x and y are the pixel coordinates.</p>\n\n<p>That being said, I believe more clarification is needed regarding the requirements of the application (shape detection).</p>\n",
        "system": ""
    },
    {
        "instruction": "Scene change/shot detection/image extraction using ffmpeg from video",
        "input": "",
        "output": "<p>I suggest also taking a look at other freely available shot detection implementations. For example, a custom threshold of <code>47</code> with Johan Mathe's <a href=\"http://johmathe.name/shotdetect.html\" rel=\"nofollow noreferrer\">Shotdetect</a> yields the following results (first frame of every shot):\n<img src=\"https://i.sstatic.net/Q8N9C.jpg\" alt=\"Shot Detection with jmathe&#39;s Shotdetect\"></p>\n\n<p>However, your question seems to deal more with the problem of <em>video stabilization</em> than it does scene change or shot detection. The blur that you see above as well as in the OP are \"artifacts\" inherent to the video and are not due to the algorithms used to cut or sample the video. If you want to reduce the amount of motion you should look into <a href=\"https://stackoverflow.com/questions/6971766/how-to-remove-distortion-due-to-motion-from-an-image\">this post</a> as well as look at <a href=\"http://docs.opencv.org/trunk/modules/videostab/doc/videostab.html\" rel=\"nofollow noreferrer\">OpenCV's Video Stabilization</a> module. There is also a lot of research addressing this challenge including <a href=\"http://www.cse.cuhk.edu.hk/~leojia/projects/motion_deblurring/index.html\" rel=\"nofollow noreferrer\">this</a> and <a href=\"http://cs.nyu.edu/~fergus/research/deblur.html\" rel=\"nofollow noreferrer\">this</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Javascript - Extracting RGB values from pixels",
        "input": "",
        "output": "<p>After days of research i have realized that there are no built in fnctions for pixel value extraction and the best method i have come across so far has been:</p>\n\n<pre><code>var idata = canvas.getImageData(locationX,locationY ,1,1);\n            var data = idata.data;\n            var R = data[0];\n            var G = data[1];\n            var B = data[2];\n            document.getElementById('colorExtraction').innerHTML = R + \" \" +G + \" \" + B ;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Face detection in backlit scenes",
        "input": "",
        "output": "<p>If you expect many such images, you can train your the VJ detector on your own such data set.  Assuming there is actual data in the dark face region and that it isn't just under-saturated, that should help improve the performance.</p>\n",
        "system": ""
    },
    {
        "instruction": "Replacing a segmented part of an image with it&#39;s unsegmented part",
        "input": "",
        "output": "<p>This is actually pretty easy.  All you have to do is take your picture after segmentation, and multiply it by a mask where any pixel in the mask that is 0 becomes 1, and anything else becomes 0.</p>\n\n<p>This will essentially blacken all of the pixels with the exception of the pixels within the mask that are 1.  By multiplying each of the pixels in your image by the mask, you would effectively produce what you have shown in the figure, but the background is black.  All you would have to do now is figure out which locations in your mask are white and set the corresponding locations in your output image to white.  In other words:</p>\n\n<pre><code>import cv2\n\n# Load in your original image\noriginalImg = cv2.imread('Inu8B.jpg',0)\n\n# Load in your mask\nmask = cv2.imread('2XAwj.jpg', 0)\n\n# Get rid of quantization artifacts\nmask[mask &lt; 128] = 0\nmask[mask &gt; 128] = 1\n\n# Create output image\noutputImg = originalImg * (mask == 0)\noutputImg[mask == 1] = 255\n\n# Display image\ncv2.imshow('Output Image', outputImg)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>\n\n<hr>\n\n<p>Take note that I downloaded the images from your post and loaded them from my computer.  Also, your mask has some quantization artifacts due to JPEG, and so I thresholded at intensity 128 to ensure that your image consists of either 0s or 1s.</p>\n\n<p>This is the output I get:</p>\n\n<p><img src=\"https://i.sstatic.net/3CeFT.png\" alt=\"enter image description here\"></p>\n\n<p>Hope this helps!</p>\n",
        "system": ""
    },
    {
        "instruction": "Alternatives to String SDK (http://string.co). What to use instead",
        "input": "",
        "output": "<p>You will find a list of alternative AR SDKs along with a comparison of each here <a href=\"http://socialcompare.com/en/comparison/augmented-reality-sdks\" rel=\"nofollow\">http://socialcompare.com/en/comparison/augmented-reality-sdks</a>  From what I can tell this list is pretty active and updated frequently.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to improve LBP operator by reducing feature dimension",
        "input": "",
        "output": "<p>Use the <a href=\"http://www.mathworks.com/help/stats/pcares.html\" rel=\"nofollow\"><code>pcares</code></a> function to do that.  <code>pcares</code> stands for <strong>PCA Residuals</strong>:</p>\n\n<pre><code>[residuals, reconstructed] = pcares(X, ndim);\n</code></pre>\n\n<p><code>residuals</code> returns the residuals obtained by retaining <code>ndim</code> principal components of the <code>n-by-p</code> matrix <code>X</code>. <code>X</code> is the <strong>data</strong> matrix, or the matrix that contains your data.  Rows of <code>X</code> correspond to observations and columns are the variables. <code>ndim</code> is a scalar and must be less than or equal to <code>p</code>. <code>residuals</code> is a matrix of the same size as <code>X</code>.</p>\n\n<p><code>reconstructed</code> will have the reduced dimensional data based on the <code>ndim</code> input.  Note that <code>reconstructed</code> will still be in the original dimension as <code>X</code>.  As such, you can choose the first <code>ndim</code> columns and this will correspond to those features constructed using the number of the dimensions for the feature specified by <code>ndim</code>.  In other words:</p>\n\n<pre><code>reduced = reconstructed(:,1:ndim);\n</code></pre>\n\n<p>As such, <code>reduced</code> will contain your data that was dimension reduced down to <code>ndim</code> dimensions.</p>\n\n<h1>Small Note</h1>\n\n<p>You need the Statistics Toolbox in order to run <code>pcares</code>.  If you don't, then this method won't work.</p>\n",
        "system": ""
    },
    {
        "instruction": "Triangulation: Find a 3D point minimizing the Distance from N 3D Lines/Rays",
        "input": "",
        "output": "<p>If you expand the formula for <code>d^2</code> above you find it is of the form</p>\n\n<pre><code>d^2 = Cxx Px Px + Cxy Px Py + ... + Ex Px + Ey Py + Ez Pz + F\n</code></pre>\n\n<p>where Cxx, Ex, F etc are constants depending on A and B vectors.  Now take the sum of these for each line, lets call this sum S which is a quadratic in Px, Py and Pz.</p>\n\n<pre><code>S = Sxx Px Px + Sxy Px Py + ... +  Rx Px + Ry Py + Rz Pz + T\n</code></pre>\n\n<p>Now differentiate</p>\n\n<pre><code>dS/dx = 2 Sxx Px +   Sxy Py +   Sxz Pz + Rx\ndS/dy =   Syx Px + 2 Syy Py +   Syz Pz + Ry\ndS/dz =   Szx Px +   Szy Py + 2 Szz Pz + Rz\n</code></pre>\n\n<p>As S is a minimum each of these must be zero. You can write this in matrix form\nA x = b. [A] is the matrix of the Sxx etc , b is the vector -[Rx,Ry,Rz] and x is the vector [Px,Py,Pz].</p>\n\n<p>I've now implemented a 2D version as a jsfiddle <a href=\"http://jsfiddle.net/SalixAlba/Y3yT9/1/\" rel=\"nofollow\">http://jsfiddle.net/SalixAlba/Y3yT9/1/</a>\nit suprisingly simple to do the guts of the algorithm is</p>\n\n<pre><code>var sxx = 0;\nvar sxy = 0;\nvar syy = 0;\nvar rx = 0;\nvar ry = 0;\nvar t = 0;\n// each line is defined by a x + b y + c = 0\nlines.forEach(function(line, index, array) {\n    var div = line.a * line.a + line.b * line.b;\n    sxx += line.a * line.a / div;        \n    sxy += line.a * line.b / div;        \n    syy += line.b * line.b / div;        \n    rx += line.a * line.c / div;        \n    ry += line.b * line.c / div;        \n    t += line.c * line.c / div;        \n});\n// Derivative of S wrt x and y is\n//  2 sxx x + 2 sxy y + 2 rx = 0\n//  2 sxy x + 2 syy y + 2 ry = 0\n\n// Solve this pair of linear equations\nvar det = sxx * syy - sxy * sxy;\nif(Math.abs(det) &lt; 1e-6) {\n    sol = { x: -10, y: -10 };\n    return;\n}\n// (x) =  1  ( syy    -sxy ) ( -rx )\n// ( ) = --- (             ) (     )\n// (y)   det ( -sxy    sxx ) ( -ry )\nvar x = ( - syy * rx + sxy * ry ) / det;\nvar y = (   sxy * rx - sxx * ry ) / det;\nconsole.log(\"x \"+x+\" y \"+y);\nsol = { x: x, y: y };\nreturn;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Removing square objects",
        "input": "",
        "output": "<p>You can compare the area of the mask to its perimeter using the following formula</p>\n\n<pre><code>ratio = 4 * pi * Area / ( Perimeter^2 )\n</code></pre>\n\n<p>For circles this ration should be very close to one, for other shapes it should be significantly lower.<br>\nSee <a href=\"http://www.mathworks.com/help/images/examples/identifying-round-objects.html\" rel=\"noreferrer\">this tutorial</a> for an example.</p>\n\n<p>The rationale behind this formula: circles are optimal in their perimeter-area ratio - max area for given perimeter. Given Perimeter, you can estimate radius of equivalent circle by <code>Perimeter = 2*pi*R</code>, using this estimated <code>R</code> you can compute the \"equivalent circle area\" using <code>eqArea = pi*R^2</code>. Now you only need to check the ratio between the actual area of the shape and the \"equivalent area\" computed.</p>\n\n<p>Note: since <code>Area</code> and <code>Perimeter</code> of objects in mask are estimated based on the pixel-level discretization these estimates may be quite crude especially for small shapes. Consider working with higher resolution masks if you notice quantization/discretization errors.</p>\n",
        "system": ""
    },
    {
        "instruction": "Cumulative Homography Wrongly Scaling",
        "input": "",
        "output": "<p>It turns out this was because my viewpoint was close to the features, meaning that the non-planarity of the tracked features was causing skew to the homography. I managed to prevent this (it's more of a hack than a method...) by using <code>estimateRigidTransform</code> instead of <code>findHomography</code>, as this does not estimate for perspective variations. </p>\n\n<p>In this particular case, it makes sense to do so, as the view does only ever undergo rigid transformations.</p>\n",
        "system": ""
    },
    {
        "instruction": "Compute fundamental matrix without point correspondences?",
        "input": "",
        "output": "<p>I would prefer to do it like the equations in Chapter 9 of \"Multiple View Geometry\". I have verified these in Matlab. It is right.</p>\n\n<p>If you can get both intrinsic and extrinsic matrix of both cameras, you can calculate the F matrix like:</p>\n\n<p>F = [e']_x * P' *p^+</p>\n\n<p>(Please refer to pp244 of \"Multiple View Geometry\" for detailed definitions)</p>\n",
        "system": ""
    },
    {
        "instruction": "Extract depth map from a 3D anaglyph image",
        "input": "",
        "output": "<p>An anaglyph is just a superposition of a left-eye and right-eye image, using different colours for each.</p>\n\n<p>Assuming you can use the colour components to extract the original greyscale left and right images, the problem is no different to any stereo vision problem. You need to determine the epipolar geometry, perform rectification on one of the images, then create a disparity map to derive relative depth information.</p>\n",
        "system": ""
    },
    {
        "instruction": "Image searching system using Bag Of Features method",
        "input": "",
        "output": "<p>You are right about the randomness of KMeans initial centers, but I would assume that if you dont explicitely change the initial seed of the random number generator, then it would always pick the same \"random\" features as initial centers.\nAnother reason could be that, if feature detection/extraction is multithreaded, the features computed on your image can appear in different orders between each run. You might check that aswell.</p>\n\n<p>Alternatively, you can pass your own centers to KMeans as initial centroids so you could pick them or generate them by yourself to make sure KMeans starts with always the same initial centers</p>\n",
        "system": ""
    },
    {
        "instruction": "Optical flow vs keypoint matching: what are the differences?",
        "input": "",
        "output": "<p>Optical flow(OF) is method around so called \"brightness constancy assumption\". You assume that pixels - more specific, theirs intensities (up to some delta) -  are not changing, only shifting. And you find solution of this equation:\nI(x,y,t) = I(x+dx, y+dy, t+dt). </p>\n\n<p>First order of Tailor series is:\nI(x + dx, y+dy, t+ dt) = I (x,y,t) + I_x * dx + I_y * dy + I_t * dt.</p>\n\n<p>Then you solve this equation and get dx and dy - shifts for every pixel.</p>\n\n<p>Optical flow is mainly used for <a href=\"http://computervision.wikia.com/wiki/Tracking\" rel=\"nofollow\">tracking</a> and <a href=\"http://en.wikipedia.org/wiki/Odometry\" rel=\"nofollow\">odometry</a>.</p>\n\n<p><strong>upd.:</strong> if applied not to whole image, but the patch, optical flow is almost the same, as <a href=\"http://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker\" rel=\"nofollow\">Lucas-Kanade-Tomashi tracker</a>.</p>\n\n<p>Difference between this method and feature-based methods is density. With feature points you usually get difference in position of the feature points only, while optical flow estimates it for whole image. </p>\n\n<p>The drawback is that vanilla OF works only for small displacements. For handling larger ones, one can downscale image and calculate OF on it - \"coarse-to-fine\" method.</p>\n\n<p>One can change \"brightness constancy assumption\" to, i.e., \"descriptor constancy assumption\" and solve the same equation but with descriptor value instead of raw intensity. <a href=\"http://people.csail.mit.edu/celiu/SIFTflow/\" rel=\"nofollow\">SIFT flow</a> is an example of this .</p>\n\n<p>Unfortunately, I don`t know much about augmented reality commercial solutions and cannot answer the last question. </p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab: Efficiently do SVD many times? (to triangulate a 3D point cloud)",
        "input": "",
        "output": "<p>There is now a <a href=\"http://www.mathworks.com/help/vision/ref/triangulate.html\" rel=\"nofollow\">triangulate</a> function in the Computer Vision System Toolbox.</p>\n",
        "system": ""
    },
    {
        "instruction": "Error detection PCB fiducial detection from Image of Board",
        "input": "",
        "output": "<p>Adjust the actual FID1 to be 0,0, adjust the expected FID1 to be 0,0 as well.  This creates a \"hinge/pivot\" point.  Now, all other fiducial and all other component can be represented as polar coordinate from ths 0,0 hinge.</p>\n\n<p>Now, forget about X,Y coordinate system.  You don't care about X,Y.  You must translate all X,Y to polar coordinates instead, using many significant digit for accuracy.</p>\n\n<p>Now, you only have to calculated delta angle from expected to actual!  Again, don't bother with X,Y calculation  By using the hinge/pivot concept, the vector distance of expected and actual is always equal.  The length of vector never changes expected to actual - only the angle changes!</p>\n\n<p>Remember the PCB is the same size no matter how you rotate it.  Think of old school protractor.  Set protractor to 0,0 origin and draw an arc!  </p>\n\n<p>I do not have big enough reputation to post an image, so unfortunately I have to link to the image instead.  Also, I have too many projects to give you a proper tutorial so this 2-minute psuedo illustration will have to be good enough.</p>\n\n<p><a href=\"https://i.sstatic.net/5AZpz.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/5AZpz.jpg\" alt=\"illustration of hinge/pivot concept\"></a></p>\n",
        "system": ""
    },
    {
        "instruction": "In image matching, what is the role of model?",
        "input": "",
        "output": "<p>\"Learning the model\" typically means \"training a classifier\".  For example, if you use the Bag of Features approach to distinguish images of dogs from images of cats, you would train a \"dog-vs-cat\" classifier.  The output of the training procedure is often called a \"model\", which you can then use to classify an image as \"dog\" or \"cat\".</p>\n\n<p>I think the reason the term \"model\" is used, is to avoid the ambiguity of the term \"classifier\". A classifier could mean a classification algorithm, such as a support vector machine (SVM), or it could mean the result of training on some specific data set using a classification algorithm. The term \"model\" removes the ambiguity.  You can train a model to distinguish between cats and dogs using a linear SVM classifier.</p>\n",
        "system": ""
    },
    {
        "instruction": "&lt;unknown&gt; is not a numpy array error",
        "input": "",
        "output": "<p><a href=\"http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html#threshold\" rel=\"nofollow\">cv2.threshold</a> returns a tuple (retval, dst), where dst is the transformed matrix. You need to pass <code>dst</code> to <code>cv2.imshow</code> instead of the returned tupled, after checking if <code>retval</code> is valid</p>\n\n<pre><code>retval, th = cv2.threshold(fImg, 127, 255, cv2.TRESH_BINARY)\nif retval:\n    cv2.imshow('th', th)\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Live stream video processing in android using opencv",
        "input": "",
        "output": "<p>With that little information there is not much we can do to help. However, I will post some code for performing a simple image processing technique to a camera stream: converting the frames to grayscale.</p>\n\n<p>You need to declare the following:</p>\n\n<pre><code>private CameraBridgeViewBase mOpenCvCameraView;\npublic CvCameraViewListener2 camListener;\n</code></pre>\n\n<p>Then, you need to initialize OpenCV. You need to add the following block of code before you call your <code>onCreate()</code> method:</p>\n\n<pre><code>private BaseLoaderCallback mLoaderCallback = new BaseLoaderCallback(this) {\n    @Override\n    public void onManagerConnected(int status) {\n        switch (status) {\n        case LoaderCallbackInterface.SUCCESS:\n        {\n            Log.i(\"TAG\", \"OpenCV loaded successfully\");\n            mOpenCvCameraView.enableView();\n            System.loadLibrary(\"nameOfYourNativeLibrary\"); // if you are working with JNI\n            run();\n        } break;\n        default:\n        {\n            super.onManagerConnected(status);\n        } break;\n        }\n    }\n};\n</code></pre>\n\n<p>You can call</p>\n\n<pre><code>if (!OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_2_4_3, this, mLoaderCallback)){\n    Log.e(\"ERR\", \"Cannot connect to OpenCV Manager\");\n}else Log.i(\"SUCCESS\", \"opencv successfull\");\n</code></pre>\n\n<p>inside your <code>onCreate()</code> method, but what you <strong>really</strong> need is to override the <code>onResume()</code> method:</p>\n\n<pre><code>@Override\npublic void onResume()\n{\n    super.onResume();\n    OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_2_4_3, this, mLoaderCallback);\n    mOpenCvCameraView.enableView();\n}\n</code></pre>\n\n<p><code>mOpenCvCameraView</code> is a <code>JavaCameraView</code>. Just in case you are not familiar with any of this, this is the <code>xml</code> layout of your <code>MainActivity</code>:</p>\n\n<pre><code>&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    xmlns:opencv=\"http://schemas.android.com/apk/res-auto\" \n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:paddingBottom=\"@dimen/activity_vertical_margin\"\n    android:paddingLeft=\"@dimen/activity_horizontal_margin\"\n    android:paddingRight=\"@dimen/activity_horizontal_margin\"\n    android:paddingTop=\"@dimen/activity_vertical_margin\"\n    tools:context=\"com.example.yourapp.MainActivity$PlaceholderFragment\" &gt;\n\n&lt;org.opencv.android.JavaCameraView\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:visibility=\"visible\"\n    android:id=\"@+id/HelloOpenCvView\"\n    opencv:show_fps=\"false\"\n    opencv:camera_id=\"any\" /&gt;\n\n&lt;/RelativeLayout&gt;\n</code></pre>\n\n<p>So your <code>onCreate()</code> method should look like this:</p>\n\n<pre><code>@Override\npublic void onCreate(Bundle savedInstanceState) {\n    Log.i(\"APP\", \"called onCreate\");\n    super.onCreate(savedInstanceState);\n    getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);\n    setContentView(R.layout.fragment_main);\n    mOpenCvCameraView = (CameraBridgeViewBase)findViewById(R.id.HelloOpenCvView);\n    if (!OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_2_4_3, this, mLoaderCallback)){\n        Log.e(\"OPENCV\", \"Cannot connect to OpenCV Manager\");\n    }else Log.i(\"OPENCV\", \"opencv successfull\");\n}\n</code></pre>\n\n<p>It is in the missing <code>run()</code> method where you should add the image processing you want to perform. In this case we will convert the frames to grayscale and display them:</p>\n\n<pre><code>private void run(){\n    camListener = new CvCameraViewListener2() {\n\n        @Override\n        public void onCameraViewStopped() {\n            // TODO Auto-generated method stub\n        }\n\n        @Override\n        public void onCameraViewStarted(int width, int height) {\n            // TODO Auto-generated method stub          \n        }\n\n        @Override\n        public Mat onCameraFrame(CvCameraViewFrame inputFrame) {\n            rgb = inputFrame.rgba();\n            // here you could just return inputframe.gray(), but for illustration we do the following\n            Mat gray = new Mat();\n            Imgproc.cvtColor(rgb, gray, Imgproc.COLOR_RGB2GRAY);\n            return gray;\n        }\n    };\n\n    mOpenCvCameraView.setVisibility(SurfaceView.VISIBLE);\n    mOpenCvCameraView.setCvCameraViewListener(camListener);\n\n}\n</code></pre>\n\n<p>I hope this helps. If you need anything else let me know.</p>\n\n<p><strong>UPDATE</strong>\nIf you want to access the pixel values of a <code>Mat</code> object you need something like this:</p>\n\n<pre><code>double mean = 0;\ndouble size = mat.rows()*mat.cols();\nfor(int k=0; k&lt;mat.rows(); k++){\n    for(int j=0; j&lt;mat.cols(); j++){\n        mean += mat.get(k, j)[0]/size;\n    }\n}\n</code></pre>\n\n<p>Note that in this case my <code>Mat</code> object has a single channel (that means if you try to access <code>mat.get(k, j)[1]</code> you would obtain an <code>Array out of bounds exception</code>. For RGB <code>Mat</code> objects you should be able to access all <code>mat.get(k, j)[0]</code>, <code>mat.get(k, j)[1]</code> and <code>mat.get(k, j)[2]</code></p>\n",
        "system": ""
    },
    {
        "instruction": "Segmenting License Plate Characters",
        "input": "",
        "output": "<p>Before I start, I know you are seeking an implementation of this algorithm in OpenCV C++, but my algorithm requires the FFT and the <code>numpy / scipy</code> packages are awesome for that.  As such, I will give you an implementation of the algorithm in OpenCV <em>using Python</em> instead.  The code is actually quite similar to the C++ API that you can easily transcribe that over instead.  That way, it minimizes the amount of time it will take for me to learn (or rather relearn...) the API and I would rather give you the algorithm and the steps I did to perform this task to not waste any time at all.  </p>\n\n<p>As such, I will give you a general overview of what I would do.  I will then show you Python code that uses <code>numpy, scipy</code> and the OpenCV packages.  As a bonus for those who use MATLAB, I will show you the MATLAB equivalent, with MATLAB code to boot!</p>\n\n<hr>\n\n<p>What you can do is try to use <a href=\"http://en.wikipedia.org/wiki/Homomorphic_filtering\">homomorphic filtering</a>.  In basic terms, we can represent an image in terms of a product of illumination and reflectance.  Illumination is assumed to be slowly varying and the main contributor of dynamic range.  This is essentially low frequency content.  Reflectance represents details of objects and assumed to vary rapidly.  This is also the primary contributor to local contrast and is essentially high frequency content.</p>\n\n<p>The image can be represented as a <strong>product</strong> of these two.  Homomorphic filtering tries and splits up these components and we filter them individually.  We then combine the results together when we are finished.  As this is a multiplicative model, it's customary to use a <strong>log</strong> operation so that we can express the product as a sum of two terms.  These two terms are filtered individually, scaled to emphasize or de-emphasize their contributions to the image, summed, then the anti-log is taken.</p>\n\n<p>The shading is due to the illumination, and so what we can do is decrease the contribution that this shading does over the image.  We can also boost the reflectance so we can get some better edges as edges are associated with high frequency information.  </p>\n\n<p>We usually filter the illumination using a low-pass filter, while the reflectance with a high-pass filter.  In this case, I'm going to choose a Gaussian kernel with a sigma of 10 as the low-pass filter.  A high-pass filter can be obtained by taking <code>1</code> and subtracting with the low-pass filter.  I transform the image into the log domain, then filter the image in the frequency domain using the low-pass and high-pass filters.  I then scale the low pass and high pass results, add these components back, then take the anti-log.  This image is now better suited to be thresholded as the image has low variation.  </p>\n\n<p>What I do as additional post-processing is that I threshold the image.  The letters are darker than the overall background, so any pixels that are lower than a certain threshold would be classified as text.  I chose the threshold to be intensity 65.  After this, I also clear off any pixels that are touching the border, then remove any areas of the image that have less than 160 (MATLAB) or 120 (Python) pixels of total area.  I also crop out some of the columns of the image as they are not needed for our analysis.  </p>\n\n<hr>\n\n<p>Here are a couple of caveats for you:</p>\n\n<h2>Caveat #1 - Removing borders</h2>\n\n<p>Removing any pixels that touch the border is <strong>not</strong> built into OpenCV.  However, MATLAB has an equivalent called <a href=\"http://www.mathworks.com/help/images/ref/imclearborder.html\"><code>imclearborder</code></a>. I'll use this in my MATLAB code, but for OpenCV, this was the following algorithm:</p>\n\n<ul>\n<li>Find all of the contours in the image</li>\n<li>For each contour that is in the image, check to see if any of the contour pixels are within the border of the image</li>\n<li>If any are, mark this contour for removal</li>\n<li>For each contour we want to remove, simply draw this whole contour in black</li>\n</ul>\n\n<p>I created a method called <code>imclearborder(imgBW, radius)</code> in my code, where <code>radius</code> is how many pixels within the border you want to clear stuff up.</p>\n\n<h2>Caveat #2 - Removing pixel areas below a certain area</h2>\n\n<p>Removing any areas where they are less than a certain amount is also <strong>not</strong> implemented in OpenCV.  In MATLAB, this is conveniently given using <a href=\"http://www.mathworks.com/help/images/ref/bwareaopen.html\"><code>bwareaopen</code></a>.  The basic algorithm for this is:</p>\n\n<ul>\n<li>Find all of the contours in the image</li>\n<li>Analyze how much each contour's area fills up if you were to fill in the interior</li>\n<li>Any areas that are less than a certain amount, clear this contour by filling the interior with black</li>\n</ul>\n\n<p>I created a method called <code>bwareaopen(imgBW)</code> that does this for us.</p>\n\n<h2>Caveat #3 - Area parameter for removing pixel areas</h2>\n\n<p>For the Python code, I had to play around with this parameter and I settled for 120.  160 was used for MATLAB.  For python, 120 got rid of some of the characters, which is not desired.  I'm guessing my implementation of <code>bwareaopen</code> in comparison to MATLAB's is different, which is probably why I'm getting different results.</p>\n\n<hr>\n\n<p>Without further ado, here's the code.  Take note that I did not use <strong>spatial filtering</strong>.  You could use <code>filter2D</code> in OpenCV and convolve this image with the Gaussian kernel, but I did not do that as Homomorphic Filtering when using low-pass and high-pass filters are traditionally done in the frequency domain.  You could explore this using spatial filtering, but you would also have to know the <strong>size</strong> of your kernels before hand.  With frequency domain filtering, you just need to know the standard deviation of the filter, and that's just one parameter in comparison to two.</p>\n\n<p>Also, for the Python code, I downloaded your image on to my computer and ran the script.  For MATLAB, you can directly reference the hyperlink to the image when reading it in with the Image Processing toolbox.</p>\n\n<hr>\n\n<h1>Python code</h1>\n\n<pre><code>import cv2 # For OpenCV modules (For Image I/O and Contour Finding)\nimport numpy as np # For general purpose array manipulation\nimport scipy.fftpack # For FFT2 \n\n#### imclearborder definition\n\ndef imclearborder(imgBW, radius):\n\n    # Given a black and white image, first find all of its contours\n    imgBWcopy = imgBW.copy()\n    contours,hierarchy = cv2.findContours(imgBWcopy.copy(), cv2.RETR_LIST, \n        cv2.CHAIN_APPROX_SIMPLE)\n\n    # Get dimensions of image\n    imgRows = imgBW.shape[0]\n    imgCols = imgBW.shape[1]    \n\n    contourList = [] # ID list of contours that touch the border\n\n    # For each contour...\n    for idx in np.arange(len(contours)):\n        # Get the i'th contour\n        cnt = contours[idx]\n\n        # Look at each point in the contour\n        for pt in cnt:\n            rowCnt = pt[0][1]\n            colCnt = pt[0][0]\n\n            # If this is within the radius of the border\n            # this contour goes bye bye!\n            check1 = (rowCnt &gt;= 0 and rowCnt &lt; radius) or (rowCnt &gt;= imgRows-1-radius and rowCnt &lt; imgRows)\n            check2 = (colCnt &gt;= 0 and colCnt &lt; radius) or (colCnt &gt;= imgCols-1-radius and colCnt &lt; imgCols)\n\n            if check1 or check2:\n                contourList.append(idx)\n                break\n\n    for idx in contourList:\n        cv2.drawContours(imgBWcopy, contours, idx, (0,0,0), -1)\n\n    return imgBWcopy\n\n#### bwareaopen definition\ndef bwareaopen(imgBW, areaPixels):\n    # Given a black and white image, first find all of its contours\n    imgBWcopy = imgBW.copy()\n    contours,hierarchy = cv2.findContours(imgBWcopy.copy(), cv2.RETR_LIST, \n        cv2.CHAIN_APPROX_SIMPLE)\n\n    # For each contour, determine its total occupying area\n    for idx in np.arange(len(contours)):\n        area = cv2.contourArea(contours[idx])\n        if (area &gt;= 0 and area &lt;= areaPixels):\n            cv2.drawContours(imgBWcopy, contours, idx, (0,0,0), -1)\n\n    return imgBWcopy\n\n#### Main program\n\n# Read in image\nimg = cv2.imread('5DnwY.jpg', 0)\n\n# Number of rows and columns\nrows = img.shape[0]\ncols = img.shape[1]\n\n# Remove some columns from the beginning and end\nimg = img[:, 59:cols-20]\n\n# Number of rows and columns\nrows = img.shape[0]\ncols = img.shape[1]\n\n# Convert image to 0 to 1, then do log(1 + I)\nimgLog = np.log1p(np.array(img, dtype=\"float\") / 255)\n\n# Create Gaussian mask of sigma = 10\nM = 2*rows + 1\nN = 2*cols + 1\nsigma = 10\n(X,Y) = np.meshgrid(np.linspace(0,N-1,N), np.linspace(0,M-1,M))\ncenterX = np.ceil(N/2)\ncenterY = np.ceil(M/2)\ngaussianNumerator = (X - centerX)**2 + (Y - centerY)**2\n\n# Low pass and high pass filters\nHlow = np.exp(-gaussianNumerator / (2*sigma*sigma))\nHhigh = 1 - Hlow\n\n# Move origin of filters so that it's at the top left corner to\n# match with the input image\nHlowShift = scipy.fftpack.ifftshift(Hlow.copy())\nHhighShift = scipy.fftpack.ifftshift(Hhigh.copy())\n\n# Filter the image and crop\nIf = scipy.fftpack.fft2(imgLog.copy(), (M,N))\nIoutlow = scipy.real(scipy.fftpack.ifft2(If.copy() * HlowShift, (M,N)))\nIouthigh = scipy.real(scipy.fftpack.ifft2(If.copy() * HhighShift, (M,N)))\n\n# Set scaling factors and add\ngamma1 = 0.3\ngamma2 = 1.5\nIout = gamma1*Ioutlow[0:rows,0:cols] + gamma2*Iouthigh[0:rows,0:cols]\n\n# Anti-log then rescale to [0,1]\nIhmf = np.expm1(Iout)\nIhmf = (Ihmf - np.min(Ihmf)) / (np.max(Ihmf) - np.min(Ihmf))\nIhmf2 = np.array(255*Ihmf, dtype=\"uint8\")\n\n# Threshold the image - Anything below intensity 65 gets set to white\nIthresh = Ihmf2 &lt; 65\nIthresh = 255*Ithresh.astype(\"uint8\")\n\n# Clear off the border.  Choose a border radius of 5 pixels\nIclear = imclearborder(Ithresh, 5)\n\n# Eliminate regions that have areas below 120 pixels\nIopen = bwareaopen(Iclear, 120)\n\n# Show all images\ncv2.imshow('Original Image', img)\ncv2.imshow('Homomorphic Filtered Result', Ihmf2)\ncv2.imshow('Thresholded Result', Ithresh)\ncv2.imshow('Opened Result', Iopen)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>\n\n<hr>\n\n<h1>MATLAB code</h1>\n\n<pre><code>clear all;\nclose all;\n\n% Read in image\nI = imread('https://i.sstatic.net/5DnwY.jpg');\n\n% Remove some columns from the beginning and end\nI = I(:,60:end-20);\n\n% Cast to double and do log.  We add with 1 to avoid log(0) error.\nI = im2double(I);\nI = log(1 + I);\n\n% Create Gaussian mask in frequency domain\n% We must specify our mask to be twice the size of the image to avoid\n% aliasing.\nM = 2*size(I,1) + 1;\nN = 2*size(I,2) + 1;\nsigma = 10;\n[X, Y] = meshgrid(1:N,1:M);\ncenterX = ceil(N/2);\ncenterY = ceil(M/2);\ngaussianNumerator = (X - centerX).^2 + (Y - centerY).^2;\n\n% Low pass and high pass filters\nHlow = exp(-gaussianNumerator./(2*sigma.^2));\nHhigh = 1 - Hlow;\n\n% Move origin of filters so that it's at the top left corner to match with\n% input image\nHlow = ifftshift(Hlow);\nHhigh = ifftshift(Hhigh);\n\n% Filter the image, and crop\nIf = fft2(I, M, N);\nIoutlow = real(ifft2(Hlow .* If));\nIouthigh = real(ifft2(Hhigh .* If));\n\n% Set scaling factors then add\ngamma1 = 0.3;\ngamma2 = 1.5;\nIout = gamma1*Ioutlow(1:size(I,1),1:size(I,2)) + ...\n       gamma2*Iouthigh(1:size(I,1),1:size(I,2));\n\n% Anti-log then rescale to [0,1]\nIhmf = exp(Iout) - 1;\nIhmf = (Ihmf - min(Ihmf(:))) / (max(Ihmf(:)) - min(Ihmf(:)));\n\n% Threshold the image - Anything below intensity 65 gets set to white\nIthresh = Ihmf &lt; 65/255;\n\n% Remove border pixels\nIclear = imclearborder(Ithresh, 8);\n\n% Eliminate regions that have areas below 160 pixels\nIopen = bwareaopen(Iclear, 160);\n\n% Show all of the results\nfigure;\nsubplot(4,1,1);\nimshow(I);\ntitle('Original Image');\nsubplot(4,1,2);\nimshow(Ihmf);\ntitle('Homomorphic Filtered Result');\nsubplot(4,1,3);\nimshow(Ithresh);\ntitle('Thresholded Result');\nsubplot(4,1,4);\nimshow(Iopen);\ntitle('Opened Result');\n</code></pre>\n\n<hr>\n\n<p>This is the result I get:</p>\n\n<h1>Python</h1>\n\n<p>Take note that I re-arranged the windows so that they're aligned in a single column.</p>\n\n<p><img src=\"https://i.sstatic.net/sXAVa.png\" alt=\"enter image description here\"></p>\n\n<h1>MATLAB</h1>\n\n<p><img src=\"https://i.sstatic.net/eH0Qb.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Data structure for circular sector in robot vision",
        "input": "",
        "output": "<p>It sounds to me that you are aware that your range data is effectively in <em>polar co-ordinates</em>.</p>\n\n<p>The uniqueness of working with such 360\u00b0 is in its circular, \u201cwrap-around\u201d nature.</p>\n\n<p>Many people end up writing their own custom implementation around this data. Their is lots of theory in the robotics literature based on it for smoothing, segmenting, finding features, etc. (for example:  \u201c<a href=\"http://link.springer.com/article/10.1023%2FB%3AJINT.0000038945.55712.65\" rel=\"nofollow\">Line Extraction in 2D Range Images for Mobile Robotics</a>\u201d.)</p>\n\n<p>Practically speaking, you might want to then consider checking out some robotics libraries. Something like <a href=\"http://robots.mobilerobots.com/wiki/ARIA\" rel=\"nofollow\">ARIA</a>. Another very good place to start is to use <a href=\"http://www.cyberbotics.com/reference/section3.23\" rel=\"nofollow\">WeBots</a> to emulate/model things - including range data - before transferring to a physical robotics platform.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to choose the number of bins when creating HSV histogram?",
        "input": "",
        "output": "<p>Disclaimer: <strong>Off-hand answers</strong> (i.e., not backed up by any documentation):</p>\n\n<p>\"256\" is a popular number for a bin size because Programmers Like Round Numbers -- it fits in a single byte. And \"180\" because the HSB circle is \"360 [degrees]\", but \"360\" does not fit into a single byte.</p>\n\n<p>For many image formats, the range of RGB values is limited to 0..255 per channel -- 3 bytes in total. To store the same amount of data (ignoring any artifacts of converting to another color model), Saturation and Brightness are often expressed in single bytes as well. The same could be done for Hue, by scaling the original range of 0..359 (as Hue is usually expressed as a value in degrees on the HSB Color Wheel) into the byte range 0..255. However, <em>probably</em> because it's easier to do calculations with a number close to the original 360\u00b0 full circle, the range is clipped to 0..179. That way the value can be stored into a single byte (and thus \"HSB\" uses as much memory as \"RGB\") and can be converted trivially back to (close to) its original value -- multiply by 2. Obviously, sticking to the storage space wins over fidelity.</p>\n\n<p>Given 256 values for both S and B, and 180 for H, you end up with a color space of 256*256*180 = 11,796,480 colors. To inspect the number of colors, you build a <a href=\"http://en.wikipedia.org/wiki/Color_histogram\" rel=\"nofollow noreferrer\">histogram</a>: an array where you can read out the total amount of pixels in a certain color or color range. Using a color <em>range</em> here, instead of actual values, significantly cuts down the memory requirements.</p>\n\n<p>For an RGB color image, with the colors fairly evenly distributed, you could shift down each channel a certain number of bits. This is how a straightforward conversion from 24-bit \"true-color\" RGB down to 15-bit RGB \"high-color\" space works: each channel gets divided by 8, reducing 256 values down to 32 (5 bits per channel). Conversion to a 16-bit high-color RGB space works the same; the bit that got left over in the 15-bit conversion is assigned to <em>green</em>. Thus, the range of colors for green is doubled, which is useful since the human eye is more perceptive for shades of green than for the other two primaries.</p>\n\n<p>It gets more complicated when the colors in the input image are <em>not</em> evenly distributed. A naive solution is to create an array of [256][256][256], initialize all to zero, then fill the array with the colors of the image, and finally sort them. There are better alternatives  -- let me consult my old <em>Computer Graphics</em> <sup>[1]</sup> here. Hold on.</p>\n\n<p><strong>13.4 Reproducing Color</strong> mentions the names of two different approaches from Heckbert (<em>Color Image Quantization for Frame Buffer Display</em>, SIGGRAPH 82): the <em>popularity</em> and the <em>median-cut</em> algorithms. (Unfortunately, that's all they say about this topic. I assume efficient code for both can be googled for.)</p>\n\n<p>A rough guess:</p>\n\n<p>The size for each bin (H,S,B) should be reflected by what you are trying to use it for. <a href=\"https://stackoverflow.com/questions/5696887/c-seeking-fast-datastructure-to-add-pixels-to-a-partitioned-hsb-histogram\">This older SO question</a>, for example, uses a large bin for <em>hue</em> -- color is considered the most important -- and only 3 different values for both saturation and brightness. Thus, bright images with <em>some</em> subdued areas (say, a comic book) will give a good spread in this histogram, but a real-color photograph will not so much.</p>\n\n<p>The main limit is that the bin sizes, multiplied with each other, should use a reasonably small amount of memory, yet cover enough of each component to get evenly filled. Perhaps some trial-and-error comes into play here. You could initially evenly distribute all of H, S, and B components over the available memory in your histogram and process a small part of the image; say, 1 out of 4 pixels, horizontally and vertically. If you notice one of the component bins fills up too fas where others stay untouched, adjust the ranges and restart.</p>\n\n<p>If you need to do an analysis of multiple pictures, make sure they are all alike in their color gamut. You cannot expect a reasonable bin size to work on <em>all</em> sorts of images; you would end up with an evenly distribution, where all matches are only so-so.</p>\n\n<hr>\n\n<p><sup>[1] <em>Computer Graphics. Principles and Practices.</em> (1997) J.D. Foley, A. van Dam, S.K. Feiner, and J.F. Hughes, 2nd ed., Reading, MA: Addison-Wesley.</sup></p>\n",
        "system": ""
    },
    {
        "instruction": "`opencv_createsamples` is missing",
        "input": "",
        "output": "<p>It should be located under <code>OpenCV-Dir\\build\\x64 or x86\\vc1x\\bin\\</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to selectively do summation of Template and Image Patch pixels based on the corresponding alpha channel pixel in Template?",
        "input": "",
        "output": "<p>I asked a similar question <a href=\"http://answers.opencv.org/question/5639/matchtemplate-with-a-mask/\" rel=\"nofollow\">here</a>. One of the answers might give you your solution.</p>\n",
        "system": ""
    },
    {
        "instruction": "Obtaining list of unique pixel values in OpenCV Mat",
        "input": "",
        "output": "<p><strong>No, there is not!</strong> You can code your own, though:</p>\n\n<pre><code>std::vector&lt;float&gt; unique(const cv::Mat&amp; input, bool sort = false)\n</code></pre>\n\n<blockquote>\n  <p>Find the unique elements of a single channel cv::Mat. </p>\n  \n  <p><strong>Parameters:</strong></p>\n  \n  <p><em>input</em>: It will be treated as if it was 1-D.</p>\n  \n  <p><em>sort</em>: Sorts the unique values (optional). </p>\n</blockquote>\n\n<p>The implementation of such function is pretty straight forward, however, the following only works with <strong>single channel</strong> <code>CV_32F</code>: </p>\n\n<pre><code>#include &lt;algorithm&gt;\n#include &lt;vector&gt;\n\nstd::vector&lt;float&gt; unique(const cv::Mat&amp; input, bool sort = false)\n{\n    if (input.channels() &gt; 1 || input.type() != CV_32F) \n    {\n        std::cerr &lt;&lt; \"unique !!! Only works with CV_32F 1-channel Mat\" &lt;&lt; std::endl;\n        return std::vector&lt;float&gt;();\n    }\n\n    std::vector&lt;float&gt; out;\n    for (int y = 0; y &lt; input.rows; ++y)\n    {\n        const float* row_ptr = input.ptr&lt;float&gt;(y);\n        for (int x = 0; x &lt; input.cols; ++x)\n        {\n            float value = row_ptr[x];\n\n            if ( std::find(out.begin(), out.end(), value) == out.end() )\n                out.push_back(value);\n        }\n    }\n\n    if (sort)\n        std::sort(out.begin(), out.end());\n\n    return out;\n}\n</code></pre>\n\n<p><strong>Example:</strong></p>\n\n<pre><code>float data[][3] = {\n  {  9.0,   3.0,  7.0 },\n  {  3.0,   9.0,  3.0 },\n  {  1.0,   3.0,  5.0 },\n  { 90.0, 30.0,  70.0 },\n  { 30.0, 90.0,  50.0 }\n};\n\ncv::Mat mat(3, 5, CV_32F, &amp;data);\n\nstd::vector&lt;float&gt; unik = unique(mat, true);\n\nfor (unsigned int i = 0; i &lt; unik.size(); i++)\n    std::cout &lt;&lt; unik[i] &lt;&lt; \" \";\nstd::cout &lt;&lt; std::endl;\n</code></pre>\n\n<p><strong>Outputs:</strong></p>\n\n<pre><code>1 3 5 7 9 30 50 70 90 \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to create Mask matrix variable for OpenCV minMaxLoc function in Python?",
        "input": "",
        "output": "<pre><code>import numpy as np\nimport cv\nfrom PIL import Image\n\n# open the image\nimg = Image.open('./pic.png', 'r')\n\nr,g,b, alpha_channel = img.split()\n\nmask = np.array(alpha_channel)\n\n# all elements in alpha_channel that have value 0 \n# are set to 1 in the mask matrix\nmask[alpha_channel==0] = 1\n\n# all elements in alpha_channel that have value 100\n# are set to 0 in the mask matrix\nmask[alpha_channel==100] = 0\n</code></pre>\n\n<p>credit to other posts:\n<a href=\"https://stackoverflow.com/questions/1962795/how-to-get-alpha-value-of-a-png-image-with-pil\">How to get alpha value of a PNG image with PIL?</a>\n<a href=\"https://stackoverflow.com/questions/10959141/converting-numpy-array-having-image-data-to-cvmat\">Converting numpy array having image data to CvMat</a></p>\n\n<p>To convert numpy array to cvmat do:</p>\n\n<pre><code>cv_arr = cv.fromarray(mask)\n</code></pre>\n\n<p>Check the dtype of mask. It should be dtype('uint8')\nWhen I convert it my cv_arr is \ncvmat(type=42424000 8UC1 rows=1245 cols=2400 step=2400 )</p>\n\n<p>I am not an expert in opencv, but it seems to me that 8UC1 is automatically selected based on the fact that dtype is uint8 (I am guessing here because I couldn't find the documentation about that).</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: Background subtracting issue when one `VideoCapture` instance is used",
        "input": "",
        "output": "<p>If you compare <code>im.data</code> and <code>im2.data</code> you will find that they are pointing to the same buffer.</p>\n\n<p>Change your code to this</p>\n\n<pre><code>Mat im,im2;\n\ncam1&gt;&gt;im;\nim = im.clone();\ncam1&gt;&gt;im2;\n</code></pre>\n\n<p>When you read a frame from VideoCapture, it does not copy the data.\nIf you want to copy the data before it gets overwritten by the next frame you have to do it yourself.</p>\n\n<p>If you have two different VideoCapture instances, you already have separate buffers so the  problem does not occur.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to detect screen tearing programmatically?",
        "input": "",
        "output": "<p>If you have control over the content being displayed, it makes it <em>much</em> easier. I recommend you do something like FCAT (or just use FCAT): See <a href=\"http://www.anandtech.com/show/6862/fcat-the-evolution-of-frame-interval-benchmarking-part-1/2\" rel=\"nofollow\">http://www.anandtech.com/show/6862/fcat-the-evolution-of-frame-interval-benchmarking-part-1/2</a> and <a href=\"http://techreport.com/review/24703/amd-radeon-hd-7990-graphics-card-reviewed/2\" rel=\"nofollow\">http://techreport.com/review/24703/amd-radeon-hd-7990-graphics-card-reviewed/2</a> and <a href=\"http://www.geforce.com/hardware/technology/fcat\" rel=\"nofollow\">http://www.geforce.com/hardware/technology/fcat</a></p>\n\n<p>Short description: It puts a different color bar down the left of each rendered frame. In the capture you can easily detect where the tears are.</p>\n",
        "system": ""
    },
    {
        "instruction": "Chilitags compilation on Linux",
        "input": "",
        "output": "<p>GCC version 4.6 is <em>far</em> from the latest version, it's up to 4.9 now.</p>\n\n<p>The problem is that the flag <code>-std=c++11</code> didn't get into GCC until version 4.7, before that it was <code>-std=c++0x</code>.</p>\n\n<p>You might also want to know that the support for C++11 (or C++0x as it was still called then) was far from complete in GCC 4.6.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding a rectangular space in an image without clear borders/edges",
        "input": "",
        "output": "<p>Your approach isn't really the way to do it. It is not efficient and very hard to automate - if you compare pixels by value, you will end up trying to find a proper pixel value range for each picture separately (because depending on the picutre, a <strong><em>similar</em></strong> pixel value will mean something different), which is troublesome and extremely time-consuming.</p>\n\n<p>Your task contains two parts: the first one is extracting the area of uniform colour, and the second is to try to fit the biggest rectangle possible inside this area.</p>\n\n<p>In order to identify the area, where you can draw your rectangle, the first step would be identifying edges of these areas. Get familiar with <strong>high-pass image filters and gradient filters</strong> - they will allow you to detect edges and thus distinguish areas that are different in terms of pixel values. </p>\n\n<p>You can try also <strong>Hough transform</strong> - it is used to identify straight lines on an image and might be especially helpful in case of your third image (the whiteboard with some text written on it), i.e. when the borders are not defined that clearly. </p>\n\n<p>In general, try googling for '<strong>edge detection</strong>', '<strong>edge extraction</strong>' etc. - this topic is well covered in many papers, image processing libraries etc. I will save you much time, and if you are going to do some serious image processing, you will have to learn it anyway.</p>\n\n<p>You won't have to implement most of these algorithms yourself if you want to test them since they are already implemented in <strong>openCV</strong>. </p>\n\n<hr>\n\n<p>After you have the edges extracted, you can for example do this: for each pixel contained within the detected area (i.e. the area in which you want to fit the rectangle):</p>\n\n<ol>\n<li><p>Try to draw a biggest rectangle possible, whose upper left corner is anchored in that pixel (if you start iterating from upper left corner of the area).</p></li>\n<li><p>Compare the dimensions of this rectangle with the biggest rectangle you were able to fit so far.</p></li>\n<li><p>Save the dimensions and position of the rectangle, if it is currently the biggest rectangle.</p></li>\n</ol>\n\n<p>After iterating through all the pixels of the selected area, you will get the biggest rectangle you can fit together with the position, at which it should be drawn.</p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab: Epipolar lines in what coordinate system?",
        "input": "",
        "output": "<p>The epipolar lines are defined in the image coordinates in pixels. If you take a look at the example in the <a href=\"http://www.mathworks.com/help/vision/ref/epipolarline.html\" rel=\"nofollow\">documentation for epipolarLine</a> you will see that line equations are defined in pixels. </p>\n\n<p><code>(x,y)</code> corresponds to <code>(col, row)</code>. See the <a href=\"http://www.mathworks.com/help/vision/gs/coordinate-systems.html\" rel=\"nofollow\">documentation</a> for more info. So if you know the row of the corresponding point, that would be the <code>y</code> value, not <code>x</code>. It seems you have a bug there.</p>\n\n<p>As far as the fractional location you are getting, there is absolutely nothing wrong with that. Think of it this way: you have a 3D world point, which happens to project onto a 2D point in image 1 with integer coordinates. Nothing says that the corresponding 2D point in image 2 must also have integer coordinates.  In fact, it is extremely unlikely that a 3D point would project to 2D points with integer coordinates in both images.</p>\n\n<p>Out of curiosity, how do you know the row of the corresponding point? </p>\n",
        "system": ""
    },
    {
        "instruction": "Detect banana or apple among the bunch of fruits on a plate with &gt; 90% success rate. (See image)",
        "input": "",
        "output": "<p>Answers to such generic questions (object detection), especially to ones like this that are very active research topics, essentially boil down to a matter of preference. That said, of the 10 \"approaches\" you mentioned, feature detection/extraction is probably the one deserving the most attention, as it's the fundamental building block of a variety of computer vision problems, including but not limited to object recognition/detection.</p>\n\n<p>A very simple but effective approach you can try is the <a href=\"http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision\" rel=\"nofollow\">Bag-of-Words</a> model, very commonly used in early attempts at fast object detection, with all global spatial relationship information lost.</p>\n\n<p>Late object detection research trend from what I observed from annual computer vision conference proceedings is that you encode each object by a graph that store feature descriptors in the nodes and store the spatial relationship information in the edges, so part of the global information is preserved, as we can now match not only the distance of feature descriptors in feature space but also the spatial distance between them in image space.</p>\n\n<p>One common pitfall specific to this problem you described is that the homogeneous texture on banana and apple skins may not warrant a healthy distribution of features and most features you detect will be on the intersections of (most commonly) 3 or more objects, which in itself isn't a commonly regarded \"good\" feature. For this reason I suggest looking into superpixel object recognition (Just Google it. Seriously.) approaches, so the mathematical model of class \"Apple\" or \"Banana\" will be a block of interconnecting superpixels, stored in a graph, with each edge storing spatial relationship information and each node storing information concerning the color distribution etc. of the neighborhood specified by the superpixel. Then recognition will be come a (partial) graph matching problem or a problem related to probabilistic <a href=\"http://en.wikipedia.org/wiki/Graphical_model\" rel=\"nofollow\">graphical model</a> with many existing research done w.r.t it.</p>\n",
        "system": ""
    },
    {
        "instruction": "Adaptive parameter for Canny Edge",
        "input": "",
        "output": "<p>If your image consist of Distinct Background &amp; Foreground, You can get the threshold for that automatically as follows explained in this paper <a href=\"http://www.academypublisher.com/proc/isip09/papers/isip09p109.pdf\" rel=\"nofollow noreferrer\">http://www.academypublisher.com/proc/isip09/papers/isip09p109.pdf</a>.</p>\n<ol>\n<li>Compute Otsu's threshold + Binary threshold for your image.</li>\n<li>Use the Otsu's threshold value as higher threshold for Canny's algorithm.</li>\n</ol>\n<h3>CODE:</h3>\n<pre><code>Mat mCanny_Gray,mThres_Gray;\nMat mSrc_Gray=imread(&quot;Test.bmp&quot;,0);\n\ndouble CannyAccThresh = threshold(mSrc_Gray,mThres_Gray,0,255,CV_THRESH_BINARY|CV_THRESH_OTSU);\n\ndouble CannyThresh = 0.1 * CannyAccThresh;\n\nCanny(mSrc_Gray,mCanny_Gray,CannyThresh,CannyAccThresh);\nimshow(&quot;mCanny_Gray&quot;,mCanny_Gray);\n</code></pre>\n<p>You can also refer <a href=\"https://stackoverflow.com/questions/4292249/automatic-calculation-of-low-and-high-thresholds-for-the-canny-operation-in-open\">this thread.</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Pairwise Distance calculation (multidimentional matrix) for features similarity",
        "input": "",
        "output": "<p>Minor comment before I start:</p>\n\n<p>You should really use <code>pdist2</code> instead.  This is much faster and you'll get the same results as <code>dumDistance</code>.  In other words, you would call it like this:</p>\n\n<pre><code>D = pdist2(page.', dictionary.');\n</code></pre>\n\n<p>You need to transpose <code>page</code> and <code>dictionary</code> as <code>pdist2</code> assumes that each <strong>row</strong> is an observation, while each column corresponds to a variable / feature.  Your data is structured such that each <strong>column</strong> is an observation.  This will return a <code>40 x 100</code> matrix like what you see in <code>dumDistance</code>.  However, <code>pdist2</code> <strong>does not use <code>for</code> loops</strong>.</p>\n\n<hr>\n\n<p>Now onto your question:</p>\n\n<p><code>D(i,j)</code> represents the Euclidean <strong>squared</strong> distance between word <code>i</code> from your page and word <code>j</code> from your dictionary.  You have 40 words on your page and 100 words in your dictionary.  Each word is represented by a 140 dimensional feature vector, and so the rows of <code>D</code> index the words of <code>page</code> while the columns of <code>D</code> index the words of <code>dictionary</code>.  </p>\n\n<p>What I mean here in terms of \"distance\" is in terms of the feature space.  Each word from your page and dictionary are represented as a 140 length vector.  Each entry <code>(i,j)</code> of <code>D</code> takes the i<sup>th</sup> vector from <code>page</code> and the j<sup>th</sup> vector from <code>dictionary</code>, each of their corresponding components subtracted, squared, and then they are summed up.  This output is then stored into <code>D(i,j)</code>.  This gives you the dissimilarity between word <code>i</code> from your <code>page</code> and word <code>j</code> from your <code>dictionary</code> at <code>D(i,j)</code>.  The higher the value, the more <strong>dissimilar</strong> the two words are.</p>\n\n<p><strong>Minor Note:</strong> <code>pdist2</code> computes the Euclidean distance while <code>dumDistance</code> computes the Euclidean <strong>squared</strong> distance.  If you want to have the same thing as <code>dumDistance</code>, simply square every element in <code>D</code> from <code>pdist2</code>.  In other words, simply compute <code>D.^2</code>.</p>\n\n<p>Hope this helps.  Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "How does the function detectMSERFeatures in matlab work?",
        "input": "",
        "output": "<p>There is a section in the detectMSERFeatures documentation (under the More About section) that may help you better understand the algorithm:</p>\n\n<p><a href=\"http://www.mathworks.com/help/vision/ref/detectmserfeatures.html\" rel=\"nofollow\">http://www.mathworks.com/help/vision/ref/detectmserfeatures.html</a></p>\n\n<p>You can get nested regions when you have an image intensity profile that that looks like a \"V\". If you use the bucket filling analogy from the documentation, then as you start to slowly pour water into the \"V\", stable regions will begin to appear and will be labeled as an MSER region. As you continue pouring water, larger stable regions will form and they will contain the previously found MSER regions.</p>\n",
        "system": ""
    },
    {
        "instruction": "Transparent background for training OpenCV",
        "input": "",
        "output": "<ol>\n<li>Yes</li>\n<li>Yes -> This makes selecting the background color much, much easier.</li>\n<li>The data of the backgrounds will be lost, but in most cases this is what you want. Otherwise, the trained data might recnogise the background more than the object.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Matlab plot3 not working in Stereo Calibration and Scene Reconstruction example",
        "input": "",
        "output": "<p>Are you running the example code as is, or are you using your own images? If you are using your own images, and z's are negative, that would indicate that your camera1 is to the right of camera2. If you switch the images so that camera1 is on the left, the z's should come out positive. Alternatively, you can keep things as they are, and find the appropriate negative limits for z.</p>\n\n<p><strong>Edit</strong>: As of R2015b release there is a function called <a href=\"http://www.mathworks.com/help/vision/ref/pcshow.html\" rel=\"nofollow\">pcshow</a> for visualizing point clouds.</p>\n",
        "system": ""
    },
    {
        "instruction": "Dsift (vl_feat) and Matlab",
        "input": "",
        "output": "<p>This is because the number of reliable features that are detected per image change.  Just because you detect 10 features in one image does <strong>not</strong> mean that you will be able to detect the same number of features in the other image.  What <strong>does matter</strong> is how close one feature from one image matches with another.  </p>\n\n<p>What you can do (if you like) is extract the, say, 10 <strong>most reliable</strong> features that are matched the best between the two images, if you want to have something constant.  Choose a number that is less than or equal to the minimum of the number of patches detected between the two.  For example, supposing you detect 50 features in one image, and 35 features in another image.  After, when you try and match the features together, this results in... say... 20 best matched points.  You can choose the best 10, or 15, or even all of the points (20) and proceed from there.</p>\n\n<p>I'm going to show you some example code to illustrate my point above, but bear in mind that I will be using <code>vl_sift</code> and not <code>vl_dsift</code>.  The reason why is because I want to show you visual results with minimal pre- and post-processing.  Should you choose to use <code>vl_dsift</code>, you'll need to do a bit of work before and after you compute the features by <code>dsift</code> if you want to visualize the same results.  If you want to see the code to do that, you can check out the <code>vl_dsift</code> help page here: <a href=\"http://www.vlfeat.org/matlab/vl_dsift.html\" rel=\"nofollow\">http://www.vlfeat.org/matlab/vl_dsift.html</a>.  Either way, the idea about choosing the most reliable features applies to both <code>sift</code> and <code>dsift</code>.</p>\n\n<p>For example, supposing that <code>Ia</code> and <code>Ib</code> are <code>uint8</code> grayscale images of the same object or scene.  You can first detect features via SIFT, then match the keypoints.</p>\n\n<pre><code>[fa, da] = vl_sift(im2single(Ia));\n[fb, db] = vl_sift(im2single(Ib));\n[matches, scores] = vl_ubcmatch(da, db);\n</code></pre>\n\n<p><code>matches</code> contains a <code>2 x N</code> matrix, where the first row and second row of each column denotes which feature index in the first image (first row) matched best with the second image (second row).</p>\n\n<p>Once you do this, sort the scores in <strong>ascending</strong> order.  Lower scores mean better matches as the default matching method between two features is the Euclidean / L<sub>2</sub> norm.  As such:</p>\n\n<pre><code>numBestPoints = 10;\n[~,indices] = sort(scores);\n\n%// Get the numBestPoints best matched features\nbestMatches = matches(:,indices(1:numBestPoints));\n</code></pre>\n\n<p>This should then return the 10 best matches between the two images.  FWIW, your understanding about how the features are represented in <code>vl_feat</code> is spot on.  These are stored in <code>da</code> and <code>db</code>.  Each column represents a descriptor of a particular patch in the image, and it is a histogram of 128 entries, so there are 128 rows per feature.</p>\n\n<p>Now, as an added bonus, if you want to display how each feature from one image matches to another image, you can do the following:</p>\n\n<pre><code>%// Spawn a new figure and show the two images side by side\nfigure;\nimagesc(cat(2, Ia, Ib));\n\n%// Extract the (x,y) co-ordinates of each best matched feature\nxa = fa(1,bestMatches(1,:));\n\n%// CAUTION - Note that we offset the x co-ordinates of the\n%// second image by the width of the first image, as the second\n%// image is now beside the first image.\nxb = fb(1,bestMatches(2,:)) + size(Ia,2);\nya = fa(2,bestMatches(1,:));\nyb = fb(2,bestMatches(2,:));\n\n%// Draw lines between each feature\nhold on;\nh = line([xa; xb], [ya; yb]);\nset(h,'linewidth', 1, 'color', 'b');\n\n%// Use VL_FEAT method to show the actual features\n%// themselves on top of the lines\nvl_plotframe(fa(:,bestMatches(1,:)));\nfb2 = fb; %// Make a copy so we don't mutate the original\nfb2(1,:) = fb2(1,:) + size(Ia,2); %// Remember to offset like we did before\nvl_plotframe(fb2(:,bestMatches(2,:)));\naxis image off; %// Take out the axes for better display\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Fusing Multiple Haar Classifiers for face detection",
        "input": "",
        "output": "<p>there's a (undocumented) overload for CascadeClassifier::detectMultiScale():</p>\n\n<pre><code>virtual void detectMultiScale( InputArray image,\n                               CV_OUT std::vector&lt;Rect&gt;&amp; objects,\n                               CV_OUT std::vector&lt;int&gt;&amp; rejectLevels,\n                               CV_OUT std::vector&lt;double&gt;&amp; levelWeights,\n                               double scaleFactor,\n                               int minNeighbors, int flags,\n                               Size minSize, Size maxSize,\n                               bool outputRejectLevels ) = 0;\n</code></pre>\n\n<p>the rejectLevels should be the confidence values you're looking for</p>\n",
        "system": ""
    },
    {
        "instruction": "imwrite does not create image",
        "input": "",
        "output": "<p>This is not documented clearly, but <code>imwrite</code> returns a boolean which is <code>true</code> if and only if it thinks that it could write the file successfully. You should check that value!</p>\n\n<p>You'll probably find out that <code>imwrite</code> returns <code>false</code>. Most likely you do not have sufficient permissions or -- as berak pointed out -- your file path is invalid.</p>\n\n<p>By the way, for proper error handling, you should also catch exceptions, in particular, if the user provides the output file URL. If OpenCV, for some reason, can't find a suitable encoder (i.e. it can't recognize which type of file you are going to write), it will throw an exception.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is the difference between dense SIFT and HoG?",
        "input": "",
        "output": "<p>SIFT descriptor chooses a 16x16 and then divides it into 4x4 windows. Over each of these 4 windows it computes a Histogram of Oriented gradients. While computing this histogram, it also performs an interpolation between neighboring angles. Once you have all the 4x4 windows, it uses a gaussian of half the window size, centered at the center of the 16x16 block to weight the values in the whole 16x16 descriptor.</p>\n\n<p>HoG on the other hand only computes a simple histogram of oriented gradients as the name says.</p>\n\n<p>I feel that SIFT is more suited in describing the importance of a point, due to the gaussian weighting involved, while HoG does not have such a bias. Due to this reason, (ideally) HoG should be better suited at classification of images over dense SIFT, if all feature vectors are concatenated into one huge vector (this is my opinion, may not be true)</p>\n",
        "system": ""
    },
    {
        "instruction": "Bundle Adjustment for Image Stitching",
        "input": "",
        "output": "<p>I think you have tried the first two steps and now the third step is the refinement through a bundler. I would strongly recommend you to read Hartley Zisserman book , Multiple view geometry, Appendix 6. Clear pseudo code is provided for LM based least square estimation for error refinement.</p>\n\n<p>Now for your problem, you are trying to refine the homography to get a better match. So your error is a photometric error, similar to x1'Hx2. This case is also provided in the appendix.</p>\n",
        "system": ""
    },
    {
        "instruction": "Matching closest image in openCV?",
        "input": "",
        "output": "<p>Go to github and check out opencv-master\\samples\\cpp\\matcher_simple.cpp(or matching_to_many_images.cpp)</p>\n\n<p>not only can it satisfy your need but it also works for images with perspective distortion (eg. rotation, affine transformation and illumination variation). simply put, it's very robust.</p>\n\n<p>but SIFT and SURF are patented, you might not be able to use it for commercial applications, which sucks. but there are many alternatives, just google around!</p>\n",
        "system": ""
    },
    {
        "instruction": "how to fix slow kmeans of opencv",
        "input": "",
        "output": "<p>The k-means implementation in OpenCV is very inefficient, and there are a number of tricks to improve performance that they do not sure. It would be considerable work to re-write it yourself. </p>\n\n<p>The implementation in <a href=\"http://www.vlfeat.org/overview/kmeans.html\" rel=\"nofollow\">VLfeat</a> offers better algorithms for k-means, but I don't know about the quality of the implementation. </p>\n",
        "system": ""
    },
    {
        "instruction": "Structure From Motion: Implementing Triangulation",
        "input": "",
        "output": "<p>To create a homogeneous coordinate from the 2D point, you have to replace the 0 in the ray calculation by a 1.</p>\n",
        "system": ""
    },
    {
        "instruction": "Stitch multiple images using OpenCV (Python)",
        "input": "",
        "output": "<p>Step by step, assuming you want to stitch four images <code>I0</code>, <code>I1</code>, <code>I2</code>, <code>I3</code>, your goal is to compute homographies <code>H_0</code>, <code>H_1</code>, <code>H_2</code>, <code>H_3</code>;</p>\n\n<ol>\n<li>Compute all pairwise homographies <code>H_01</code>, <code>H_02</code>, <code>H_03</code>, <code>H_12</code>, <code>H_13</code>, <code>H_23</code> where homography <code>H_01</code> warps image <code>I0</code> into <code>I1</code>, etc...</li>\n<li>Select one anchor image e.g. <code>I1</code> which position will remain fixed i.e <code>H_1</code> = Identity</li>\n<li>Find image that better align with <code>I1</code> based on maximum number of\nconsistent matches e.g. <code>I3</code></li>\n<li>Update <code>H_3</code> = <code>H_1 * inv(H_13)</code> = <code>inv(H_13)</code> = <code>H_31</code></li>\n<li>Find image that better matches <code>I1</code> or <code>I3</code> e.g <code>I2</code> matching <code>I3</code></li>\n<li>Update <code>H_2</code> = <code>H_3</code> * <code>H_23</code></li>\n<li>Same as above for image <code>I0</code></li>\n<li>Do bundle adjustment to globally optimize alignment</li>\n</ol>\n\n<p>See section 4 of this seminal paper <a href=\"https://web.archive.org/web/20140115053733/http://cs.bath.ac.uk:80/brown/papers/ijcv2007.pdf\" rel=\"noreferrer\">Automatic Panoramic Image Stitching using Invariant Features</a> for an in depth explanation.</p>\n",
        "system": ""
    },
    {
        "instruction": "Is it possible in Matlab to plot not matching SURFPoints?",
        "input": "",
        "output": "<p>You can get the (x,y) locations of the points stored in an M-by-2 matrix by using the <code>Location</code> property of the <code>SURFPoints</code> object. Then you can get the unmatched points using logical indexing:</p>\n\n<pre><code>targetPointsLoc = targetPoints.Location;\nunmatchedIdx = true(size(targetPoitnsLoc, 1), 1);\nunmatchedIdx(boxPairs(:, 2)) = false;\nunmatchedTargetPoints = targetPointsLoc(unmatchedIdx, :);\n</code></pre>\n\n<p>Now you can use <code>plot</code> to display the unmatched points.</p>\n\n<p>Out of curiosity, why do you care about the unmatched points?</p>\n",
        "system": ""
    },
    {
        "instruction": "Image segmentation for screenshots (Region of Interest)",
        "input": "",
        "output": "<p>Please take a look at the new OmniParser by Microsoft Research <a href=\"https://microsoft.github.io/OmniParser/\" rel=\"nofollow noreferrer\">https://microsoft.github.io/OmniParser/</a></p>\n<p>It was trained by collecting and rendering a lot of DOM structures on top of YOLO8 and BLIP-2</p>\n",
        "system": ""
    },
    {
        "instruction": "Deep Belief Networks vs Convolutional Neural Networks",
        "input": "",
        "output": "<p>Generally speaking, DBNs are generative neural networks that stack Restricted Boltzmann Machines (RBMs) . You can think of RBMs as being generative autoencoders; if you want a deep belief net you should be stacking RBMs and not plain autoencoders as Hinton and his student Yeh proved that stacking RBMs results in sigmoid belief nets.</p>\n\n<p>Convolutional neural networks have performed better than DBNs by themselves in current literature on benchmark computer vision datasets such as MNIST. If the dataset is not a computer vision one, then DBNs can most definitely perform better. In theory, DBNs should be the best models but it is very hard to estimate joint probabilities accurately at the moment. You may be interested in Lee et. al's (2009) work on Convolutional Deep Belief Networks which looks to combine the two.</p>\n",
        "system": ""
    },
    {
        "instruction": "object detection LEDs in simple scene",
        "input": "",
        "output": "<p>I think it should go without explicitly saying so, but there are probably hundreds of things that could be tried, and with only one example image it is quite difficult to advise.  For instance are the LED always green?  we don't know.  </p>\n\n<p>That aside, imho, two good places to start would be with the ol' faithful template matching, or blob detection.</p>\n\n<p>Then if that is not robust enough, you will need to look at some alternative representations of the template/blob, like the classic HoG (good for shape, maybe a bit heavy this app.), or even your own bespoke one that encodes your own domain specific knowledge of this problem. </p>\n\n<p>Then if that is not robust enough, build a dataset of representative +ve and -ve examples, as big as you can, and then train a machine like svm , or a boosted classifier.</p>\n\n<p>Template Matching:</p>\n\n<p><a href=\"http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html\" rel=\"nofollow\">http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html</a></p>\n\n<p>Blob detection:</p>\n\n<p><a href=\"https://code.google.com/p/cvblob/\" rel=\"nofollow\">https://code.google.com/p/cvblob/</a>   </p>\n\n<p>Machine Learning:</p>\n\n<p><a href=\"http://docs.opencv.org/modules/ml/doc/ml.html\" rel=\"nofollow\">http://docs.opencv.org/modules/ml/doc/ml.html</a> </p>\n\n<p>TIPS: \nAdd as much domain knowledge as possible, i.e. if they are always green, use color in the representation, like hog on g channel for instance.  If they are always circular, try to encode that, like use a log-polar grid in the template,rather than a regular grid... and so on.</p>\n\n<p>Machine Learning is not magic, a linear classifier will essentially weight different points in the feature space, so you still require a good representation, so if the Template matching was a total fail, the it is unlikely that simple linear ml with help, but if the Template matching was okay, then ml may well boost the performance to a good level.</p>\n",
        "system": ""
    },
    {
        "instruction": "Barrel Distortion - Correcting image when expected/received control points are known",
        "input": "",
        "output": "<p>Hum, can you explain why the standard calibration techniques aren't suitable? You don't need to know the \"true\" camera parameters, but you do need to estimate the linear (actually, affine) part of the distortion, which is almost the same thing.</p>\n\n<p>Explanation: assuming you are dealing with a plain old spherical-like lens, the first model I'd try for your case is a two-parameter radial distortion of the form:</p>\n\n<pre><code>X = f * |x - c|\nY = k1 * X^2 + k2 * X^4\ny = c + Y / f\n</code></pre>\n\n<p>where</p>\n\n<ul>\n<li>x = (u, v) are the distorted pixel coordinates; </li>\n<li>c = (cu, cv) is an unknown center of distortion (i.e. the place in the image with zero\ndistortion, usually on (or very close to) the lens's focal axis. </li>\n<li>|x -c| is the radial distance of x from c in the distorted image</li>\n<li>f is an unknown scale factor</li>\n<li>X is the location of the distorted pixel in scaled-centered coordinates</li>\n<li>k1 and k2 are unknown distortion coefficients</li>\n<li>Y is the undistorted pixel in scaled-centered coordinates</li>\n<li>y is the undistorted pixel, located on the same radius c->x as x, at a distance Y/f from c.</li>\n</ul>\n\n<p>So your unknowns are cu, cv, f, k1 and k2. It's starting to look like a camera calibration problem, isn't it? </p>\n\n<p>Except you don't really need to estimate a \"true\" focal length f, since (I assume) you are not interested in computing rays in 3D space. So you can simplify the problem by assigning f as the value that makes the diameter of your data point distribution equal to, say, 2, so that all the centered-scaled points X will have coordinates no larger than 1.0 in absolute value. This helps in two ways: it improves the numerical conditioning of the problem, and drops the number of unknowns to 4. </p>\n\n<p>You can usually initialize the estimation by using the center of your image for c, and zero values for k1 and k2, plug your data in your favorite least-squares optimizer, run, get the solution for the unknowns, and verify that it makes sense (on additional independent images). Rinse and repeat until you get something satisfactory.</p>\n\n<p>Note that you can enrich the data set used for the estimation by using more than one image, assuming, of course, that the lens parameters are constant.</p>\n",
        "system": ""
    },
    {
        "instruction": "Car Tracking using Optical Flow. Why isnt the vectors plotting properly",
        "input": "",
        "output": "<p>Try using </p>\n\n<pre><code>lines = [Rv(:); Cv(:); Rv(:)+H(:); Cv(:)+V(:)];\n</code></pre>\n\n<p>instead of</p>\n\n<pre><code>lines = [Rv;Cv; Rv+H(:)'; Cv+V(:)'];\n</code></pre>\n\n<p>Better yet, if you have a recent version of Matlab, try using the <code>insertShape</code> function instead of <code>vision.ShapeInserter</code>.</p>\n\n<p><strong>Edit:</strong>\nIf you have a recent version of the Computer Vision System Toolbox, try the new optical flow functions: <code>opticalFlowHS</code>, <code>opticalFlowLK</code>, <code>opticalFlowLKDoG</code>, and <code>opticalFlowFarneback</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "3D morphable model fitting to an image ( face reconstruction)",
        "input": "",
        "output": "<p><strong>Update</strong>...</p>\n\n<p>If you're really just after some code - perhaps these specific face fitting/tracking libraries might be more what you are after (although I know they are more tending towards Active Appearance Models...)</p>\n\n<ul>\n<li><a href=\"https://github.com/kylemcdonald/FaceTracker\" rel=\"nofollow\">https://github.com/kylemcdonald/FaceTracker</a> </li>\n<li><a href=\"http://face.ci2cv.net\" rel=\"nofollow\">http://face.ci2cv.net</a></li>\n<li><a href=\"http://staff.estem-uc.edu.au/roland/research/demolib-home/\" rel=\"nofollow\">http://staff.estem-uc.edu.au/roland/research/demolib-home/</a></li>\n</ul>\n\n<p>I'm afraid that I don't have code for exactly what you're trying (it's been 7 years since I did any of this stuff I things have moved on somewhat. Good luck!)</p>\n\n<p><strong>...</strong></p>\n\n<p>Model issues aside - it sounds to me that you know what you are doing, and just need to express it in a form that you can use with any number of optimisation libraries out there...</p>\n\n<p>I\u2019m sure you probably know all this - but for this particular problem, short of asking the authors themselves for the code, or implementing it yourself, you are left with existing libraries. The issue then is often tailoring your model to work with them (hence my earlier questions).</p>\n\n<p>e.g.</p>\n\n<p><a href=\"http://www.gnu.org/software/gsl/manual/html_node/Multimin-Algorithms.html\" rel=\"nofollow\">http://www.gnu.org/software/gsl/manual/html_node/Multimin-Algorithms.html</a></p>\n\n<p><a href=\"https://software.sandia.gov/opt++/\" rel=\"nofollow\">https://software.sandia.gov/opt++/</a></p>\n\n<p>In turn used by many higher-level libraries, such as...</p>\n\n<p><a href=\"http://docs.scipy.org/doc/scipy/reference/optimize.html\" rel=\"nofollow\">http://docs.scipy.org/doc/scipy/reference/optimize.html</a></p>\n\n<p><a href=\"http://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html\" rel=\"nofollow\">http://stat.ethz.ch/R-manual/R-devel/library/stats/html/optim.html</a></p>\n\n<p><a href=\"http://weka.sourceforge.net/doc.dev/weka/core/Optimization.html\" rel=\"nofollow\">http://weka.sourceforge.net/doc.dev/weka/core/Optimization.html</a></p>\n\n<p>etc.</p>\n\n<p>I think I can anticipate your core problem, which is that many of these ``off-the-shelf'' methods will not deliver the best fitting - since the number of parameters is pretty big and (as you hint at) are hard to factor.</p>\n\n<p>As is so often the case with any form of model-fitting in Computer Vision, key advances are linked to fundamentally better optimisation (algorithm) and representation (data-structure/model).</p>\n\n<p>So you could look at more experimental libraries, as opposed to the established ones above:</p>\n\n<p><a href=\"http://deeplearning.net/software/pylearn2/\" rel=\"nofollow\">http://deeplearning.net/software/pylearn2/</a></p>\n\n<p><a href=\"http://ab-initio.mit.edu/wiki/index.php/NLopt\" rel=\"nofollow\">http://ab-initio.mit.edu/wiki/index.php/NLopt</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How do you decide the parameters of a Convolutional Neural Network for image classification?",
        "input": "",
        "output": "<p><strong>The Number of hidden layers:</strong>\nThe number of hidden layers required depends on the intrinsic complexity of your dataset, this can be understood by looking at what each layer achieves:</p>\n\n<ul>\n<li><p>Zero hidden layers allow the network to model only a linear function. This is inadequate for most image recognition tasks.</p></li>\n<li><p>One hidden layer allows the network to model an <em>arbitrarily complex</em> function. This is adequate for many image recognition tasks.</p></li>\n<li><p>Theoretically, two hidden layers offer little benefit over a single layer,  however, in practice some tasks may find an additional layer beneficial. This should be treated with caution, as a second layer can cause over-fitting. Using more than two hidden layers is <del>almost never beneficial</del> only beneficial for especially complex tasks, or when a very large amount of training data is available (updated based on Evgeni Sergeev comment).</p></li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/tpGdY.png\" alt=\"enter image description here\"></p>\n\n<p>To cut a long story short, if you have time then test both one and two hidden layers to see which achieves the most satisfactory results. If you do not have time then you should take a punt on a single hidden layer, and you will not go far wrong.</p>\n\n<p><strong>The Number of convolutional layers:</strong>\nIn my experience, the more convolutional layers the better (within reason, as each convolutional layer reduces the number of input features to the fully connected layers), although after about two or three layers the accuracy gain becomes rather small so you need to decide whether your main focus is generalisation accuracy or training time. That said, all image recognition tasks are different so the best method is to simply try incrementing the number of convolutional layers one at a time until you are satisfied by the result.</p>\n\n<p><strong>The number of nodes per hidden layer:</strong>\n...Yet again, there is no magic formula for deciding upon the number of nodes, it is different for each task. A rough guide to go by is to use a number of nodes 2/3 the size of the previous layer, with the first layer 2/3 the size of the final feature maps. This however is just a rough guide and depends again on the dataset. Another commonly used option is to start with an excessive number of nodes, then to remove the unnecessary nodes through <em>pruning</em>.</p>\n\n<p><strong>Max pooling window size:</strong>\nI have always applied max pooling straight after convolution so am perhaps not qualified to make suggestions on the window size you should use. That said, 19x19 max pooling seems overly severe since it literally throws  most of your data away. Perhaps you should look at a more conventional LeNet network layout: </p>\n\n<p><a href=\"http://deeplearning.net/tutorial/lenet.html\" rel=\"noreferrer\">http://deeplearning.net/tutorial/lenet.html</a></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=n6hpQwq7Inw\" rel=\"noreferrer\">https://www.youtube.com/watch?v=n6hpQwq7Inw</a></p>\n\n<p>In which you repeatedly perform convolution(5x5 or 3x3 usually) followed by max pooling (usually with a 2x2 pooling window, although 4x4 can be necessary for large input images).</p>\n\n<p><strong>In Conclusion</strong>\nThe best way to find a suitable network layout is literally to perform trial and error tests. Lots of tests. There is no one-size-fits-all network, and only you know the intrinsic complexity of your dataset. The most effective way of performing the number of necessary tests is through <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\" rel=\"noreferrer\">cross validation</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Object Recognition using Android API",
        "input": "",
        "output": "<p>For general objects in 3D, that is an unsolved problem in computer vision right now. A lot of researchers are working on it, but right now computers cannot reliably identify that an arbitrary object they haven't seen before is a \"chair\", for example. (If you think about it, such labeling actually requires a lot of judgment and world knowledge to know what kinds of things humans can sit on, and that's beyond the current state of AI for objects in general.)</p>\n\n<p>There are algorithms that basically do a Google Image Search: they take a given picture and use some fairly advanced computer vision to find similar-looking pictures on the web (i.e. Google Goggles). There are APIs for those; check out\n<a href=\"https://stackoverflow.com/questions/2080731/google-goggles-api\">Google goggles API</a></p>\n\n<p>Those work well for 2D pictures, like posters and product logos, that always look exactly the same, but not for things like plants and animals.</p>\n",
        "system": ""
    },
    {
        "instruction": "How do I use BackgroundSubtractorMOG in OpenCV 3.0.0 with Java?",
        "input": "",
        "output": "<p>Seems like here <a href=\"https://stackoverflow.com/questions/30288953/creating-backgroundsubtractormog2-in-android-by-opencv-3-0-rc1\">opencv3.0 creating backgroundsubtractormog2</a> they solved it</p>\n\n<p>you need to do it via Video.createBackgroundSubtractorMog2() </p>\n",
        "system": ""
    },
    {
        "instruction": "how can i apply blob extraction on an edge image?",
        "input": "",
        "output": "<p>To improve your results, you could try:</p>\n\n<ol>\n<li>low pass filtering (Gaussian or median) the image before edge detection</li>\n<li>morphological operations (erosion, dilation, open, close) after binarizing</li>\n<li>Removing certain blobs by their size, shape etc.</li>\n</ol>\n\n<p>However, none of these can guarantee very accurate object detection. Other possible approaches are:</p>\n\n<ol>\n<li>If you have an image/s of the scene without objects, you can use background subtraction</li>\n<li>If you know characteristics/features of objects, you can use segmentation or object recognition using those features.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Generate an image that can be most easily detected by Computer Vision algorithms",
        "input": "",
        "output": "<p>I think you need something like AR markers.\nTake a look at <a href=\"http://www.hitl.washington.edu/artoolkit/\" rel=\"nofollow\">ArToolkit</a>, <a href=\"http://handheldar.icg.tugraz.at/artoolkitplus.php\" rel=\"nofollow\">ArToolkitPlus</a> or <a href=\"http://www.uco.es/investiga/grupos/ava/node/26\" rel=\"nofollow\">Aruco</a> libraries, they have marker generators and detectors. \nAnd papeer about marker generation: <a href=\"http://www.uco.es/investiga/grupos/ava/sites/default/files/GarridoJurado2014.pdf\" rel=\"nofollow\">http://www.uco.es/investiga/grupos/ava/sites/default/files/GarridoJurado2014.pdf</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Making 3D model from pictures - writing the stereo matching part",
        "input": "",
        "output": "<p>The link on the page refers to a yet-to-be published paper:  </p>\n\n<blockquote>\n  <p>[143] Anonymous. Accurate stereo matching by two step global\n  optimization. ECCV 2014 submission 74.</p>\n</blockquote>\n\n<p>You'll have to wait until <a href=\"http://eccv2014.org/\" rel=\"nofollow\">ECCV 2014 (September 6-12, 2014)</a> to read it.</p>\n\n<p>Meanwhile, you can take a look at <a href=\"http://opencv.org\" rel=\"nofollow\">OpenCV</a>. It implements several stereo algorithms and get help you get up and running with the setup. Once you write your own implementation, you can contribute it to the community via OpenCV.</p>\n",
        "system": ""
    },
    {
        "instruction": "Why does linear svms work well with HoG descriptors?",
        "input": "",
        "output": "<p>For more information read journal paper named, \"<a href=\"https://arxiv.org/pdf/1406.2419.pdf\" rel=\"nofollow\">Why do linear SVMs trained on HOG features perform so well?</a> \" Hilton Bristow, Simon Lucey (2014)</p>\n",
        "system": ""
    },
    {
        "instruction": "Opencv Object tracking and count objects which passes ROI in video frame",
        "input": "",
        "output": "<p>Your question might be to broad when you are asking about general technique that count moving objects in video sequences. I would give some hints that might help you:</p>\n\n<ul>\n<li>As usual in computer vision, there does not exist one specific way to solve your problem. Try do do some research about <em>people detection, background extraction</em> and <em>motion detection</em> to have a wider point of view</li>\n<li>State more clearly user requirements of your system, namely how many people can occur in the image frame? The things get complicated when you would like to track <em>more</em> than one person. Furthermore, can other moving objects appear on an image (e.g. animals)? If no and only one person are supposed to be track, the answer to your problem is pretty easy, see an explanation below. If yes, you will have to do more research.</li>\n<li>Usually you cannot find in OpenCV API direct solution to computer vision problem, namely there is not such method that solve directly problem of people counting. But for sure there exists some paper, reference (usually some scientific stuff) which can be adopted to solve your problem. So there is no method that \"count people crossing vertical line\". You have to solve problem my merging some algorithms together.</li>\n</ul>\n\n<p>In the link you have provided one can see that they use some algorithm for background extraction which determined what is a non-moving background and moving foreground (in our case, a walking person). We are not sure if they use something more (or sophisticated), but information about background extraction is sufficient to start with problem solving.</p>\n\n<p>And here is my contribution to the solution. Assuming only one person walks in front of the stable placed camera and no other objects motion can be observed, do as following:</p>\n\n<ol>\n<li>Save frame when no person is moving in front of the camera, which will be used later as a reference for <em>background</em></li>\n<li>In a loop, apply some background detector to extract parts in the image representing motion (<a href=\"http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html?highlight=backgroundsubtractorMOG#backgroundsubtractormog\" rel=\"nofollow noreferrer\">MOG</a> or even you can just calculate difference between background and current frame, followed by binary threshold and blob counting, see my answer <a href=\"https://stackoverflow.com/questions/17155005/backgound-extraction/17170690#17170690\">here</a>)</li>\n<li>From the assumption, only one blob should be detected (if not, use some metrics the chooses \"the best one\". for example choose the one with maximum area). That blob is the person we would like to track. Knowing its position on an image, compare to the position of the \"vertical line\". Objects moving from left to right are exiting and from right to left entering. </li>\n</ol>\n\n<p>Remember that this solution will only work in case of the assumption we stated. </p>\n",
        "system": ""
    },
    {
        "instruction": "How to remove similar images based on hog features?",
        "input": "",
        "output": "<p>In another way(not using hog features) you can compute <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.html\" rel=\"nofollow\">color histogram</a> for each image and compare against others.</p>\n\n<p>Like,</p>\n\n<ol>\n<li><p>Get the first image and compute the histogram, </p></li>\n<li><p>Now for each other images calculate histogram and <a href=\"http://docs.opencv.org/modules/imgproc/doc/histograms.html?highlight=comparehist#comparehist\" rel=\"nofollow\">compare</a> with the first one.</p></li>\n</ol>\n\n<p>If you find close match on the histogram you can discard it. And by using <code>CV_COMP_CORREL</code> you will get match in the range of 0-1. </p>\n",
        "system": ""
    },
    {
        "instruction": "Align depth image to RGB image",
        "input": "",
        "output": "<p>First, check your calibration numbers. Your rotation matrix is approximately the identity and, assuming your calibration frame is metric, your translation vector says that the second camera is 2 centimeters to the side and one centimeter displaced in depth. Does that approximately match your setup? If not, you may be working with the wrong scaling (likely using a wrong number for the characteristic size of your calibration target - a checkerboard?).</p>\n\n<p>Your code looks correct - you are re-projecting a pixel of the depth camera at a known depth, and the projecting it back in the second camera to get at the corresponding rgb value.</p>\n\n<p>One think I would check is whether your using your coordinate transform in the right direction. IIRC, OpenCV produces it as [R | t], but you are using it as [R | -t], which looks suspicious. Perhaps you meant to use its inverse, which would be [R' | -R'*t ], where I use the apostrophe to mean transposition.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is cross check in computer vision?",
        "input": "",
        "output": "<p>From the <a href=\"http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html?highlight=bfmatcher#BFMatcher::BFMatcher(int%20normType,%20bool%20crossCheck)\" rel=\"nofollow noreferrer\">C++ API documentation</a>:</p>\n\n<blockquote>\n  <p><strong>crossCheck</strong> \u2013 If it is false, this will be default BFMatcher behavior when it finds the k nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the matcher\u2019s collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent pairs. Such technique usually produces best results with minimal number of outliers when there are enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.</p>\n</blockquote>\n\n<hr>\n\n<p><strong>Edit:</strong></p>\n\n<p>From what I understood I'm quite sure you can sum up the above by saying that, if you found the closest match <code>B</code> for feature <code>A</code>, the tuple <code>(A,B)</code> is only considered a <code>Consistent Pair</code> and therefore returned if <code>A</code> is also the closest match for your feature <code>B</code>.</p>\n\n<p><strong>A 1D example:</strong></p>\n\n<p>-----A------B---C</p>\n\n<p>In this case even though <code>B</code> is the best match for <code>A</code> it is not vice versa. Instead, <code>(B,C)</code> will be your <code>Consistent Pair</code></p>\n\n<p>-----A--B-------C</p>\n\n<p>Here, on the other hand, obviously <code>(A,B)</code> is the better match and it will be returned.</p>\n\n<p>(The other approach by Loewe mentioned in the documentation has also already been discussed on SO <a href=\"https://stackoverflow.com/questions/17967950/improve-matching-of-feature-points-with-opencv/17977207#17977207\">over here</a>)</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV HOG Algorithm in Real Time on Raspberry Pi",
        "input": "",
        "output": "<p>Unfortunately not, even overclocked to 1000MHz and with 64MB for video it's not enough.\nOn my old mac with a 2.1 GHz Dual Core Intel CPU and 2GB of ram I could barely get between 8-12 FPS for a 640x480 stream. </p>\n\n<p>I haven't tried OpenCV 3.0 (just 2.4.8) on Raspberry PI so don't have any softcascades test results to share, but it sounds promising.</p>\n\n<p>Another idea I can think of is using LBP cascades. You could start with a HAAR since there's one already for detecting bodies so it would be easy to test, but LBP should be a bit faster. Perhaps you could train a cascade that works really well for a set environment.</p>\n\n<p>Also, if it helps, you can use my little <a href=\"https://github.com/orgicus/PiCapture\" rel=\"nofollow\">OpenCV wrapper for the PiCamera</a> for tests. It basically returns frames from the Pi Camera module as <code>cv::Mat</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: Error copying one image to another",
        "input": "",
        "output": "<pre><code>// will read in a rgb image , no matter what the content is\nImg = imread(\"../../../stereo_images/left01.jpg\"); \n</code></pre>\n\n<p>to make it read grayscale images use:</p>\n\n<pre><code>Img = imread(\"../../../stereo_images/left01.jpg\", CV_LOAD_IMAGE_GRAYSCALE); \n</code></pre>\n\n<p>then, you don't need to copy per pixel (and you should even avoid that), just use:</p>\n\n<pre><code>Mat im2 = Img.clone();\n</code></pre>\n\n<p><em>if</em> you do per-pixel loops, watch out to get the indices right. it's row-col world here, not x,y, so it should be:</p>\n\n<pre><code> copyImg.at&lt;uchar&gt;(i,j) = Img.at&lt;uchar&gt;(i,j);\n</code></pre>\n\n<p>in your case</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting array of circles using Hough Circle Transform",
        "input": "",
        "output": "<p>Since they are circles, and all dark, why not filter them out with a morphological disk and subtract the original image from the filtered one to get good responses?  Morphology isn't particularly fast, but faster than Hough.  What you'd do is dilate the background (with a disk shaped) until the black is gone, then subtract the original image from that.  Then threshold.  You can then do size filtering to eliminate any tiny scraps that could get through.</p>\n\n<p>Given this application, I don't think Hough is the strongest choice, unless this is a school project. </p>\n",
        "system": ""
    }
]