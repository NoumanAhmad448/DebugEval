[
    {
        "instruction": "Assign 3x1 mat to 3 channels mat",
        "input": "",
        "output": "<p>OpenCV gives you a reshape function to change the number of channels/rows/columns implicitly:</p>\n\n<p><a href=\"http://docs.opencv.org/modules/core/doc/basic_structures.html#mat-reshape\" rel=\"nofollow\">http://docs.opencv.org/modules/core/doc/basic_structures.html#mat-reshape</a></p>\n\n<p>This is very efficient since no data is copied, only the matrix header is changed.</p>\n\n<p>try:</p>\n\n<pre><code>cv::Mat mat3Channels = mat.reshape(3,1);\n</code></pre>\n\n<p>Didn't test it, but should work. It should give you a 1x1 matrix with 3 channel element (Vec3d) if you want a <code>Vec3b</code> element instead, you have to convert it:</p>\n\n<pre><code>cv::Mat mat3ChannelsVec3b;\nmat3Channels.convertTo(mat3ChannelsVec3b, CV_8UC3);\n</code></pre>\n\n<p>If you just want to write your <code>mat</code> back, it might be better to create a single <code>Vec3b</code> element instead:</p>\n\n<pre><code>cv::Vec3b element3Channels;\nelement3Channels[0] = multiply.at&lt;double&gt;(0,0);\nelement3Channels[1] = multiply.at&lt;double&gt;(1,0);\nelement3Channels[2] = multiply.at&lt;double&gt;(2,0);\n</code></pre>\n\n<p>But care in all cases, that Vec3b elements can't save values <code>&lt; 0</code> and <code>&gt; 255</code></p>\n\n<p>Edit: After reading your question again, you ask how to assign...</p>\n\n<p>I guess you have another matrix:</p>\n\n<pre><code>cv::Mat outputMatrix = cv::Mat(im.rows, im.cols, CV_8UC3, cv::Scalar(0,0,0));\n</code></pre>\n\n<p>Now to assign <code>multiply</code> to the element in <code>outputMatrix</code> you ca do:</p>\n\n<pre><code>cv::Vec3b element3Channels;\nelement3Channels[0] = multiply.at&lt;double&gt;(0,0);\nelement3Channels[1] = multiply.at&lt;double&gt;(1,0);\nelement3Channels[2] = multiply.at&lt;double&gt;(2,0);\noutputMatrix.at&lt;Vec3b&gt;(i, j) = element3Channels;\n</code></pre>\n\n<p>If you need alpha channel too, you can adapt that easily.</p>\n",
        "system": ""
    },
    {
        "instruction": "Foreground detection without an image sequence",
        "input": "",
        "output": "<p>What you are looking for is a figure-ground segmentation algorithm. Does it have to be fully automatic? If you can draw an initial contour of the object by hand, you can use a class of algorithms called \"active contours\". If you need this to be fully automatic, you can use an algorithm called N-cuts.</p>\n\n<p>If you are using MATLAB, and you are ok with semi-automatic segmentation, try the <a href=\"http://www.mathworks.com/help/images/image-segmentation-using-the-image-segmenter-app.html\" rel=\"nofollow\">Image Segmenter App</a> in the Image Processing Toolbox.</p>\n",
        "system": ""
    },
    {
        "instruction": "Android detect red color in a video if it exists",
        "input": "",
        "output": "<p>Basically, a picture contains 3 channels colors from a video stream. This 3 channels compose each 3 digitals primary colors RGB (red green blue). Generally, 3 matrix on android contains each channel values.\nWhen you know that, you understand that the color appeared in red when the 3 composites are as (255, 0, 0) in case of image depth is 8bit. In other word, the red value is greater than 2 others. </p>\n\n<p>You must test and determine a predicate.</p>\n\n<p>To help you, I tell you to look OpenCV framework for Android... ^^</p>\n",
        "system": ""
    },
    {
        "instruction": "video read in matlab -audio output port is not relavant",
        "input": "",
        "output": "<pre><code>clear all;\nclose all;\nclc;\nVidObj=VideoReader('E:\\workspace\\mat2012b\\video compression\\original.mp4');\nn=VidObj.NumberOfFrames;\nvideoFReader = vision.VideoFileReader('original.mp4');\nvideoFWriter = vision.VideoFileWriter('vid_new_compressed_ffd5.avi',...\n   'AudioInputPort',1,'AudioDataType','int16','VideoCompressor','ffdshow video encoder','FileFormat','avi',...\n   'FrameRate',videoFReader.info.VideoFrameRate);\n[audio,fs]=audioread('original.mp4');\nop=floor(fs/videoFReader.info.VideoFrameRate);\n\nfor i=1:n\n    videoFrame= step(videoFReader);\n    audios=audio( (i-1)*op + 1 : i*op , : );\n    step(videoFWriter, videoFrame,audios);\nend\n\nrelease(videoFReader);\nrelease(videoFWriter);\n</code></pre>\n\n<p>This above code reads an audio and video and writes back to audio and video. I tried to get .mp4 as output it didnt work. This code can give .avi and .wmv as output.</p>\n",
        "system": ""
    },
    {
        "instruction": "Direct Table &amp; Lookup Table",
        "input": "",
        "output": "<p>For a regular 24-bit RGB representation most probably you just have to multiply the number of pixel on number of bytes per pixel. 24 bits = 3 bytes, so the size is 800 * 600 * 3 bytes = 1440000 bytes \u2248 1.37 MiB. In some cases you may have rows of an image aligned on some boundary in memory, usually 4 or 8 or 32 bytes.  But since 800 is divisible by 32, this will not change anything, still 1.37 MiB.</p>\n\n<p>Now, for a look-up table, you have 1 byte per pixel, since you have only to address one entry in the table. This yields 800 * 600 * 1 = 480000 bytes \u2248 0.46 MiB. Plus the table itself: 256 colors, 24 bits (3 bytes) each - 256 * 3 = 768 bytes. Negligible comparing to the size of the image.</p>\n",
        "system": ""
    },
    {
        "instruction": "undefined function or variable error in MATLAB",
        "input": "",
        "output": "<p>visionSupportPackages is not available in R2012, see here <a href=\"http://nl.mathworks.com/help/vision/release-notes.html\" rel=\"nofollow\">http://nl.mathworks.com/help/vision/release-notes.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "What does this line (cap&gt;&gt; frame;) mean in the source code?",
        "input": "",
        "output": "<p>This line of code take new frame from camera</p>\n\n<pre><code>cap&gt;&gt; frame;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Overlapping Sliding windows over image",
        "input": "",
        "output": "<p>Alright so I figured out what the issue was.</p>\n\n<p>This code didn't cause overlaps:</p>\n\n<pre><code>grid_h_max =(imgheight/winh)\ngrid_w_max= (imgwidth / winw)\nwin = sliding_window(img, window_size, shiftSize=None, flatten=False) \n</code></pre>\n\n<p>Dividing the entire image dimensions with those of the window dimensions are obviously going to give non-overlapping results.</p>\n\n<p>To get the right number of windows per dimension, I simply allow the sliding_window function to tell me itself what the number of windows is, vertically and horizontally:</p>\n\n<pre><code>win = sliding_window(img, window_size, shiftSize=None, flatten=False) \ngrid_h_max = win.shape[0]\ngrid_w_max = win.shape[1]\n</code></pre>\n\n<p>This gave me approximately 5000 windows again.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is the difference between imregionalmax() of matlab and scipy.ndimage.filters.maximum_filter",
        "input": "",
        "output": "<p>It appears as if scipy's <code>maximum_filter</code> returns the actual local max values, while Matlab's <code>imregionalmax</code> returns a mask with the <em>locations</em> of the local maxima.<br>\nI would expect</p>\n\n<pre><code> lm = scipy.ndimage.filters.maximum_filter( img, ... )\n msk = (img == lm) #// convert local max values to binary mask\n</code></pre>\n\n<p>should give you similar results to Matlab's.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is a feature descriptor in image processing (algorithm or description)?",
        "input": "",
        "output": "<p>A feature <em>detector</em> is an algorithm which takes an image and outputs <em>locations</em> (i.e. pixel coordinates) of significant areas in your image. An example of this is a <a href=\"http://en.wikipedia.org/wiki/Corner_detection\" rel=\"noreferrer\">corner detector</a>, which outputs the locations of corners in your image but does not tell you any other information about the features detected. </p>\n\n<p>A feature <em>descriptor</em> is an algorithm which takes an image and outputs <em>feature descriptors</em>/<em>feature vectors</em>. Feature descriptors encode interesting information into a series of numbers and act as a sort of numerical \"fingerprint\" that can be used to differentiate one feature from another. Ideally this information would be invariant under image transformation, so we can find the feature again even if the image is transformed in some way. An example would be <a href=\"http://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" rel=\"noreferrer\">SIFT</a>, which encodes information about the local neighbourhood image gradients the numbers of the feature vector. Other examples you can read about are <a href=\"http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\" rel=\"noreferrer\">HOG</a> and <a href=\"https://en.wikipedia.org/wiki/Speeded_up_robust_features\" rel=\"noreferrer\">SURF</a>.</p>\n\n<hr>\n\n<p>EDIT: When it comes to feature <em>detectors</em>, the \"location\" might also include a number describing the size or scale of the feature. This is because things that look like corners when \"zoomed in\" may not look like corners when \"zoomed out\", and so specifying scale information is important. So instead of just using an <code>(x,y)</code> pair as a location in \"image space\", you might have a triple <code>(x,y,scale)</code> as location in \"scale space\".</p>\n",
        "system": ""
    },
    {
        "instruction": "Meaning of &quot;white rectangles are subtracted from the sum of pixels in the grey rectangles&quot; in Viola-Jones",
        "input": "",
        "output": "<p>I believe you are talking about the <a href=\"http://en.wikipedia.org/wiki/Haar-like_features\" rel=\"nofollow\">Haar-like features</a> used in the original variant of the Viola-Jones algorithm. The Haar-like features are very simple \"box filters\". They approximate first and second order Gaussian derivative filters. </p>\n\n<p>Let's say you have a 1x4 Haar filter that looks like this: [-1 -1 1 1]. Applying this filter is equivalent to subtracting the sum of the two pixels on the left from the sum of the two pixels on the right. If you do this at every location in your image, you will get an approximation of the first-order x-derivative of the image. </p>\n\n<p>Applying filters of this form, i. e. consisting of rectangular regions filled with 1 or -1 can be done very efficiently using a technique called <em>integral image</em>. This is why Haar features were used for the Viola-Jones algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "Split rows and columns in matlab",
        "input": "",
        "output": "<p>Taking advantage of the very regular structure of the input image you can do the following:</p>\n\n<pre><code>img = imread('https://i.sstatic.net/SuKT2.jpg'); %//read image\nbw = sum(img,3) &lt; 10;  %//convert to binary mask\n</code></pre>\n\n<p>Taking the sum of pixels along the rows reveals the lines of text (positive sum separated by blanks)</p>\n\n<pre><code>lines = bwlabel( sum( bw,2) &gt; 1 ); %// label each line\nlbw = bsxfun( @times, single(bw), lines ); %// label each line in the mask\n</code></pre>\n\n<p>Now we can ignore the top 8 lines (header) and the last two  footer lines</p>\n\n<pre><code>sbw = lbw &gt; 8 &amp; lbw &lt; max(lines)- 2; %// select only the relevant lines\n</code></pre>\n\n<p>We can separate the text into columns, assuming a large enough gap of at least 15 pixels</p>\n\n<pre><code>col = bwlabel( imfilter( single(sum(sbw,1) &lt; 1), ones(1,15)/15, 'symmetric', 'same' ) &lt; .9 );\n</code></pre>\n\n<p>Compute the bounding box of the selected lines of the two columns</p>\n\n<pre><code>st = regionprops( bsxfun(@times, sbw, col ), 'BoundingBox' );\n</code></pre>\n\n<p>Visualize the resulting bounding boxes</p>\n\n<pre><code>figure;imshow( img, 'border','tight' );hold on; \nrectangle('Position', st(1).BoundingBox, 'EdgeColor','r','LineWidth',2);\nrectangle('Position', st(2).BoundingBox, 'EdgeColor','r','LineWidth',2);    \n</code></pre>\n\n<p>Resulting with</p>\n\n<p><img src=\"https://i.sstatic.net/kYKRr.png\" alt=\"enter image description here\"></p>\n\n<p>Given bounding boxes it is fairly sttraight forward to crop</p>\n\n<pre><code>col1 = imcrop( img, st(1).BoundingBox ); %// crop the description of the products\ncol2 = imcrop( img, st(2).BoundingBox ); %// crop the prices\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Harris corner response in fixed-point integers?",
        "input": "",
        "output": "<p>Yes, the response (R) needs to be a floating point variable too. You can get the integer response using a rounding function if the fractional part of the response is not important for your calculations or you may convert the code to use <a href=\"http://en.wikipedia.org/wiki/Fixed-point_arithmetic\" rel=\"nofollow\">fixed point arithmetic</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Sliding window - how to get window location on image?",
        "input": "",
        "output": "<p>It might be easier for you to understand what's going on if you try using\n<code>flatten=False</code> to create a 'grid' of windows onto the image:</p>\n\n<pre><code>import numpy as np\nfrom scipy.misc import lena\nfrom matplotlib import pyplot as plt\n\nimg = lena()\nprint(img.shape)\n# (512, 512)\n\n# make a 64x64 pixel sliding window on img. \nwin = sliding_window(img, (64, 64), shiftSize=None, flatten=False)\n\nprint(win.shape)\n# (8, 8, 64, 64)\n# i.e. (img_height / win_height, img_width / win_width, win_height, win_width)\n\nplt.imshow(win[4, 4, ...])\nplt.draw()\n# grid position [4, 4] contains Lena's eye and nose\n</code></pre>\n\n<p>To get the corresponding pixel coordinates, you could do something like this:</p>\n\n<pre><code>def get_win_pixel_coords(grid_pos, win_shape, shift_size=None):\n    if shift_size is None:\n        shift_size = win_shape\n    gr, gc = grid_pos\n    sr, sc = shift_size\n    wr, wc = win_shape\n    top, bottom = gr * sr, (gr * sr) + wr\n    left, right = gc * sc, (gc * sc) + wc\n\n    return top, bottom, left, right\n\n# check for grid position [3, 4]\nt, b, l, r = get_win_pixel_coords((3, 4), (64, 64))\n\nprint(np.all(img[t:b, l:r] == win[3, 4, :, :]))\n# True\n</code></pre>\n\n<p>With <code>flatten=True</code>, the 8x8 grid of 64x64-pixel windows will just get flattened out into 64-long vector of 64x64-pixel windows. In that case you\ncould use something like <code>np.unravel_index</code> to convert from the 1D vector index\ninto a tuple of grid indices, then use these to get the pixel coordinates as\nabove:</p>\n\n<pre><code>win = sliding_window(img, (64, 64), flatten=True)\n\ngrid_pos = np.unravel_index(12, (8, 8))\nt, b, l, r = get_win_pixel_coords(grid_pos, (64, 64))\n\nprint(np.all(img[t:b, l:r] == win[12]))\n# True\n</code></pre>\n\n<hr>\n\n<p>OK, I'll try and address some of the questions you raised in the comments.</p>\n\n<blockquote>\n  <p>I want the pixel location of the window relative to the actual pixel dimensions original image.</p>\n</blockquote>\n\n<p>Perhaps I was not clear enough - you can already do this using something like my <code>get_win_pixel_coords()</code> function, which gives you the top, bottom, left and right coordinates of the window relative to the image. For example:</p>\n\n<pre><code>win = sliding_window(img, (64, 64), shiftSize=None, flatten=False)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.hold(True)\nax1.imshow(win[4, 4])\nax1.plot(8, 9, 'oy')         # position of Lena's eye, relative to this window\n\nt, b, l, r = get_win_pixel_coords((4, 4), (64, 64))\n\nax2.hold(True)\nax2.imshow(img)\nax2.plot(t + 8, l + 9, 'oy') # position of Lena's eye, relative to whole image\n\nplt.show()\n</code></pre>\n\n<p>Also notice that I've updated <code>get_win_pixel_coords()</code> to deal with cases where <code>shiftSize</code> is not <code>None</code> (i.e the windows don't perfectly tile the image with no overlap).</p>\n\n<blockquote>\n  <p>So I'm guessing that in that case, I should just make the grid be equal to the original image's dimensions, is that right? (instead of using 8x8).</p>\n</blockquote>\n\n<p>No, if the windows tile the image without overlap (i.e. <code>shiftSize=None</code>, which I've assumed so far), then if you made the grid dimensions equal to the pixel dimensions of the image, every window would just contain a single pixel! </p>\n\n<blockquote>\n  <p>So in my case, for an image of width: 360 and height: 240, would that mean I use this line: <code>grid_pos = np.unravel_index(*12*, (240, 360))</code>. Also, what does 12 refer to in this line?</p>\n</blockquote>\n\n<p>As I said, making the 'grid size' equal to the image dimensions would be pointless, since every window would contain only a single pixel (at least, assuming that the windows are non-overlapping). The 12 would refer to the index into the flattened grid of windows, e.g.:</p>\n\n<pre><code>x = np.arange(25).reshape(5, 5)    # 5x5 grid containing numbers from 0 ... 24\nx_flat = x.ravel()                 # flatten it into a 25-long vector\nprint(x_flat[12])                  # the 12th element in the flattened vector\n# 12\nrow, col = np.unravel_index(12, (5, 5))  # corresponding row/col index in x\nprint(x[row, col])\n# 12\n</code></pre>\n\n<blockquote>\n  <p>I am shifting 10 pixels with each window, and the first sliding window starts from coordinates 0x0 on the image, and the second starts from 10x10, etc, then I want it the program to return not just the window contents but the coordinates corresponding to each window, i.e. 0,0, and then 10,10, etc</p>\n</blockquote>\n\n<p>As I said, you can already get the position of the window relative to the image using the top, bottom, left, right coordinates returned by <code>get_win_pixel_coords()</code>. You could wrap this up into a single function if you really wanted:</p>\n\n<pre><code>def get_pixels_and_coords(win_grid, grid_pos):\n    pix = win_grid[grid_pos]\n    tblr = get_win_pixel_coords(grid_pos, pix.shape)\n    return pix, tblr\n\n# e.g.:\npix, tblr = get_pixels_and_coords(win, (3, 4))\n</code></pre>\n\n<p>If you want the coordinates of <em>every</em> pixel in the window, relative to the image, another trick you could use is to construct arrays containing the row and column indices of every pixel in the image, then apply your sliding window to these:</p>\n\n<pre><code>ridx, cidx = np.indices(img.shape)\nr_win = sliding_window(ridx, (64, 64), shiftSize=None, flatten=False)\nc_win = sliding_window(cidx, (64, 64), shiftSize=None, flatten=False)\n\npix = win[3, 4]    # pixel values\nr = r_win[3, 4]    # row index of every pixel in the window\nc = c_win[3, 4]    # column index of every pixel in the window\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Camera Parameters of Rotated Frame",
        "input": "",
        "output": "<p>The linear parts of your camera parameters will translate into a view-projection matrix. Concatenating it with a scale(-1,-1,1) transformation will give you the view-projection for the new situation (BTW, scale(-1,-1,1) == rotate(180\u00b0=pi, 0,0,1)).</p>\n",
        "system": ""
    },
    {
        "instruction": "Using cvGet2D function - opencv",
        "input": "",
        "output": "<p>You also forgot to say exactly what's the problem.</p>\n\n<p>I'll assume you didn't test the result of <code>cvLoadImage()</code> and the application crashed at the first <code>for</code> loop:</p>\n\n<pre><code>IplImage *img = cvLoadImage(\"car1.jpg\",0);\nif (!img) \n{\n    printf(\"!!! Failed to load image\\n\");\n    return;\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV SURF class Error",
        "input": "",
        "output": "<p>The classes <code>cv::SurfFeatureDetector</code> and <code>cv::SurfDescriptorExtractor</code> are aliases of the class <code>cv::SURF</code>. The difference in results is due to the following reason:</p>\n\n<p>In the function <code>test1</code>, <code>detector</code> and <code>extractor</code> objects are being initialized with different parameters. <code>detector</code> is using <code>minHessian</code> value of <strong>400</strong> while extractor is using the opencv implementation defined default value.</p>\n\n<p>In function <code>test2</code>, keypoint detection and descriptor computation are done using a single <code>cv::SURF</code> object with <code>hessianThreshold</code> value of <strong>400</strong>.</p>\n\n<p>To reproduce the results of <code>test2</code> in <code>test1</code>, initialize both objects with same parameters like this:</p>\n\n<pre><code>int minhessin = 400;\ncv::SurfFeatureDetector detector(minhessin);\ncv::SurfDescriptorExtractor extractor(minHessian);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Set number of keypoints in Android OpenCV",
        "input": "",
        "output": "<p>Whilst I haven't been able to find a way of setting the number of keypoints to search for, I have been able to work out how to extract the correct number of keypoints afterwards.  Doing this isn't computationally efficient but from the comments I received I don't think doing it before is possible.  </p>\n\n<p>My keypoints variable is:</p>\n\n<pre><code>private final MatOfKeyPoint mTargetKeypoints = new MatOfKeyPoint();\n</code></pre>\n\n<p>After it has been \"filled\" with features (it seems to stop after 500) the individual features can be extracted by transforming it to an array (where each element of the array is a feature.  </p>\n\n<pre><code>mTargetKeypoints.toArray()[0]; //NOTE: Not actual code, used in a print statement\n</code></pre>\n\n<p>When I print the above the result is:</p>\n\n<p>KeyPoint [pt={82.0, 232.0}, size=31.0, angle=267.77094, response=0.0041551706, octave=0, class_id=-1]</p>\n\n<p>The individual information can then be extracted with the inbuilt Keypoint functions, e.g.:</p>\n\n<pre><code>mTargetKeypoints.toArray()[0].pt.x //Printing outputs x location of feature.\nmTargetKeypoints.toArray()[0].response // Printing outputs response of feature. \n</code></pre>\n\n<p><a href=\"https://stackoverflow.com/questions/10328298/what-does-size-and-response-exactly-represent-in-a-surf-keypoint\">This</a> SO question indicates that the <em>response</em> indicates \"how good\" the keypoint is.  Thus from here it is relatively simple to pick the best 30 features to use.</p>\n",
        "system": ""
    },
    {
        "instruction": "opencv: Best way to detect corners on chessboard",
        "input": "",
        "output": "<p>Please read these two links:</p>\n\n<p><a href=\"http://www.aishack.in/tutorials/sudoku-grabber-opencv-plot/\" rel=\"nofollow noreferrer\">http://www.aishack.in/tutorials/sudoku-grabber-opencv-plot/</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/10196198/how-to-remove-convexity-defects-in-a-sudoku-square\">How to remove convexity defects in a Sudoku square?</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Locate object based on shape",
        "input": "",
        "output": "<p>Try <a href=\"https://en.wikipedia.org/wiki/Phase_correlation\" rel=\"nofollow noreferrer\">Phase Correlation</a> of the image with the template.\nBelow is the algorithm:</p>\n\n<ol>\n<li>Convert both template and the image to the gray scale </li>\n<li>Compute the 2DFT of the template after removing the mean and zero-padding it to image size</li>\n<li>Compute the 2DFT of the image after removing the mean</li>\n<li>Multiply the 2DFT of the image with the conjugate of the 2DFT of the template</li>\n<li>Divide out the magnitudes of the 2DFT of both the image and the template (leaving behind the phase)</li>\n<li>Take the Inverse 2DFT and find the peaks in the real part </li>\n</ol>\n\n<p>Code below implements the algorithm:</p>\n\n<pre><code>%% Load video frames and template\nimageFile = 'image.png';\ntemplateFile = 'template.png';\nimageColor = imread(imageFile);\nimage = im2double( rgb2gray( imageColor ) );\ntemplate = im2double( rgb2gray( imread(templateFile) ));\n\n[hI, wI] = size(image);\n[hT, wT] = size(template);\n\n%% Perform frequency domain correlation\ntemplateDFT = fft2((template - mean2(template))/std2(template), hI, wI);\ntemplateDFTMag = abs(templateDFT);\n\ntemplateFilter = conj(templateDFT) ./ templateDFTMag;\n\nimageDFT = fft2((image - mean2(image))/std2(image) );\nimageDFTMag = abs(imageDFT);\ncorrDFT = imageDFT .* templateFilter ./ imageDFTMag;\ncorr = real(ifft2(corrDFT));\n\n%% Detect peak and plot\n[maxCorr, maxIdx] = max(corr(:));\n[maxRow, maxCol] = ind2sub([hI wI], maxIdx);\n\nfigure; imshow(imageColor); hold on; \nrectangle('Position', [maxCol-wT/2, maxRow-hT/2, wT, hT], 'EdgeColor', 'y');\n</code></pre>\n\n<p>Below is what I get:</p>\n\n<p><img src=\"https://i.sstatic.net/o1e0c.png\" alt=\"enter image description here\"></p>\n\n<p>Caveats:</p>\n\n<ul>\n<li>This scheme is not very robust to large scale variation or rotation. However, small scale variation will be tolerated. </li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Texture for Superpixels?",
        "input": "",
        "output": "<p>There are at least two methods I can think of.\nEither compute the filter response on the entire undivided image and group the responses in each superpixel, or compute color histogram of the superpixel as an approximation for a texture feature.</p>\n",
        "system": ""
    },
    {
        "instruction": "Extracting image from video at a given time using OpenCV",
        "input": "",
        "output": "<p>why don't you just do, what @micka proposed ?</p>\n\n<pre><code>import cv2\n\nvidcap = cv2.VideoCapture('d:/video/keep/Le Sang Des Betes.mp4')\nvidcap.set(cv2.CAP_PROP_POS_MSEC,20000)      # just cue to 20 sec. position\nsuccess,image = vidcap.read()\nif success:\n    cv2.imwrite(\"frame20sec.jpg\", image)     # save frame as JPEG file\n    cv2.imshow(\"20sec\",image)\n    cv2.waitKey()                    \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Draw a rectangular bounding box around a person in an image",
        "input": "",
        "output": "<p>Your problem actually isn't drawing the bounding box - it's locating the person inside the image, which you haven't quite done properly.  If you don't do this correctly, then you won't be able place the correct bounding box around the person.  This is what I have done to locate the person in the image, then drawing a bounding box around this person.  This is assuming that your image is stored in <code>im</code>:</p>\n\n<ol>\n<li>Notice that the intensity distribution of the person is darker than most of the scene.  As such, I'm going to threshold the image by choosing any pixels less than intensity 65 to be white while the other pixels black.</li>\n<li>I clear any white pixels that surround the image border</li>\n<li>I perform a <code>regionprops</code> call extracting the <code>BoundingBox</code> and <code>Area</code> properties.</li>\n<li>I search through all of the areas and find the <code>BoundingBox</code> with the largest <code>Area</code>.</li>\n<li>I use this <code>BoundingBox</code> and draw it on our image.</li>\n</ol>\n\n<p>Therefore:</p>\n\n<pre><code>%// Step #1\nim_thresh = im &lt; 65;\n\n%// Step #2\nim_thresh2 = imclearborder(im_thresh);\n\n%// Step #3\nrp = regionprops(im_thresh2, 'BoundingBox', 'Area');\n\n%// Step #4\narea = [rp.Area].';\n[~,ind] = max(area);\nbb = rp(ind).BoundingBox;\n\n%// Step #5\nimshow(im);\nrectangle('Position', bb, 'EdgeColor', 'red');\n</code></pre>\n\n<hr>\n\n<p>This is what we get:</p>\n\n<p><img src=\"https://i.sstatic.net/Eo84n.png\" alt=\"enter image description here\"></p>\n\n<p>Bear in mind that this isn't perfect.  You may have to play around with the threshold to get a more accurate bounding box, but this should be enough for you to start with.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV - Partial Image Matching",
        "input": "",
        "output": "<p>I suggest to use Matching features (keypoints)  approaches, they are scale-invariant features (SIFT) and Speeded Up Robust Features (SURF). A lot of examples are available on Opencv docs, you can check <a href=\"http://docs.opencv.org/doc/tutorials/features2d/feature_homography/feature_homography.html#feature-homography\" rel=\"nofollow\">this one</a> </p>\n\n<p>Another alternative is proposed in <a href=\"http://www.willowgarage.com/sites/default/files/orb_final.pdf\" rel=\"nofollow\">this paper</a>: </p>\n\n<p>If you have a predefined reference images, it's efficient to use detection approaches using cascade classifiers, well implemented with opencv.</p>\n\n<p>You can integrate Cascade Classifier of opencv (<a href=\"http://docs.opencv.org/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html\" rel=\"nofollow\">here is tutorial</a>), (it is also useful to detect a part of human body)</p>\n\n<p>Opencv provides few xml files already trained to detect faces, eyes and even body. You may also train for your aimed object to get your own xml files (<a href=\"http://docs.opencv.org/doc/user_guide/ug_traincascade.html?highlight=train%20cascade%20classifier\" rel=\"nofollow\">here is a tuto</a> how to train yours from the scratch)</p>\n",
        "system": ""
    },
    {
        "instruction": "How to group nearest blobs using EmguCV",
        "input": "",
        "output": "<p>If you want to implement it by yourself use Find-Union or DFS algorithm.\nIn Find-Union first all 'stars' are in separated boxes and then connect neighbours. (using appropiate connectivity algorithm: <a href=\"http://en.wikipedia.org/wiki/Pixel_connectivity\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Pixel_connectivity</a> - probably 8 connectivity will give you best results. )</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting markers in image to use for perspective transform in OpenCV",
        "input": "",
        "output": "<p>Cute idea! As others have noted, two points are not enough, but your markers define four lines, and those are enough. </p>\n\n<p>If you can reliably detect edges on the line segments forming your markers, you can fit lines to them and then either directly compute the line homography (and then inverse-transpose it to convert it to a point homography), or intersect the lines in pairs to get four independent corners, and then compute the point homography on them.</p>\n\n<p>Some experimentation may be needed to reliably detect your markers. You may want to consider using template matching.</p>\n",
        "system": ""
    },
    {
        "instruction": "Can openCV stereo matching algorithms (stereoBM and stereoSGBM) work with verticaly rectified images",
        "input": "",
        "output": "<p>Last time I checked the code neither StereoBM nor StereoSGBM supported vertical stereo pairs (the implementation assumes a horizontal stereo pair). You can however rotate by 90 degrees your images so that they form a horizontal stereo pair. Then compute a disparity map with either method and rotate it by 90 degrees in the opposite direction so that you get a disparity map for your input vertical stereo pair.</p>\n",
        "system": ""
    },
    {
        "instruction": "Blocking artefacts with DCT decompression Opencv",
        "input": "",
        "output": "<p>The DCT coefficients of an image are signed doubles with negative values. After quantising and rounding the elements I converted the matrix from a CV_64FC3 type to CV_8UC3 expecting the equivalent of a static_cast to be applied to the elements.</p>\n\n<p>However OpenCV when converting from 64F to 8UC zeroed out all negative values resulting in information loss that caused the blockiness.</p>\n\n<p>The solution was apply a static cast converting the double to an unsigned char and writing it into the returned array instead of utilising the opencv convert function.</p>\n",
        "system": ""
    },
    {
        "instruction": "Cheat Sheet or Flowchart for Computer Vision",
        "input": "",
        "output": "<p>That's a monumental task. The best I could find is from <a href=\"http://www.embedded-vision.com/technology/computer-vision-algorithms\" rel=\"nofollow noreferrer\">this article</a> and it's a little bit old:</p>\n\n<p><img src=\"https://i.sstatic.net/7vyls.png\" alt=\"enter image description here\"></p>\n\n<p>Maybe it's a good time to commit to <a href=\"https://www.kickstarter.com/projects/726845063/flexcv-flexible-computer-vision-video-software\" rel=\"nofollow noreferrer\">FlexCV</a> on Kickstarter.com, a GUI for OpenCV that allows you to create complex algorithms in a matter of minutes by connecting graphical elements together. It's an alternative for <a href=\"https://adaptive-vision.com/en/\" rel=\"nofollow noreferrer\">Adaptive Vision</a>, but purely based on OpenCV features.</p>\n",
        "system": ""
    },
    {
        "instruction": "opencv and python error &quot; name &#39;kernel&#39; is not defined&quot;",
        "input": "",
        "output": "<p>Error was due to numpy installation. I searched alot and got some clue related to numpy. I re-installed numpy and everything is working fine now.</p>\n",
        "system": ""
    },
    {
        "instruction": "Search for segment similarity inside strokes",
        "input": "",
        "output": "<p>I believe <a href=\"http://www.cs.berkeley.edu/~malik/papers/BMP-shape.pdf\" rel=\"nofollow\">Shape Context</a> can be helpful for your task. You can read more about it <a href=\"http://en.wikipedia.org/wiki/Shape_context\" rel=\"nofollow\">here</a>.<br>\nShape context descriptor allows you to robustly describe a <strong>local</strong> shape. Finding two shape-context descriptors that are nearly identical strongly suggests that the underlying text pattern are quite similar.</p>\n\n<p>You can find Matlab implementation in the <a href=\"https://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sc_digits.html\" rel=\"nofollow\">project's home page</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "HOG Detection : Webcam unable to grab new frame",
        "input": "",
        "output": "<p>I have found this to happen even when you pause the execution of the code for a little while.\nIn my case  MJPEG OVERREAD would appear in the console, after which no more images would get grabbed from the stream, but the stream would still be `open\u00b4 </p>\n\n<p>I build this little check around this problem which resets the capture. just place this in your WHILE</p>\n\n<pre><code>    if (cap.isOpened())\n    {\n        cap.read(img);\n\n        if (img.rows == 0)\n        {\n            cap.release();\n            cap.open(\"\");//Insert own url\n            cap.read(img);\n        }\n    }\n    else\n    {\n        cap.open(\"\"); //Insert own url\n        cap.read(img);\n    }\n</code></pre>\n\n<p>if this \"solves\" the problem by keeping you app running, you should capture the frames on a separate thread to prevent the stream from dying like this.\nIf this does not \"solve\" the problem, I'm not sure what else could be wrong</p>\n",
        "system": ""
    },
    {
        "instruction": "Re-establishing new feature points using Matlab&#39;s vision functions",
        "input": "",
        "output": "<p>In the if statement you detect new points:</p>\n\n<pre><code>    if FrameCount==30  %If 30 frame have stepped though, find new feature points\n        disp('help')\n        points = detectMinEigenFeatures(rgb2gray(videoFrame),'MinQuality',0.04,'FilterSize',3);\n        points = points.Location;\n        FrameCount=0;\n    end\n</code></pre>\n\n<p>Now, inside that same <code>if</code> you have to tell the point tracker about those new points:</p>\n\n<pre><code>setPoints(tracker, points);\n</code></pre>\n\n<p>Otherwise, your variable <code>points</code> gets overwritten by the next line:</p>\n\n<pre><code>[points, isFound] = step(pointTracker, videoFrame);\n</code></pre>\n\n<p>This is why you never see the newly detected points.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to place foreground images at specified locations over background image in Matlab?",
        "input": "",
        "output": "<p>Here is a simple solution using <a href=\"http://www.mathworks.com/help/matlab/ref/ginput.html\" rel=\"nofollow noreferrer\">ginput</a> in which the user clicks on a figure (here only once) and you fetch the coordinates of the point(s). In this example, there is a message box asking the user to select a point, then the foreground image is drawn over the background image. Notice that in my example I replace the pixels from the background image with those of the foreground. i.e. I do not use alpha and transparency. Hope that's fine for you; if not please tell me.</p>\n\n<p>In the example, my 'background' image is the peppers image that ships with Matlab and the 'foreground' image is the pears image that also ships with Matlab. I use a small portion of the image for the demonstration.</p>\n\n<p>Here is the code:</p>\n\n<pre><code>clear\nclc\nclose all\n%// Set up background image (peppers.png) and foreground image (part of\n%// pears.png)\nBackgroundImage = imread('peppers.png');\n\nDummyForeground = imread('pears.png');\nForegroundImage = DummyForeground(50:200,50:200,:);\n\n%// Get size of foreground image\n[rowFore,colFore,channelFore] = size(ForegroundImage);\n\nfigure\n\nimshow(BackgroundImage);\n\nhMsg = msgbox('Select an anchor point for foreground image','modal');\nuiwait(hMsg)\n</code></pre>\n\n<p>This looks like this:</p>\n\n<p><img src=\"https://i.sstatic.net/cjtVD.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code>%// Use ginput to prompt user to select a single point (i.e. the 1 in\n%// brackets).\n[x,y] = ginput(1);\n</code></pre>\n\n<p>Calling <code>ginput</code> results in the following:</p>\n\n<p><img src=\"https://i.sstatic.net/cCmpj.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code> x = round(x);\n y = round(y);\n\n%// Important!\nhold on\n\n%// Replace pixels of background image with foreground image. \nBackgroundImage(y:y+rowFore-1,x:x+colFore-1,:) = ForegroundImage;\n\nimshow(BackgroundImage);\n</code></pre>\n\n<p>And finally the background image with the foreground image:</p>\n\n<p><img src=\"https://i.sstatic.net/kNk9i.jpg\" alt=\"enter image description here\"></p>\n\n<p>Note: It looks like there is a shift between the cursor and the actual image placed; that happened when I took the screenshot haha it's not a bug :)</p>\n\n<p>Now if you would like to add many images in the foreground, you could easily modify the code using multiple points for ginput, calling it like so:</p>\n\n<pre><code>[x,y] = ginput %// Indefinite # of points)\n\nor [x,y] = ginput(SomeNumber) %// Any number you want\n</code></pre>\n\n<p>and add appropriate images for every point you selected. </p>\n\n<p>Hope that was clear enough and it gets you started!</p>\n",
        "system": ""
    },
    {
        "instruction": "Viewpoint invariant detection and recognition of simple 3d objects from image",
        "input": "",
        "output": "<p>Based on your description of your problem, I see several drawbacks of a Haar or LBP-based detector. First, these features do not use color, which seems to be important here. Second, a classifier using Haar or LBP features is sensitive to in-plane and out-of-plane rotation. If your objects can be in any 3D orientation, you would need to discretize the range of 3D rotations and train a separate detector for each one.  For example, for face detection you typically use two detectors: one  for frontal faces, and one for profile faces. Finally, if there is not enough texture for bag-of-words, there also may not be enough texture for Haar or LBP.</p>\n\n<p>Since your objects are simple 3D shapes, I would start by trying to detect straight lines and circles using the Hough transform, and trying to group them to form the object's outlines.</p>\n",
        "system": ""
    },
    {
        "instruction": "Implementing Naive Bayes Nearest Neighbor (NBNN) in MATLAB",
        "input": "",
        "output": "<p>If your images are 32x32 pixels, then trying to detect interest points is not a good idea.  As you have observed, you would get very few features, if any.  Upsampling the images is one option. Another option is to use a global descriptor like HOG (<code>extractHOGFeatures</code>).</p>\n",
        "system": ""
    },
    {
        "instruction": "What are the advantages of using an autoencoder to build a set of filters versus a prebuilt set of gabor filters in relation to CNNs?",
        "input": "",
        "output": "<p>Using autoencoders may in some cases improve performance, yield biologically plausible filters, and more importantly, give you a model based on your data instead of predefined filters. Autoencoders will give you filters that may fit your data better, in general. The only downside is of course the additional computation time. However, in many cases it doesn't hinder you from learning online.</p>\n\n<p>For more evaluations you might have a look at:\n<a href=\"http://www.idsia.ch/~masci/papers/2011_icann.pdf\" rel=\"noreferrer\">http://www.idsia.ch/~masci/papers/2011_icann.pdf</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Setting iPhone lens position for focus",
        "input": "",
        "output": "<pre><code>/*!\n @method setFocusModeLockedWithLensPosition:completionHandler:\n @abstract\n    Sets focusMode to AVCaptureFocusModeLocked and locks lensPosition at an explicit value.\n\n @param lensPosition\n    The lens position, as described in the documentation for the lensPosition property. A value of AVCaptureLensPositionCurrent can be used\n    to indicate that the caller does not wish to specify a value for lensPosition.\n @param handler\n    A block to be called when lensPosition has been set to the value specified and focusMode is set to AVCaptureFocusModeLocked. If\n    setFocusModeLockedWithLensPosition:completionHandler: is called multiple times, the completion handlers will be called in FIFO order. \n    The block receives a timestamp which matches that of the first buffer to which all settings have been applied. Note that the timestamp \n    is synchronized to the device clock, and thus must be converted to the master clock prior to comparison with the timestamps of buffers \n    delivered via an AVCaptureVideoDataOutput. The client may pass nil for the handler parameter if knowledge of the operation's completion \n    is not required.\n\n @discussion\n    This is the only way of setting lensPosition.\n    This method throws an NSRangeException if lensPosition is set to an unsupported level.\n    This method throws an NSGenericException if called without first obtaining exclusive access to the receiver using lockForConfiguration:.\n*/\n- (void)setFocusModeLockedWithLensPosition:(float)lensPosition completionHandler:(void (^)(CMTime syncTime))handler NS_AVAILABLE_IOS(8_0);\n</code></pre>\n<h1>HERE IT IS</h1>\n<p>A method provided by apple API in iOS8 to let you manually set the focus lens position.</p>\n<h1>Some quick starter code</h1>\n<pre><code>-(IBAction)focusAndLock:(id)sender\n{\n    if (self.isFocusLocked)\n    {\n         NSLog(@&quot;unlock the lens&quot;);\n        [[[self videoDeviceInput] device] setExposureMode:AVCaptureExposureModeAutoExpose];\n        [[[self videoDeviceInput] device] setFocusMode:AVCaptureFocusModeAutoFocus];\n        [self.focusAndLock setTitle:@&quot;set Focus lock&quot; forState:UIControlStateNormal];\n        [self.focusAndLock setTitleColor:[UIColor blueColor] forState:UIControlStateNormal];\n        self.isFocusLocked = NO;\n    }\n    else\n    {\n        NSLog(@&quot;lock the lens&quot;);\n        NSError *error;\n        if ([[[self videoDeviceInput] device] lockForConfiguration:&amp;error])\n        {\n            [[[self videoDeviceInput] device] setFocusModeLockedWithLensPosition:[[self videoDeviceInput] device].lensPosition completionHandler:nil];\n            [[[self videoDeviceInput] device] setExposureMode:AVCaptureExposureModeLocked];\n            [[[self videoDeviceInput] device] setFocusMode:AVCaptureFocusModeLocked];\n            [self.focusAndLock setTitle:[NSString stringWithFormat:@&quot;FOCUS LOCKED: %f&quot;,[[self videoDeviceInput] device].lensPosition] forState:UIControlStateNormal];\n            [self.focusAndLock setTitleColor:[UIColor greenColor] forState:UIControlStateNormal];\n            self.isFocusLocked = YES;\n            NSArray *paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);\n            NSString *documentsDirectory = [paths objectAtIndex:0];\n            NSURL *selectedPath = [[NSURL alloc] initFileURLWithPath:[documentsDirectory stringByAppendingPathComponent:@&quot;focusData.plist&quot;]];\n            NSMutableDictionary *focusDict = [[NSMutableDictionary alloc] init];\n            [focusDict setValue:[NSNumber numberWithFloat:[[self videoDeviceInput] device].lensPosition] forKey:@&quot;focusLock&quot;];\n            [focusDict writeToURL:selectedPath atomically:YES];\n        }\n    }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Homography estimation from consecutive video frame lines gives bad results",
        "input": "",
        "output": "<p>Well, you computed a homography mapping lines to lines. If you want the corresponding pointwise homography you need to invert and transpose it. See, for example, Chapter 1.3.1 of Hartley and Zisserman's \"Multiple View Geometry\".</p>\n",
        "system": ""
    },
    {
        "instruction": "Defining an (initial) set of Haar Like Features",
        "input": "",
        "output": "<p>From your question i am able to understand that you wanted to know what are 1,62,336 features.</p>\n\n<p>From 4 original viola jones features(<a href=\"http://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework</a>) </p>\n\n<p>We can generate 1,62,336 features by varying size of 4 original features and their position on 24*24 input image.</p>\n\n<p>For example consider one of the original feature which has two rectangles adjacent to each other.\nLet us consider size of each rectangle is 1 pixel. Initially if one rectangle is present on (0,0) of 24*24 image then it is considered as one feature &amp; now if you move it horizontally by one pixel( to (1,0) ) then it is considered as second feature as its position is changed to (1,0). In this way u can move it horizontally upto (22,0) generating 23 features. Similarly, if you move along vertical axis from (0,0) up to (0,23) then u can generate 24 features. Now if you move on image covering every position (for example (1,1),(1,2).....(22,23) ) then u can generate 24*23=552 features.</p>\n\n<p>Now if we consider width of each rectangle is 2 pixels and height is 1 pixel. Initially if one rectangle is present on (0,0) and is moved along horizontal axis up to (20,0) as said above then we can have 21 features, as its height is same if we move along vertical axis from (0,0) to (0,23) we can have 24 features. Thus if we move so as to cover every position on image then we can have 24*21=504 features.</p>\n\n<p>In this way if we increase width of each rectangle by one pixel keeping height of each rectangle as 1 pixel every time we cover complete image, so that its width changes from 1 pixel to 24 pixels we get no. of features = 24*(23+21+19.....3+1)</p>\n\n<p>Now, if we consider width of each rectangle is 1 pixel and height as 2 pixel. Initially if one rectangle is present on (0,0) and is moved along horizontal axis up to (23,0) then we can have 23 features as its width is 1 pixel, as its height is 2 pixels if we move along vertical axis from (0,0) to (0,22) then we can have 23 features. Thus if we move so as to cover every position on image then we can have 23*23=529 features.</p>\n\n<p>Similarly, if we increase width of each rectangle by one pixel keeping height of each rectangle as 2 pixels every time we cover complete image, so that its width changes from 1 pixel to 24 pixels we get no. of features = 23*(23+21+19.....3+1)</p>\n\n<p>Now, if we increase height of each rectangle by 1 pixel after changing width of each rectangle from 1 pixel to 24 pixels until height of each rectangle becomes 24 pixels, then </p>\n\n<p>no. of features = 24*(23+21+19.....3+1) + 23*(23+21+19.....3+1) + 22*(23+21+19.....3+1) +.................+ 2*(23+21+19.....3+1) + 1*(23+21+19.....3+1)</p>\n\n<pre><code>            = 43,200 features\n</code></pre>\n\n<p>Now if we consider 2nd viola jones original feature which has two rectangles with one rectangle above other(that is rectangles are arranged vertically), as this is similar to 1st viola jones original feature it will also have </p>\n\n<p>no. of features = 43,200</p>\n\n<p>Similarly if we follow above process, from 3rd original viola jones feature which has 3 rectangles arranged along horizontal direction, we get </p>\n\n<p>no. of features = 24*(22+19+16+....+4+1) + 23*(22+19+16+....+4+1) + 22*(22+19+16+....+4+1) +................+ 2*(22+19+16+....+4+1) + 1*(22+19+16+....+4+1)</p>\n\n<pre><code>            =27,600\n</code></pre>\n\n<p>Now, if we consider another feature which has 3 rectangles arranged vertically(that is one rectangle upon another) then we get </p>\n\n<p>no. of features = 27,600 (as it is similar to 3rd original viola jones feature)</p>\n\n<p>Lastly, if we consider 4th original viola jones feature which has 4 rectangles we get </p>\n\n<p>no.of features = 23*(23+21+19+......3+1) + 21*(23+21+19+......3+1) + 19*(23+21+19+......3+1) ..................+ 3*(23+21+19+......3+1) + 1*(23+21+19+......3+1)</p>\n\n<pre><code>           = 20,736\n</code></pre>\n\n<p>Now summing up all these features we get = 43,200 + 43,200 + 27,600 + 27,600 + 20,736</p>\n\n<pre><code>                                     = 1,62,336 features\n</code></pre>\n\n<p>Thus from above 1,62,336 features Adaboost selects some of them to form strong classifier.</p>\n",
        "system": ""
    },
    {
        "instruction": "Assembling a Haar-like filter for edge detection",
        "input": "",
        "output": "<p>Based on the information given in the paper <a href=\"http://cvl.ice.cycu.edu.tw/meeting/2010.11.16.pdf\" rel=\"nofollow\">Vehicle Detection Method using Haar-like Feature on Real Time System</a> I can't tell how the group has done it exactly. However I can suggest a way on how this could be implemented.</p>\n\n<p>The main difference between a haar-like feature and a convolution kernel is that the haar-like feature has a 'fixed position' within the image while a kernel is applied to each pixel. </p>\n\n<p>A convolution kernel usually maps a local neighbourhood of a pixel to a value between 0-255. Haar-like features however define a mapping from the entire image to a single value. That is taking the sum of each pixel biased by -1, 0, 1. This bias is depending on the position of the pixel in the image.</p>\n\n<h3>Converting a convolution kernel into a set of features</h3>\n\n<p>That said we can extend a convolution kernel for a single pixel X and make it look similar to a haar-like feature by saying: We map the entire image by computing the sum of all pixels biased by what is given in the kernel for the neighbourhood of X and 0 else. If we do this for all pixels in the image we end up having width*height many features grouping them together in a large feature vector. Clearly they hold the same amount of information a convolution would have. However we loose the ability to easily access the 'origin' of a feature, meaning which pixel the kernel was attached to in order to compute this feature. This information is 'encoded' within the definition of that feature.</p>\n\n<h3>Converting a set of features into a convolution kernel</h3>\n\n<p>So can we reverse above process? For general features this can not be done. In case of the features given above we can take the non zero pixel values as the convolution kernel. If the feature set is chosen well we will end up having the same kernel for each feature. Then yes, we can get to a kernel from the feature set. Further we can find the 'origin' of the feature by averageing the position of each nonzero pixel in the feature's map.</p>\n\n<p>In case of our haar-like features the answer is conveniently given in the paper. The convolution kernel is something of size 6 x 6 with the left half being 1 and the right half being -1, just as you suggested. (Top being 1 and bottom being -1 for the other mask.) Now the center of this 6 x 6 kernel would be 3.5 x 3.5 which is no pixel value.</p>\n\n<p>Further the map defined as the kernel does not map into 0-255. This can be fixed by scaling the result or by applying a threshold. The second will loose some information, but probably provide more stable results.</p>\n\n<p>The key question now is which features were chosen in order to optain the \"haar-like feature edge\" image seen in they're Fig. 5? In other words, at which positions does the kernel have to be evaluated? The best bet is to attach the kernel to every corner of 4 pixels in the image. This way a detected edge lives between pixels, which somehow makes sense as an edge is defined by the gradient between two (or more) pixels.\nNote that the resulting image will have 1 row and 1 column less then the original image. However the resulting values are similar to boundaries and everything between two boundaries is 'the same object'.</p>\n\n<p>Another way this can be done is to choose the (3,3) position of the kernel as the ancor point and attach this point to every pixel in the image just like eigenchris suggested. You could also choose (1,1) or any other point. However, as eigenchris mentioned, edges will appear off. So to make correct classification and to select the right regions of the original image you would have to account for the distance between the (true) center of the kernel and the one you chose.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to compare pose and position of two objects from rotation and translation matrix?",
        "input": "",
        "output": "<p>Let's say the two matrices are M1 and M2, which are applied to vectors from the right:</p>\n\n<pre><code>x1 = M1 * x\nx2 = M2 * x\n</code></pre>\n\n<p>The \"difference\" between the two matrices can be defined as the matrix Md that needs to applied to x1 to get x2:</p>\n\n<pre><code>Md * x1 = x2\nMd * (M1 * x) = M2 * x\n</code></pre>\n\n<p>To create this identity for all vectors x, Md needs to satisfy the equation:</p>\n\n<pre><code>Md * M1 = M2\n</code></pre>\n\n<p>To isolate Md in this equation, we multiply by the inverse of M1:</p>\n\n<pre><code>Md * M1 * inv(M1) = M2 * inv(M1)\n</code></pre>\n\n<p>Matrix multiplication is associative, so we can group the left side as:</p>\n\n<pre><code>Md * (M1 * inv(M1)) = M2 * inv(M1)\n</code></pre>\n\n<p>and the result for Md is:</p>\n\n<pre><code>Md = M2 * inv(M1)\n</code></pre>\n\n<p>So you can solve this with a 4x4 matrix inversion and a matrix multiplication.</p>\n\n<p>An alternative is that you decompose the original matrixes into a rotation and a translation each, which are applied to vectors as:</p>\n\n<pre><code>x1 = R1 * x + t1\nx2 = R2 * x + t2\n</code></pre>\n\n<p>where R1 and R2 are 3x3 matrices, t1 and t2 translation vectors. Following the same steps as before, but also writing the difference broken down into a rotation Rd and a translation td:</p>\n\n<pre><code>Rd * x1 + td = x2\nRd * (R1 * x + t1) + td = R2 * x + t2\nRd * R1 * x + Rd * t1 + td = R2 * x + t2\n</code></pre>\n\n<p>For the rotation parts of both sides to match, Rd needs to satisfy:</p>\n\n<pre><code>Rd * R1 = R2\n</code></pre>\n\n<p>Not surprisingly, the calculation of Rd looks very similar to Md above. As a simplification, we can take advantage of the fact that the inverse of a rotation is its transpose:</p>\n\n<pre><code>Rd = R2 * inv(R1)\nRd = R2 * transp(R1)\n</code></pre>\n\n<p>Now in the equation above, the translation parts also need to match, which gives us td:</p>\n\n<pre><code>Rd * t1 + td = t2\ntd = t2 - Rd * t1\n</code></pre>\n\n<p>In summary, you calculate the rotation matrix and translation vector of the difference as:</p>\n\n<pre><code>Rd = R2 * transp(R1)\ntd = t2 - Rd * t1\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Detect Main Object in Image?",
        "input": "",
        "output": "<p>There was a post on Lyst about exactly that problem. And by exactly I mean segmenting shoes. Here's the <a href=\"http://developers.lyst.com/data/images/2014/02/13/background-removal/\" rel=\"nofollow noreferrer\">link</a></p>\n<p>Basically, the steps they used were :</p>\n<ol>\n<li>Inverting the colors</li>\n<li>Applying Sobel filter</li>\n<li>Applying gaussian blur</li>\n<li>Thresholding</li>\n<li>Flood fill</li>\n</ol>\n<p>The example is complete with python code snippet at the end.</p>\n<p>In your case, if you always have white background, you could probably just use thresholding.</p>\n",
        "system": ""
    },
    {
        "instruction": "Get the SIFT descriptor for specified point using OpenCV",
        "input": "",
        "output": "<p>Usually, keypoint detectors work on a local neighbourhood around a point. This is the <code>size</code> field of OpenCV's <code>KeyPoint</code> class. The <code>angle</code> field is the dominant orientation of the keypoint (this could be set to -1, note).</p>\n\n<p><a href=\"http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_feature_detectors.html?highlight=featuredetector#FeatureDetector\" rel=\"nofollow\">OpenCV KeyPoint class</a></p>\n\n<p><a href=\"http://physics.nyu.edu/grierlab/manuals/opencv/classcv_1_1KeyPoint.html\" rel=\"nofollow\">Another reference here.</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Low contrast image segmentation",
        "input": "",
        "output": "<p>Try clustering the image by gray level using a clustering method such as kmeans. Below I've used kmeans directly on the images without any gray level transformations (using 3 clusters gave me better results). You should be able to improve results by clustering a preprocessed image using methods outlined in the comments.</p>\n\n<p><img src=\"https://i.sstatic.net/7sQZJ.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/JgB4f.png\" alt=\"enter image description here\"></p>\n\n<p>Shape of the clusters may slightly vary due to the randomness of kmeans.</p>\n\n<p>Now if you take connected components of the clustered image and calculate the average gray level of those regions, the defects should have a lower average than the other regions.</p>\n\n<p>I did clustering part in Matlab.</p>\n\n<pre><code>im = imread('r2SOV.png');%Uy1Fq r2SOV\ngr = im;\nsize = size(gr);\n\n% perform closing using a 5x5 circular structuring element\nsel = strel('disk', 2, 4);\nmcl = imclose(gr, sel);\n% cluster gray levels using kmeans: using 3 clusters\nx = double(mcl(:));\nidx = kmeans(x, 3);\ncl = reshape(idx, size);\n\nfigure, imshow(label2rgb(cl))\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Can OpenCV, or similar, be used to measure a picture frame from a photo?",
        "input": "",
        "output": "<p>No can do from a single image, without additional information. Absolute scale is lost in projection. A good survey of what is possible/not possible from single image is <a href=\"http://www.cs.cmu.edu/~ph/869/papers/Criminisi99.pdf\" rel=\"nofollow\">this paper</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is the best camera calibration method?",
        "input": "",
        "output": "<p>There are many possible sources of error.  </p>\n\n<p>First of all, while all three of the calibration implementations you have tried use essentially the same algorithm, there are enough differences that explain the discrepancies in the results. </p>\n\n<p>The main difference is in the checkerboard corner detection. The Caltech Calibration Toolbox does not have automatic checkerboard detection, and uses a second optimization pass to refine the corners. Both OpenCV and the Camera Calibrator App do detect the checkerboard automatically, but the algorithm used in the Camera Calibrator App is far better. It is more robust, meaning that it is likely to detect a board when OpenCV does not, and its sub-pixel localization is more precise. My point is that in these three approaches you are calibrating using different data points.  So it is not surprising that your results are different.</p>\n\n<p>Once you have calibrated, what kind of reprojection errors are you getting?  The Camera Calibrator App shows you a bar graph of the reprojection errors. You should look at it, and exclude the images that give you high errors. Ideally, you want your mean reprojection error to be less than half a pixel. The lower the better. </p>\n\n<p>Now I have to ask you how you are measuring the distance from the camera to the checkerboard?  The extrinsics you are getting from the calibration represent the transformation from the checkerboard's coordinate system into the camera's coordinate system, whose origin is <em>inside</em> the camera case, at its optical center. This is hard to measure accurately. A better way would be to <a href=\"http://www.mathworks.com/help/vision/examples/measuring-planar-objects-with-a-calibrated-camera.html?refresh=true\">place a flat object of a known size on the checkerboard and measure it using the camera</a>. In fact, you can measure the distances between the detected checkerboard corners. Note that the detection accuracy is another source of error.</p>\n\n<p>Another thing, please be sure not to save your calibration images as jpeg. The compression artifacts will affect the accuracy of the checkerboard corner detection. Use a lossless format like tiff or png.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can I find camera pose from matlab calibration?",
        "input": "",
        "output": "<p>Pose relative to what?  The extrinsic parameters give you the pose of the camera relative to the checkerboard in each of the calibration images. Pose here is represented as a translation and a rotation from the checkerboard's coordinate system into the camera's coordinate system.</p>\n\n<p>By the way, there is now <a href=\"http://www.mathworks.com/videos/camera-calibration-with-matlab-81233.html\" rel=\"nofollow\">support for camera calibration in the Computer Vision System Toolbox</a>. There is also a function called <code>extrinsics</code>, which computes the pose of a calibrated camera (in terms of translation and rotation) relative to a world coordinate system (e. g. represented by checkerboard points).</p>\n",
        "system": ""
    },
    {
        "instruction": "find the edge based on normals",
        "input": "",
        "output": "<p>An intuitive definition of an edge in a depth image is where the surface normal faces away from the viewer. Assuming a viewing direction <code>[0 0 -1]</code> (into the XY plane) any normal that has nearly vanishing <code>z</code> component can be characterized as an edge.</p>\n\n<pre><code>e = abs( depth(:,:,3) ) &lt; 1e-3; %// a nice starting point\n</code></pre>\n\n<p>You need to set the threshold based on your data.</p>\n\n<p>After that you might consider applying some non-maximal suppression or other morphological cleaning operations.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting triangles using hough transforms",
        "input": "",
        "output": "<p>Have you tried using contrast? If the image above is a representative example, the contrast from the white to the black and then to the green gives a very distinctive feature to be recognised. Make sure to threshold the image before applying edge detection.</p>\n\n<p>You could also try mapping the lines found from contrast to a triangle of the specific size you wanted (I notice you mentioned mapping to triangles, but no mention of using size). Note however that a size constraint would be affected by the distance of the camera from the object.</p>\n\n<p>Also, if you have full control over the object for detection (as it looks like you are using this to detect something of your own making), you could add other shapes within the triangle, or more contrast layers. Full circles are a good shape to use (as most noise won't have a complete loop)</p>\n",
        "system": ""
    },
    {
        "instruction": "Can rotating a polygon 180 degree produce its reflection?",
        "input": "",
        "output": "<p>It depends on the shape. Some polygons have rotational symmetry, some have reflectional symmetry. If you rotate a rectangle by 180 degrees, or reflect it in a suitable axis, you get back the same shape.</p>\n\n<p>But generally, no. They are different operations. If you rotate a shape by 180 degrees, you get an upside down shape, not a reflected shape.</p>\n",
        "system": ""
    },
    {
        "instruction": "Prospective algorithmic approach for this image in OpenCV",
        "input": "",
        "output": "<p>Following is a simple approach to your problem, if I understood you correctly. My result is shown below. </p>\n\n<p><img src=\"https://i.sstatic.net/6HxJj.jpg\" alt=\"The result of the code I have posted\"></p>\n\n<p>And here is the code</p>\n\n<pre><code>int max_area_threshold = 10000;\nint min_area_threshold = 1000;\nfloat rational_threshold = 0.7;\n\ncv::Mat img = cv::imread(\"sample.jpg\", CV_8UC1);\ncv::Mat img_binary;\n\n//Create binary imae by tresholding\ncv::threshold(img, img_binary, 25, 255, CV_THRESH_BINARY);\n\n//Invert black-white\ncv::bitwise_not(img_binary, img_binary);\n\n//Eliminating small segments\ncv::erode(img_binary, img_binary, cv::Mat(), cv::Point(-1, -1), 2, 1, 1);\ncv::dilate(img_binary, img_binary, cv::Mat(), cv::Point(-1, -1), 1, 1, 1);\n\n//Find contours\nstd::vector&lt;std::vector&lt;cv::Point&gt;&gt; contours;\nstd::vector&lt;cv::Vec4i&gt; hierarchy;\ncv::findContours( img_binary, contours, hierarchy, CV_RETR_TREE, CV_CHAIN_APPROX_SIMPLE);\n\nfor( int i = 0; i&lt; contours.size(); i++ )\n{\n    if(contours[i].size() &lt; 5)\n        continue;\n\n    //Fit ellipse to contour\n    cv::RotatedRect boundRect = cv::fitEllipse(contours[i]);\n\n    //Check the squareness of the bounding box\n    if(abs((boundRect.size.width / (float)boundRect.size.height)-1.0) &gt; rational_threshold)\n        continue;\n\n    //Elliminate too big segments\n    if(boundRect.boundingRect().area() &gt; max_area_threshold)\n        continue;\n\n    //Elliminate too small segments\n    if(boundRect.boundingRect().area() &lt; min_area_threshold)\n        continue;\n\n    drawContours(img, contours, i, cv::Scalar(255), 0.2, 8, hierarchy, 0, cv::Point() );\n}\n\ncv::imwrite(\"result.jpg\", img);\n</code></pre>\n\n<p>I hope it helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "Error in Fundamental Matrix?",
        "input": "",
        "output": "<p>Please note that from decomposing E, 4 solutions are possible (2 possible rotations X 2 possible translations).\nSpecifically regarding R, it can also be:\nR = U<em>W</em>transpose(V);\nSimilarly, T can also be:\nT = -U(:,3);</p>\n\n<p>To check if this is your bug, please post here all the 4 possible solutions for a given case where you get ThetaZ~90. </p>\n\n<p>Another thing I would check (since you have K), is estimating the essential matrix directly (without going through fundamental matrix): <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/47032-camera-geometry-algorithms/content//CV/CameraGeometry/EssentialMatrixFrom2DPoints.m\" rel=\"nofollow\">http://www.mathworks.com/matlabcentral/fileexchange/47032-camera-geometry-algorithms/content//CV/CameraGeometry/EssentialMatrixFrom2DPoints.m</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Does the chessboard pattern need to remain the same after a camera has been calibrated?",
        "input": "",
        "output": "<p>Once you calibrate, you can use any checkerboard to localize your camera.  In fact, you can use any set of reference points with known 3D world coordinates, as long as you can accurately detect them in the image. The <code>extrinsics</code> function in the Computer Vision System Toolbox takes a set of image points and a set of corresponding world points, regardless of where they come from.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding a rectangle and its corners in an image",
        "input": "",
        "output": "<p>If your rectangles are well defined as pictures show, then you can use <a href=\"http://en.wikipedia.org/wiki/Hough_transform\" rel=\"nofollow\">Hough transform</a> to determine parameters of lines (rectangle edges) and identify rectangle position and orientation.</p>\n",
        "system": ""
    },
    {
        "instruction": "Using Vision Caltech Camera Calibration toolbox for MATLAB",
        "input": "",
        "output": "<p>There is an <a href=\"http://www.mathworks.com/videos/camera-calibration-with-matlab-81233.html\" rel=\"nofollow\">easier way to calibrate cameras in MATLAB</a>. The Computer Vision System Toolbox also includes the <code>undistortImage</code> function, which is what you are looking for.</p>\n",
        "system": ""
    },
    {
        "instruction": "Arguments of the Kalman Filter in OpenCV",
        "input": "",
        "output": "<p>As described by <a href=\"http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html?highlight=kalman#kalmanfilter-kalmanfilter\" rel=\"nofollow\">the documentation</a>, the 2,1, and 0 are the dimensionalities of the state, measurements, and control spaces.</p>\n\n<p>That is, your state vector has length 2, measurements are scalars, and there is no control signal.</p>\n",
        "system": ""
    },
    {
        "instruction": "Specifying the type of the vehicule detected in a Video frame",
        "input": "",
        "output": "<p>This problem is an active research area, and there are many possible approaches.  One possibility is to train a classifier to distinguish a car from a truck.  You can use this <a href=\"http://www.mathworks.com/help/vision/examples/digit-classification-using-hog-features.html\" rel=\"nofollow\">example showing how to classify digits using HOG features and an SVM classifier</a> to get started.</p>\n",
        "system": ""
    },
    {
        "instruction": "Image matching and object detection, how to perform detection for small object?",
        "input": "",
        "output": "<p>the standard approach is to do sliding window detection, i.e start with a window size, slide it over the image and take the window (at some position x,y) which matches best, this thing can be done with windows of multiple sizes.</p>\n",
        "system": ""
    },
    {
        "instruction": "iOS detect rectangles from camera with openCV",
        "input": "",
        "output": "<p>So the solution was actually pretty simple...</p>\n\n<p>Instead of trying to use <code>matImage</code> to set the <code>imageView.image</code>, it just needed to transform <code>matImage</code> to be actually modified in the imageView since the CvVideoCamera was already initialized with (and linked to) the imageView:</p>\n\n<p><code>self.videoCamera = [[CvVideoCamera alloc]initWithParentView:self.imageView];</code></p>\n\n<p>finally the function was like this: </p>\n\n<pre><code>#ifdef __cplusplus\n-(void)processImage:(cv::Mat &amp;)matImage\n{\n    matImage = CVSquares::detectedSquaresInImage(matImage, self.angleTolerance, self.threshold, self.levels, self.accuracy);\n}\n#endif\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Saving Kinect Frames as screenshots",
        "input": "",
        "output": "<p>just try:</p>\n\n<pre><code>private unsafe void saveFrame(Object reference)\n{\n  MultiSourceFrame mSF = (MultiSourceFrame)reference;\n\n  using (var frame = mSF.DepthFrameReference.AcquireFrame())\n  {\n      if (frame != null)\n      {\n          using (Microsoft.Kinect.KinectBuffer depthBuffer = frame.LockImageBuffer())\n          {\n              if ((frame.FrameDescription.Width * frame.FrameDescription.Height) == (depthBuffer.Size / frame.FrameDescription.BytesPerPixel))\n              {\n                  ushort* frameData = (ushort*)depthBuffer.UnderlyingBuffer;\n                  byte[] rawDataConverted = new byte[(int)(depthBuffer.Size / 2)];\n\n\n                  for (int i = 0; i &lt; (int)(depthBuffer.Size / 2); ++i)\n                  {\n                      ushort depth = frameData[i];\n                      rawDataConverted[i] = (byte)(depth &gt;= frame.DepthMinReliableDistance &amp;&amp; depth &lt;= frame.DepthMaxReliableDistance ? (depth) : 0);\n                  }\n\n                  String date = string.Format(\"{0:hh-mm-ss}\", DateTime.Now);\n                  String filePath = System.IO.Directory.GetCurrentDirectory() + \"/test/\" +date+\".raw\";\n                  File.WriteAllBytes(filePath, rawDataConverted);\n                  rawDataConverted = null;\n\n              }\n          }\n      }\n    }\n }\n</code></pre>\n\n<p>You also take a look here:\n<a href=\"https://stackoverflow.com/questions/32883611/kinect-v2-saving-raw-detph-data\">Saving raw detph-data</a></p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Template Matching Wrong Match Score / Area",
        "input": "",
        "output": "<p>It doesn't work because when pictures are converted in grayscale, they might look similar.</p>\n\n<p>cv::minMaxLoc doesn't work for this kind of situations.</p>\n\n<p>You should use something else, such as Feature extractor or edge detector and compare it using a metric such as Mahalanobis distance</p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab code for Local binary pattern",
        "input": "",
        "output": "<p>It's a vectorized implementation of LBP, rather well-suited for Matlab.</p>\n\n<p>After the initialization instructions, let's look the main loop, beginning at the line \"<code>for i = 1:neighbors</code>\". The loop is pretty clear: it computes the comparison of one neighbor with the center pixel, the loop iterates over all neighbors. You've got this point, so now enter deep into the loop to understand how it accumulates all results.</p>\n\n<p>The core of the loop is in fact over complicated because it takes into account the real circle instead of an approximate integer circle. So the purpose of the major part of the instructions is to compute the  interpolated intensity of the neighbor pixel. Here it differs from the C++ code you have as reference, where it takes only the integer, 1-pixel-wide-radius circle. Remember that with the lbp.m code you can -- theoretically, I will discuss that later -- compute the LBP along a circle of radius R with N sampling points, so the C++ would correspond to a circle of radius 1 and with 8 sampling points, if only there was no interpolation. But there is an interpolation when the neighbor does not fit the pixel grid of the image, when <code>(abs(x - rx) &lt; 1e-6) &amp;&amp; (abs(y - ry) &lt; 1e-6)</code> is false).</p>\n\n<p>If <code>(abs(x - rx) &lt; 1e-6) &amp;&amp; (abs(y - ry) &lt; 1e-6)</code> is true, there is no interpolation, so the computation of all comparisons between the central pixel and the current neighbor is stored directly into <code>D</code>. Else, it computes a bilinear interpolation of the intensity at the sampling neighbor point, over the entire image: <code>N = w1*d_image(fy:fy+dy,fx:fx+dx) + w2*d_image(fy:fy+dy,cx:cx+dx) +  w3*d_image(cy:cy+dy,fx:fx+dx) + w4*d_image(cy:cy+dy,cx:cx+dx);</code>.</p>\n\n<p>And finally, turn to the update part: <code>v = 2^(i-1);  result = result + v*D;</code>. <code>v</code> is the equivalent of the shift: for the ith neighbor, you shift the value of the comparison by <code>i-1</code> to the left, or equivalently multiplying be <code>2^(i-1)</code>. Then you sum with <code>result</code>. So at the end of the loop, the computation is really equivalent to your C++ code, except that it's done over the entire image instead of one single pixel. And the C++ code can be seen as a unrolled version of the matlab loop with the neighbor circle of radius 1 and 8 sampling points. At this point, the LBP map is computed, the following blocks are additional processing of the LBP map (remap through a mapping table, and optionally computing the histogram of the LBP image instead of the LBP image itself).</p>\n\n<p>Now, a little discussion about the whole script. There is a flaw here that is hidden at the end of the script. In fact, through the code, you are limited to 32 neighbors, no more, because at the end the LBP image is cast to <code>int32</code>. The flaw is that the variable <code>result</code> is allocated as a double matrix and not an integer matrix, so I really hope that there is no approximation problem when updating <code>result</code> and later when casting into integer, leading to changing bits in the LBP. Normally there should not be as there is at least 52 precision bits (according to <a href=\"http://en.wikipedia.org/wiki/Double-precision_floating-point_format\">wikipedia</a> for IEEE 754 spec). I think it's risky here ... and on the contrary I am not aware of a matlab type for long fixed-sized, efficient bit vector. I would use <code>int64</code> instead of <code>int32</code>, but the limit will be there at 64 sampling neighbors.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Now, if your wish is to commpute some local binary patterns restricted on the 3*3 neighborhood, this Matlab function is way too generic for you, and the best thing is to unroll the loop for this neighborhood, and thus be really close to the C++ code. Here is a piece of code for that (I use bitwise or instead of addition, but it's equivalent):</p>\n\n<pre><code>result = uint8(ysize, xsize);\nresult = (image(1:end-2,2:end-1) &gt; image(2:end-1,2:end-1));                                 % &lt;=&gt; v += (img(y-1,x  ) &gt; c) &lt;&lt; 0;\nresult = result|bitshift((image(1:end-2,3:end) &gt; image(2:end-1,2:end-1)), 1, 'uint8');      % &lt;=&gt; v += (img(y-1,x+1) &gt; c) &lt;&lt; 1;\nresult = result|bitshift((image(2:end-1,3:end) &gt; image(2:end-1,2:end-1)), 2, 'uint8');      % &lt;=&gt; v += (img(y  ,x+1) &gt; c) &lt;&lt; 2;\nresult = result|bitshift((image(3:end,3:end) &gt; image(2:end-1,2:end-1)), 3, 'uint8');        % &lt;=&gt; v += (img(y+1,x+1) &gt; c) &lt;&lt; 3;\nresult = result|bitshift((image(3:end,2:end-1) &gt; image(2:end-1,2:end-1)), 4, 'uint8');      % &lt;=&gt; v += (img(y+1,x  ) &gt; c) &lt;&lt; 4;\nresult = result|bitshift((image(3:end,1:end-2) &gt; image(2:end-1,2:end-1)), 5, 'uint8');      % &lt;=&gt; v += (img(y+1,x-1) &gt; c) &lt;&lt; 5;\nresult = result|bitshift((image(2:end-1,3:end) &gt; image(2:end-1,2:end-1)), 6, 'uint8');      % &lt;=&gt; v += (img(y  ,x-1) &gt; c) &lt;&lt; 6;\nresult = result|bitshift((image(1:end-2,1:end-2) &gt; image(2:end-1,2:end-1)), 7, 'uint8');    % &lt;=&gt; v += (img(y-1,x-1) &gt; c) &lt;&lt; 7;\n</code></pre>\n\n<p>It's the exact translation of the C code to a Matlab script, using the powerful vectorization. With this in hand, it's pretty simple to change for another order or different tests in this neighborhood. I also mention this point because there is an error in the Matlab script for this case, line 53 there is a wrong sign: neighobrhood is better as<code>spoints=[-1 -1; -1 0; -1 1; 0 -1; 0 -1; 1 -1; 1 0; 1 1];</code> instead of <code>spoints=[-1 -1; -1 0; -1 1; 0 -1; -0 1; 1 -1; 1 0; 1 1];</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Camera pose estimation",
        "input": "",
        "output": "<p>In order to compute a camera pose, you need to have a <strong>reference frame</strong> that is given by some <strong>known points</strong> in the image.\nThese known points come for example from a <a href=\"http://docs.opencv.org/doc/tutorials/calib3d/table_of_content_calib3d/table_of_content_calib3d.html?highlight=calibration\" rel=\"nofollow\">calibration pattern</a>, but can also be some known <em>landmarks</em> in your images (for example, the 4 corners of teh base of Gizeh pyramids).</p>\n\n<p>The problem of estimating the pose of the camera given known landmarks seen by the camera (ie, finding 3D position from 2D points) is classically known as <strong>PnP</strong>.\nOpenCV provides you a <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#solvepnp\" rel=\"nofollow\">ready-made solver</a> for this problem.</p>\n\n<p>However, you need first to <strong>calibrate</strong> your camera, ie, you need to determine what makes it unique.\nThe parameters that you need to estimate are called <em>intrinsic</em> parameters, because they will depend on the camera focal length, sensor size... but not on the camera location or orientation.\nThese parameters will mathematically explain how world points are projected onto your camera sensor frame.\nYou can estimate them from known planar patterns (again, OpenCV has some <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#calibratecamera\" rel=\"nofollow\">ready-made functions</a> for that).</p>\n",
        "system": ""
    },
    {
        "instruction": "Performing Camera Calibration Checkered board pattern",
        "input": "",
        "output": "<p>A concern with a symmetric checkerboard is that you cannot use it to compute absolute poses w.r.t. the camera if there is a possibility of occlusions - since you cannot tell which corner is which if they aren't all visible. There are nonsymmetrical patterns that can deal with this issue as well. My favorite is Matsunaga and Kanatani's \"2D barcode\" one, which uses sequences of square lengths with unique crossratios. See the paper <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.4689&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">here</a> </p>\n",
        "system": ""
    },
    {
        "instruction": "Substitute estimateGeometrictransformation",
        "input": "",
        "output": "<p>If <code>points</code> is a <code>briskPoints</code> object, you should be able to use <code>points.Location</code> to get the x-y coordinates, and pass them to <code>estimateGeometricTransformation</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Obtaining depth image from 2D image",
        "input": "",
        "output": "<p>The statement <code>DIBR can be used</code> is wrong. DIBR is a method that works with 3D videos, <strong><em>using</em></strong> the depth image to enhance the 2D view. It is not fully automated, you need to supply the depth map by hand for DIBR to work.</p>\n\n<p>You can, however, use 2 cameras to obtain 3D information from 2 x 2D; or use <strong>2D video</strong> to <strong><em>estimate</em></strong> depth from pixel motion. Some other approaches use perspective information of the 2D image to imitate human understanding of depth - like roads, buildings.</p>\n\n<p>In conclusion, depth map generation from plain 2D image is a harder problem than that of stereo vision, or infrared projectors used in depth sensors.</p>\n",
        "system": ""
    },
    {
        "instruction": "Compare SIFT operator and MOPS operator",
        "input": "",
        "output": "<p><em>How do they make the descriptor rotation-invariant?</em></p>\n\n<p>This is explained by D. Lowe in the <a href=\"http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf\" rel=\"nofollow\">original SIFT paper</a> (see Chapter 5. Orientation Assignment):</p>\n\n<blockquote>\n  <p>By assigning a <strong>consistent orientation</strong> to each keypoint based on local image\n  properties, the keypoint descriptor can be represented relative to this\n  orientation and therefore achieve invariance to image rotation.</p>\n</blockquote>\n\n<p><em>How do they make the descriptor illumination invariant?</em></p>\n\n<p>Here again this is detailed in this paper (see Chapter 6.1. Descriptor Representation):</p>\n\n<blockquote>\n  <p>[...] the feature vector is modified to reduce the effects of illumination\n  change. First, the vector is <strong>normalized to unit length</strong> [...] we reduce the\n  influence of large gradient magnitudes by <strong>thresholding the values</strong> [...] then\n  renormalizing to unit length</p>\n</blockquote>\n\n<p>According to the <a href=\"http://research.microsoft.com/pubs/70120/tr-2004-133.pdf\" rel=\"nofollow\">original paper</a>, MOPS follows the same principles.</p>\n",
        "system": ""
    },
    {
        "instruction": "Logo detection/recognition in natural images",
        "input": "",
        "output": "<p>There are many algorithms from shape matching to haar classifiers. The best algorithm very depend on kind of logo. </p>\n\n<p>If you want to continue with feature registration, i recommend:</p>\n\n<ol>\n<li><p>For detection of small logos, use tiles. Split whole image to smaller (overlapping) tiles and perform usual detection. It will use \"locality\" of searched features. </p></li>\n<li><p>Try ASIFT for affine invariant detection. </p></li>\n<li><p>Use many template images for reference feature extraction, with different lightning , different background images (black, white, gray) </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Tonal equalization on different images",
        "input": "",
        "output": "<p>Once I participated in the development of surveillance system and there was a similar problem too. Instead of histogram equalization it used a different approach. Algorithm was as following:</p>\n\n<pre><code>For every pixel:\n- Calculate an average luminosity around the pixel with some window (e.g. at square with 50 pixels)\n- Calculate multiplier as some fixed value (like 128 for byte sized components)\ndivided by the average from previous step. Account for possible \ndivision by zero here. So Multiplier = 128 / (Average + small value)\n- Multiply pixel luminosity by multiplier.\n</code></pre>\n\n<p>Averages can be calculated incrementally in one go for the whole frame and in real time. It allowed to remove luminosity variations  from clouds, changing weather etc. rather effectively.</p>\n",
        "system": ""
    },
    {
        "instruction": "Normalized cut: what does this code do?",
        "input": "",
        "output": "<p>I believe you came across a piece of code written by <a href=\"http://www1.icsi.berkeley.edu/~stellayu/\" rel=\"nofollow\">Prof Stella X Yu</a>.<br>\nIndeed, when <code>W</code> is positive this code has no effect and this is the usual case for NCuts.<br>\nHowever, in a <a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/stellayu/doc/cvpr01.pdf\" rel=\"nofollow\">CVPR 2001 paper</a> Yu and Shi extend NCuts to handle <strong>negative</strong> interactions as well as positive ones. In these circumstances <code>dr</code> (r for \"repulsion\") plays a significant role.</p>\n\n<p>Speaking of negative weights, I must say that personally I do not agree with the approach of Yu and Shi.<br>\nI strongly believe that when there is repulsion information <a href=\"http://www.wisdom.weizmann.ac.il/~bagon/pub/LargeScaleCorrClust_2011.pdf\" rel=\"nofollow\">Correlation Clustering</a> is a far better objective function than the extended NCuts objective. Results of some image segmentation experiments I conducted with negative weights suggested that Correlation clustering objective is better than the extended NCuts.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding clusters in matrices",
        "input": "",
        "output": "<p>Cluster analysis is not the tool that you are looking for.</p>\n\n<p>Your requirements may be more of image processing nature than clustering.</p>\n\n<p>Clustering doesn't operate \"visually\" on matrixes. One cell is pretty much like another in clustering, and one mostly uses matrixes because there are fast implementations available, and you end up with a short notion and reasonably high performance at the same time.\nThe type of matrixes used by Clustering are usually <em>distance matrixes</em>.</p>\n\n<p>Furthermore, most clustering algorithms will assume attributes to be <em>continuous</em>.</p>\n\n<p>If you simply map your cell to <code>(x,y,value)</code> I'm not at all surprised it does not work. The numerical ranges of the three colums do not match, and a \"point\" with value 0 just isn't the same as one with value 1. It's just not the data model appropriate for such algorithms.</p>\n\n<p>The thing is, none of the 100 clustering algorithms I've used does what you want.</p>\n\n<p>Instead, image analysis techniques will be more likely to help you.</p>\n",
        "system": ""
    },
    {
        "instruction": "Cases where Morphological Opening and Closing yields the same results?",
        "input": "",
        "output": "<p>Yes there are.  As one small example, if you had a binary image where it consists of a bunch of squares that are disconnected and distinct.  Provided that you specify a structuring element that is square, and choosing the structuring element so that it is smaller than the smallest square in the image, then doing either operation will give you the same results.</p>\n\n<p>If you did an opening on this image and a closing on this image, you will produce the same results.  Remember, an opening is an erosion followed by a dilation where a closing is a dilation followed by an erosion.  In terms of analyzing the shapes, erosion slightly shrinks the area of the image while dilation slightly enlarges it.</p>\n\n<p>By doing an erosion followed by a dilation (opening), you're shrinking the object and then growing it again.  This will bring the image back to where it was before, provided that you choose the structuring element like what we talked about before.  Similarly, if you did an dilation followed by an erosion (closing), you're growing the object and then shrinking it again, also bringing the image back to where it was before... following that same guideline I just talked about of course.  </p>\n\n<p>If you were to choose a structuring element where it is larger than the smallest object, doing an opening will <strong>remove</strong> this object from the image, and so you won't get the original image back.  Also, you need to make sure that the objects are well far away from each other, and that the size of the structuring element does not overlap any of the objects as you slide over and do the morphology operations.  The reason why is because if you were to do a closing, you would join these two objects together and so that won't get you the same results either!</p>\n\n<hr>\n\n<p>Here's an example image that I generated that is binary:</p>\n\n<p><img src=\"https://i.sstatic.net/XGbZB.png\" alt=\"enter image description here\"></p>\n\n<p>To generate this image in MATLAB, you can do:</p>\n\n<pre><code>A = false(200,200);\nA(30:60,30:60) = true;\nA(90:110,90:110) = true;\nA(10:30, 135:155) = true;\nA(150:180,100:120) = true;\n</code></pre>\n\n<p>In Python, you can do this with <code>numpy</code>:</p>\n\n<pre><code>import numpy as np\nA = np.zeros((200,200), dtype='uint8')\nA[29:60,29:60] = 255\nA[89:110,89:110] = 255\nA[9:30, 134:155] = 255\nA[149:180, 99:120] = 255\n</code></pre>\n\n<p>The reason why I had to create the array as <code>uint8</code> in <code>numpy</code> is because when we want to show this image, I'm going to use OpenCV and it requires that the image be at least a <code>uint8</code> type.</p>\n\n<p>Now, let's choose a 5 x 5 square structuring element, and let's perform a closing and an opening with this image.  We will display the results in a single figure going from left to right:</p>\n\n<pre><code>se = strel('square', 5);\nA_close = imclose(A, se);\nA_open = imopen(A, se);\nfigure;\nsubplot(1,3,1);\nimshow(A);\ntitle('Original');\nsubplot(1,3,2);\nimshow(A_close);\ntitle('Closed');\nsubplot(1,3,3);\nimshow(A_open);\ntitle('Open');\n</code></pre>\n\n<p>This is the result:</p>\n\n<p><img src=\"https://i.sstatic.net/Ae9sR.png\" alt=\"enter image description here\"></p>\n\n<p>It certainly looks the same!  To really show the difference, let's subtract the closed and opened result from the original image.  You should get a blank image in the end if they're both equal to the original image.</p>\n\n<pre><code>figure;\nsubplot(1,2,1);\nimshow(abs(double(A) - double(A_close)));\nsubplot(1,2,2);\nimshow(abs(double(A) - double(A_open)));\n</code></pre>\n\n<p>Bear in mind that I converted the images to <code>double</code> to facilitate subtraction, and I used <code>abs</code> to ensure that negative differences are reflected.  This is what I get:</p>\n\n<p><img src=\"https://i.sstatic.net/EZgGF.png\" alt=\"enter image description here\"></p>\n\n<p>As you can see, both results are totally blank, meaning they're exact copies of the original image after each result.</p>\n\n<p>The equivalent code in Python for the first part is the following:</p>\n\n<pre><code>import cv2\nse = np.ones((5,5), dtype='uint8')\nA_close = cv2.morphologyEx(A, cv2.MORPH_CLOSE, se)\nA_open = cv2.morphologyEx(A, cv2.MORPH_OPEN, se)\ncv2.imshow('Original', A)\ncv2.imshow('Close', A_close)\ncv2.imshow('Open', A_open)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>\n\n<p>Here's what I get:</p>\n\n<p><img src=\"https://i.sstatic.net/ZVXh5.png\" alt=\"enter image description here\"></p>\n\n<p>You'll need to install the OpenCV package for this Python code.  I displayed all of the images as three separate figures, then left the windows there until you choose any one of them and push a key.  Once you do this, all of the windows will close.  If you want to show the subtraction stuff, this is the code in Python:</p>\n\n<pre><code>A_close_diff = A - A_close\nA_open_diff = A - A_open\ncv2.imshow('Close Diff', A_close_diff)\ncv2.imshow('Open Diff', A_open_diff)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>\n\n<p>I didn't name the figures in MATLAB because what we're showing is obvious, but for OpenCV, you need to name the windows, and so I put names that describe what we're showing for each.  I also didn't need to take the absolute value, because in <code>numpy</code>, doing arithmetic operations that result in an overflow or underflow will simply wrap around itself, while for MATLAB, the values get clipped.  That's why for MATLAB, I needed to convert to <code>double</code> and take the absolute value because <code>imshow</code> doesn't display negative intensities or if we were to have a situation where we did <code>0 - 1</code>, the output would be 0 and you wouldn't be able to show that this location has a difference.  With Python, doing <code>0 - 1</code> for <code>uint8</code>, will result in 255, so we can certainly see a difference here.... so there's no need to do any of this <code>abs</code> and casting stuff that we did in MATLAB.  Here's what I get:</p>\n\n<p><img src=\"https://i.sstatic.net/hDnfC.png\" alt=\"enter image description here\"></p>\n\n<hr>\n\n<p>In general, you can reproduce what I did with any kind of shape and any size shape, so long as you choose a structuring element that mimics the properties of the shape that is in your image, and you choose a structuring element that is smaller than the smallest shape seen in that image.  I'm sure there are many more examples that don't have to follow these specific guidelines, but this is the best example that I can think of at this moment.</p>\n\n<hr>\n\n<p>This should hopefully get you started.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Efficient way of using ssim() function in Matlab for comparing image structures (or any other alternative)",
        "input": "",
        "output": "<p>Never mind, here I'm going through all image pairs, twice (switched parameters), which is not needed.So it is possible to reduce the speed by n-1/2.</p>\n\n<p>If you want efficiency over accuracy (which in my case, it is), finding the score from the correlation of histograms is one possible way.</p>\n\n<p>It took me 55 seconds to process 72 frames with ssim(), while only 1.2 seconds with difference of histograms. </p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV CV_* functions error",
        "input": "",
        "output": "<p>Here's OpenCV 3.0 documentation:\n<a href=\"http://docs.opencv.org/trunk/index.html\" rel=\"nofollow\">http://docs.opencv.org/trunk/index.html</a></p>\n\n<p>Conversion constants are described here:\n<a href=\"http://docs.opencv.org/trunk/modules/imgproc/doc/miscellaneous_transformations.html\" rel=\"nofollow\">http://docs.opencv.org/trunk/modules/imgproc/doc/miscellaneous_transformations.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Scale and multiscale space Theory",
        "input": "",
        "output": "<p>\"Scale\" here alludes to both the size of the image as well as the size of the objects themselves... at least for current feature detection algorithms.  The reason why you construct a scale space is because we can focus on features of a particular size depending on what scale we are looking at.  The smaller the scale, the coarser or smaller features we can concentrate on.  Similarly, the larger the scale, the finer or larger features we can concentrate on. </p>\n\n<p>You do all of this on the same image because this is a common pre-processing step for feature detection.  The whole point of feature detection is to be able to detect features over multiple scales of the image.  You only output those features that are reliable over all of the different scales.  This is actually the basis of the <a href=\"http://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" rel=\"nofollow noreferrer\">Scale-Invariant Feature Transform (SIFT)</a> where one of the objectives is to be able to detect keypoints robustly that can be found over multiple scales of the image.</p>\n\n<p>What you do to create multiple scales is decompose an image by repeatedly subsampling the image and blurring the image with a Gaussian filter at each subsampled result.   This is what is known as a <a href=\"http://en.wikipedia.org/wiki/Scale_space\" rel=\"nofollow noreferrer\">scale space</a>.  A typical example of what a scale space looks like is shown here:</p>\n\n<p><img src=\"https://i.sstatic.net/4Rs2P.png\" alt=\"\"></p>\n\n<p>The reason why you choose a Gaussian filter is fundamental to the way the scale space works.  At each scale, you can think of each image produced as being a more \"simplified\" version of the one found from the previous scale.  With typical blurring filters, they introduce new spurious structures that don't correspond to those simplifications made in the finer scales.  I won't go into the details, but there is a whole bunch of scale space theory where in the end, scale space construction using the Gaussian blur is the most fundamental way to do this, because new structures are not created when going from a fine scale to any coarse scale.  You can check out that Wikipedia article I linked you to above that talks about the scale space for more details.</p>\n\n<hr>\n\n<p>Now, traditionally a scale space is created by convolving your image with a Gaussian filter of various standard deviations, and that Wikipedia article has a nice pictorial representation of that.  However, when you look at more recent feature detection algorithms like SURF or SIFT, they use a combination of blurring using different standard deviations as well as subsampling the image, which is what I talked about at the beginning of this post.</p>\n\n<p>Either way, check out that Wikipedia post for more details.  They talk about about this stuff more in depth than what I've done here.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "How to find the 3D point for 2 images",
        "input": "",
        "output": "<p>For the triangulation algorithm see Hartley, Richard, and Andrew Zisserman. Multiple View Geometry in Computer Vision. Second Edition. Cambridge, 2000. p. 312.</p>\n\n<p>Or you can use the <code>triangulate</code> function in the Computer Vision System Toolbox for MATLAB.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detection through a webcam",
        "input": "",
        "output": "<p>Build open cv with nonfree modules and use SURF Features. They are awesome and can detect and seperate almost everything:</p>\n\n<p><a href=\"http://docs.opencv.org/master/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html\" rel=\"nofollow noreferrer\">http://docs.opencv.org/master/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html</a></p>\n\n<p>Here are some Code examples (sorry just C++ not python): <a href=\"http://docs.opencv.org/doc/user_guide/ug_features2d.html\" rel=\"nofollow noreferrer\">http://docs.opencv.org/doc/user_guide/ug_features2d.html</a></p>\n\n<p>This is the important code:</p>\n\n<pre><code>// detecting keypoints\nSurfFeatureDetector detector(400);\nvector&lt;KeyPoint&gt; keypoints1, keypoints2;\ndetector.detect(img1, keypoints1);\ndetector.detect(img2, keypoints2);\n\n// computing descriptors\nSurfDescriptorExtractor extractor;\nMat descriptors1, descriptors2;\nextractor.compute(img1, keypoints1, descriptors1);\nextractor.compute(img2, keypoints2, descriptors2);\n\n// matching descriptors\nBruteForceMatcher&lt;L2&lt;float&gt; &gt; matcher;\nvector&lt;DMatch&gt; matches;\nmatcher.match(descriptors1, descriptors2, matches);\n</code></pre>\n\n<p>If speed matter you can use \n    cv::FlannBasedMatcher matcher;</p>\n\n<pre><code>std::vector&lt; cv::DMatch &gt; matches;\nmatcher.match( descriptors_this, descriptors_givenImage, matches );\n</code></pre>\n\n<p>Where FLANN is just a very cool, very fast framework which uses a lot of methods like tree, bvhs and other stuff to speed things up a lot.</p>\n\n<p>Here are some answers if you need help with installing surf: <a href=\"https://stackoverflow.com/questions/9968993/opencv-undefined-reference-surffeaturedetector-and-bruteforcematcher\">OpenCV - undefined reference: SurfFeatureDetector and BruteForceMatcher</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Parallel scripts on MATLAB",
        "input": "",
        "output": "<p>I think the function in the Parallel Computing Toolbox you may be looking for is <a href=\"http://www.mathworks.com/help/distcomp/parfeval.html\" rel=\"nofollow\">parfeval</a>. It lets you spawn an asynchronous task, and get its result whenever it is ready.</p>\n",
        "system": ""
    },
    {
        "instruction": "CV - Extract differences between two images",
        "input": "",
        "output": "<p>One problem in your code is <code>cv::threshold</code> which only uses 1 channel images. Finding the pixelwise &quot;difference&quot; between two images in only grayscale often leads to unintuitive results.</p>\n<p>Since your provided images are a bit translated or the camera wasnt stationary, I've manipulated your background image to add some foreground:</p>\n<p>background image:</p>\n<p><img src=\"https://i.sstatic.net/2SLhj.png\" alt=\"enter image description here\" /></p>\n<p>foreground image:</p>\n<p><img src=\"https://i.sstatic.net/8RCek.png\" alt=\"enter image description here\" /></p>\n<p>code:</p>\n<pre class=\"lang-cpp prettyprint-override\"><code>    cv::Mat diffImage;\n    cv::absdiff(backgroundImage, currentImage, diffImage);\n\n    cv::Mat foregroundMask = cv::Mat::zeros(diffImage.rows, diffImage.cols, CV_8UC1);\n\n    float threshold = 30.0f;\n    float dist;\n\n    for(int j=0; j&lt;diffImage.rows; ++j)\n        for(int i=0; i&lt;diffImage.cols; ++i)\n        {\n            cv::Vec3b pix = diffImage.at&lt;cv::Vec3b&gt;(j,i);\n            \n            dist = (pix[0]*pix[0] + pix[1]*pix[1] + pix[2]*pix[2]);\n            dist = sqrt(dist);\n\n            if(dist&gt;threshold)\n            {\n                foregroundMask.at&lt;unsigned char&gt;(j,i) = 255;\n            }\n        }\n</code></pre>\n<p>giving this result:</p>\n<p><img src=\"https://i.sstatic.net/H6vXx.png\" alt=\"enter image description here\" /></p>\n<p>with this difference image:</p>\n<p><img src=\"https://i.sstatic.net/3xQJI.png\" alt=\"enter image description here\" /></p>\n<p>in general it is hard to compute a complete foreground/background segmentation from pixel-wise difference interpretations.</p>\n<p>You will probably have to add postprocessing stuff to get a real segmentation, where you start from your foreground mask. Not sure whether there are any stable universal solutions yet.</p>\n<p>As berak mentioned, in practice it won't be enough to use a single background image, so you will have to compute/manage your background image over time. There are plenty of papers covering this topic and afaik no stable universal solution yet.</p>\n<p>here are some more tests. I converted to <code>HSV</code> color space: <code>cv::cvtColor(backgroundImage, HSVbackgroundImagebg, CV_BGR2HSV); cv::cvtColor(currentImage, HSV_currentImage, CV_BGR2HSV);</code> and performed the same operations in this space, leading to this result:</p>\n<p><img src=\"https://i.sstatic.net/daLDv.png\" alt=\"enter image description here\" /></p>\n<p>after adding some noise to the input:</p>\n<p><img src=\"https://i.sstatic.net/iyNT1.png\" alt=\"enter image description here\" /></p>\n<p>I get this result:</p>\n<p><img src=\"https://i.sstatic.net/5cInC.png\" alt=\"enter image description here\" /></p>\n<p>so maybe the threshold is a bit too high. I still encourage you to have a look at HSV color space too, but you might have to reinterpret the &quot;difference image&quot; and rescale each channel to combine their difference values.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculating essential matrix using rotation, translation and camera parameters",
        "input": "",
        "output": "<p>You need to invert the camera matrices to get to normalized image coordinates, so it is:</p>\n\n<pre><code>E = inv(transpose(Kleft)) * R * Tx * inv(Kright)\n</code></pre>\n\n<p>where Tx is the matrix representation of the cross product by t:</p>\n\n<pre><code>Tx = [ 0    -t(3)  t(2)\n       t(3)  0    -t(1)\n      -t(2)  t(1)   0 ] \n</code></pre>\n\n<p>See this wikipedia <a href=\"https://en.wikipedia.org/wiki/Essential_matrix\" rel=\"nofollow\">article</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Undefined function &#39;imageSet&#39; in matlab",
        "input": "",
        "output": "<p><code>imageSet</code> was added in the R2014b release. It is an object that gives you an easy way to get the paths of all the image files in directory. Think of it as a container of paths to images. You can modify the example to work with an older version of MATLAB by using a cell array of image file names instead of <code>imageSet</code>. However, there is always a chance that you may run into other new functions or objects.</p>\n\n<p>There is also <a href=\"http://www.mathworks.com/help/releases/R2014a/vision/examples/video-mosaicking.html\" rel=\"nofollow\">a version of the image stitching example in the R2014a release</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to convert CImg float image type to unchar image(0-255) type",
        "input": "",
        "output": "<p>You just have to normalize your image values, like this :</p>\n\n<pre><code>CImg&lt;unsigned char&gt; img_normalized = img.get_normalize(0,255);\n</code></pre>\n\n<p>then work on the values of 'img_normalized' instead.</p>\n",
        "system": ""
    },
    {
        "instruction": "Probability map of an image",
        "input": "",
        "output": "<p>You could generate 6 maps for all classes. Or you could select 6 different colors, and use the probability to interpolate a new color, then you will get just one map. </p>\n",
        "system": ""
    },
    {
        "instruction": "How to get co-ordinates of a contour of a binary image?",
        "input": "",
        "output": "<p>You can get you contour mask by eroding your mask with a simple small structure element (disk/square)</p>\n\n<pre><code>con = BW &amp; ~imerode( BW, [0 1 0; 1 1 1; 0 1 0] ); \nfigure; imshow( con ); title('contour mask');\n[cy cx] = find(con); %// get coordinates of contour pixels\n</code></pre>\n\n<hr>\n\n<p>Coming to think about it, there are several (related) functions for tracing boundaries of binary masks in the image processing toolbox.</p>\n\n<p>Have you tried <a href=\"http://www.mathworks.com/help/images/ref/bwtraceboundary.html\" rel=\"nofollow\"><code>bwtraceboundary</code></a> or <a href=\"http://www.mathworks.com/help/images/ref/bwboundaries.html\" rel=\"nofollow\"><code>bwboundaries</code></a>?</p>\n",
        "system": ""
    },
    {
        "instruction": "Face detection MATLAB - slanted and up side down",
        "input": "",
        "output": "<p>I have done some experiments in the past, and I have learned that the <code>vision.CascadeObjectDetector</code> using the default frontal face model can tolerate about 15 degrees of in-plane rotation.  So I would advise rotating the image by 15 or even 10 degrees at a time, rather than 23.</p>\n\n<p>The problem with training your own detector in this case is the fact that the underlying features (Haar, LBP, and HOG) are not invariant to in-plane rotation. You would have to train multiple detectors, one for each orientation, every 15 degrees or so.</p>\n\n<p>Also, are you detecting faces in still images or in video?  If you are looking at a video, then you may want to try tracking the faces. This way, even if you miss a face because somebody's head is tilted, you'll have a chance to detect it later. And once you detect a face, you can track it even if it tilts. Take a look at this <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/47105-detect-and-track-multiple-faces\" rel=\"nofollow\">example</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Connected componenet labeling using BFS",
        "input": "",
        "output": "<p>BFS could be easily implemented in iterative way. Here is a sample in CPP:</p>\n\n<pre><code>void\nbfs(const vector&lt; vector&lt;int&gt; &gt; &amp;g)\n{\n        // visit self, then neighbors, store neighbors using stack.\n        vector&lt;bool&gt; visited(g.size(), false);\n        vector&lt;int&gt; s;\n        Queue Q;\n        Q.enqueue(6);\n        while (Q.size() &gt; 0)\n        {\n                int t = Q.dequeue();\n                if (!visited[t])\n                {\n                        cout &lt;&lt; \"visit node: \" &lt;&lt; t &lt;&lt; endl;\n                        visited[t] = true;\n                        for (int i = 0; i &lt; g[t].size(); ++i)\n                        {\n                                if (!visited[g[t][i]])\n                                {\n                                        Q.enqueue(g[t][i]);\n                                }\n                        }\n                }\n        }        \n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCv - Depth Map",
        "input": "",
        "output": "<p>Will work best IMO:\nUse a cheap laser line (around 20$ in a tools / hardware shop), and search the laser in the image. It the lines runs over the whole billet it is one piece. If there is a break in the line or a strong disortion there are 2 billets. </p>\n\n<p>My idea:</p>\n\n<pre><code>glm::vec3 laserColor(1,0,0); //red\nglm::vec3 currentColor;\nfloat maximum_color_distance = 0.1;\nfor(int a = 0; a &lt; image.rows;a++)\n{\n     for(int b = 0; b &lt; image.rows;b++)\n     { \n        currentColor = image.at(a,b);\n        float current_distance = glm::distance(laserColor, currentColor);\n        if(current_distance &gt; maximum_color_distance)\n        {\n           image.at(a,b) = 0;\n        }\n     }\n}\n\nglm::vec2 leftPixel = getMostLeftLaserPixel(image);\nglm::vec2 rightPixel = getMostRightLaserPixel(image);\n\n Line line = calculateLine (leftPixel,rightPixel);\n\nline.hasHoles(image, laserColor); //checks for 3x3 pxiel blocks which are on the line, returns false if all pixels in a 3x3 block have an other color then the laser. 3x3 to take small erros to serious\n</code></pre>\n\n<p>Maybe you can use a kinect in the right distance if the billets are far away enough. Stereo Images use most time block matching on an epipolar line. This means take 3x3 pixels, go from left to right, and try to find 3x3 pixels which look similar. This could be a problem by using metal. First you have repeating patters. Second you have reflection, which will be different on each camera, since they have a slightly different viewing angle.</p>\n\n<p>The kinect could have the same problems with the metal reflectness too, but not with repeating patterns: </p>\n\n<p>\"It depends on the material that is used to make the object. If the object is made out of metal, glass or it is shiny, the depth camera will face difficulty in capturing the image accurately.\"</p>\n\n<p>OpenCV provides the rectifyStereoIamge (or something called similar) function which helps a lot when you use 2 cameras. </p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Haar Classifier Killed",
        "input": "",
        "output": "<p>Reducing the <strong>width and height</strong> in the <code>opencv_traincascade</code> command made the program run correctly.</p>\n\n<p>Your <code>-w 80 -h 40</code> is probably too high for your PC. A recommanded size should be <code>-w 24 -h 24</code>.</p>\n\n<p><em>I noticed an high usage of hard drive while performing the command unless it's killed. Maybe our hard drives are not quick enough.</em></p>\n",
        "system": ""
    },
    {
        "instruction": "Split up a binary image using their white boundaries in MATLAB",
        "input": "",
        "output": "<p>If the blocks are always delimited by a completely vertical line, you can find where they are by comparing the original image (here transformed from RGB to grayscale to be a single plane) to a matrix that is made of repeats of the first row of the original image only.  Since the lines are vertical the intensity of the pixels in the first line will be the same throughout.  This generates a binary mask that can be used in conjunction with a quick thresholding to reject those lines that are all black pixels in every row.  Then invert this mask and use <code>regionprops</code> to locate the bounding box of each region.  Then you can pull these out and do what you like.</p>\n\n<p>If the lines dividing the blocks of text are not always vertical or constant intensity throughout then there's a bit more work that needs to be done to locate the dividing lines, but nothing that's impossible.  Some example data would be good to have in that case, though. </p>\n\n<pre><code>img = imread('http://puu.sh/cU3Nj/b020b60f0b.png');\n\nimshow(img);\n\nimgGray = rgb2gray(img);\nimgMatch = imgGray == repmat(imgGray(1,:), size(imgGray, 1), 1);\nwhiteLines = imgMatch &amp; (imgGray &gt; 0);\nboxes = regionprops(~whiteLines, 'BoundingBox');\n\nfor k = 1:6\n    subplot(3,2,k)\n    boxHere = round(boxes(k).BoundingBox);\n    imshow(img(boxHere(2):(boxHere(2)+boxHere(4)-1), boxHere(1):(boxHere(1)+boxHere(3)-1), :));\nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Fourier Angle Transformation of Picture, C++",
        "input": "",
        "output": "<p>You don't show us the image or its transform, so I'll assume the gradient lines are indeed clear enough. In that case you'd use the Hough transform. Output is a set of lines, each with an angle. </p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab formula optimization without for loops",
        "input": "",
        "output": "<p>Assuming <code>x</code> and <code>y</code> to be row vectors, you can use the following code to pre-calculate all <code>ro</code> values in a 2D matrix which hopefully should speed things up for you inside the nested loops for the rest of the work you might be doing involving the <code>ro</code> values -</p>\n\n<pre><code>theta_vec = [-pi/2:nBinsTheta:pi/2-nBinsTheta].'; %//'\nro_vals = round( cos(theta_vec)*x + sin(theta_vec)*y );\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to detect text region from a document image?",
        "input": "",
        "output": "<p>The usual pattern of object recognition will work here - threshold, detect regions, filter regions, then do what you need with the remaining regions.</p>\n\n<p>Thresholding is easy here.  The background is pure white (or can be filtered to be pure white) so anything that is above 0 in the inverted grayscale image is either text or an image.  Then regions can be detected within this thresholded binary image.</p>\n\n<p>For filtering the regions, we just have to identify what makes the text different from the pictures.  Text regions are going to be small since every letter is its own region.  Pictures are big regions in comparison.  Filtering by region area with the proper threshold will pull out all of the pictures and remove all of the text, assuming none of the pictures are about the size of a single letter anywhere on the page.  If they are then other filtering criteria can be used (saturation, hue variance, ...).</p>\n\n<p>Once the regions are filtered by the area and saturation criteria then a new image can be created by inserting the pixels in the original image that fall within the bounding boxes of the filtered regions into a new image. </p>\n\n<p>MATLAB implementation:</p>\n\n<pre><code>%%%%%%%%%%%%\n% Set these values depending on your input image\n\nimg = imread('https://www.mathworks.com/matlabcentral/answers/uploaded_files/21044/6ce011abjw1elr8moiof7j20jg0w9jyt.jpg');\n\nMinArea = 2000; % Minimum area to consider, in pixels\n%%%%%%%%%\n% End User inputs\n\ngsImg = 255 - rgb2gray(img); % convert to grayscale (and invert 'cause that's how I think)\nthreshImg = gsImg &gt; graythresh(gsImg)*max(gsImg(:)); % Threshold automatically\n\n% Detect regions, using the saturation in place of 'intensity'\nregs = regionprops(threshImg, 'BoundingBox', 'Area');\n\n% Process regions to conform to area and saturation thresholds\nregKeep = false(length(regs), 1);\nfor k = 1:length(regs)\n\n    regKeep(k) = (regs(k).Area &gt; MinArea);\n\nend\n\nregs(~regKeep) = []; % Delete those regions that don't pass qualifications for image\n\n% Make a new blank image to hold the passed regions\nnewImg = 255*ones(size(img), 'uint8');\n\nfor k = 1:length(regs)\n\n    boxHere = regs(k).BoundingBox; % Pull out bounding box for current region\n    boxHere([1 2]) = floor(boxHere([1 2])); % Round starting points down to next integer\n    boxHere([3 4]) = ceil(boxHere([3 4])); % Round ranges up to next integer\n    % Insert pixels within bounding box from original image into the new\n    % image\n    newImg(boxHere(2):(boxHere(2)+boxHere(4)), ...\n        boxHere(1):(boxHere(1)+boxHere(3)), :) = img(boxHere(2):(boxHere(2)+boxHere(4)), ...\n        boxHere(1):(boxHere(1)+boxHere(3)), :);\n\nend\n\n% Display\nfigure()\nimage(newImg);\n</code></pre>\n\n<p>As you can see in the image linked below, it does what is needed.  All but the pictures and the masthead are removed.  The good thing is that this will work just fine with colored and grayscale images if you're working with newspapers away from the front page.  </p>\n\n<p>Results:</p>\n\n<p><a href=\"https://i.sstatic.net/VtWjU.png\" rel=\"nofollow\">https://i.sstatic.net/VtWjU.png</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How to crop rectangle box in image in matlab code",
        "input": "",
        "output": "<p>That code only <strong>draws</strong> the rectangles in your image.  If you actually want to crop out portions of the image with the defined rectangles, use <a href=\"http://www.mathworks.com/help/images/ref/imcrop.html\" rel=\"nofollow\"><code>imcrop</code></a>.  </p>\n\n<p>As such, you would do something like this to store all of your cropped rectangles.  This is assuming that <code>im</code> and <code>dets</code> are already defined in your code from your function:</p>\n\n<pre><code>k = size(dets,1);\ncropped = cell(1,k);\n\nfor i=1:k\n   cropped{k} = imcrop(im, dets(i,:));\nend\n</code></pre>\n\n<p><code>cropped</code> would be a cell array where each element will store a cropped image defined by each rectangle within your <code>dets</code> array.  This is assuming that <code>dets</code> is a 2D array where there are 4 columns, and the number of rows determines how many rectangles you have.  Each row of <code>dets</code> should be structured like:</p>\n\n<pre><code>[xmin ymin width height] \n</code></pre>\n\n<p><code>xmin</code>, <code>ymin</code> are the horizontal and vertical co-ordinate of the top-left corner of the rectangle, and <code>width</code> and <code>height</code> are the width and height of the rectangle.</p>\n\n<p>If you want to access a cropped portion in the cell array, simply do:</p>\n\n<pre><code>crp = cropped{k};\n</code></pre>\n\n<p><code>k</code> would be the k<sup>th</sup> rectangle detected in your image.</p>\n",
        "system": ""
    },
    {
        "instruction": "Identifying moving objects using optical flow",
        "input": "",
        "output": "<p>I managed to find a method that more or less does what I want in OpenCV.</p>\n\n<p>After finding the sparse optical flow points between two consecutive images with GoodFeaturesToTrackDetector and PyrLKOpticalFlow (giving me prevPts and nextPts), I use findHomography with RANSAC to estimate the motion due to camera movement while excluding the outliers due to independently moving objects.  I then used perspectiveTransform to warp the prevPts to account for the camera motion (giving me warpedPts).  I can then compare the warpedPts to the nextPts in order to find moving objects.</p>\n\n<p>The end result is that even with the camera moving there is not much change between a point in warpedPts and nextPts if the object is stationary, while there is a significant change when the tracked points are on a moving object.  From there is is just a matter of grouping the moving points on the basis of proximity and similarity of movement.</p>\n",
        "system": ""
    },
    {
        "instruction": "Find a particular color in a 2D image",
        "input": "",
        "output": "<p>If you know the color you can just look for it:</p>\n\n<p>C++</p>\n\n<pre><code>glm::vec3 targetColor(1.0f,0,0);//(red,blue,green)\nfloat accepted_color_distance = 0.1f; \n/*how much must look the colors alike? distance 0 means the colors must be equal,\nbut there are numeric precission problems. Use 0.0001f or \nsomething instead of 0.*/\n\nbool markersFound[image.rows][image.cols]; //true means pixel y,x is a markers\n\nfor(unsigned int a = 0; a &lt; image.rows; a++) //init markersFound with false\n{\n     for(unsigned int b = 0; b &lt; image.cols; b++)\n     { \n         bool markersFound[a][b] = false;\n     }\n}     \n\nfor(unsigned int a = 0; a &lt; image.rows; a++)\n{\n     for(unsigned int b = 0; b &lt; image.rows; b++)\n     { \n       glm::vec3 currentColor = image.at(a,b);\n       float color_distance = glm::distance(currentColor, targetColor);\n\n       if(color_distance &lt; accepted_color_distance) //if a marker is found save it\n       {\n         bool markersFound[a][b] = true;\n       }\n      }\n }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "error detection on food packaging -using Open Cv",
        "input": "",
        "output": "<p>It's not a trivial task by any stretch of the imagination. Two images of the same identical object will always be different according to lightning conditions, perspective, shooting angle, etc.</p>\n\n<p>Basically you need to:\n1. Process the 2 images into \"digested\" data - dominant color, shapes, etcw\n2. Design and run your own similarity algorithm between the 2 objects</p>\n\n<p>You may want to look at Feature detectors in OpenCV: Surf, SIFT, etc.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding hamming distance between ORB feature descriptors",
        "input": "",
        "output": "<p>ORB descriptors are just 32 byte uchar Mat's.</p>\n\n<p>the bruteforce and flann matchers do some more work, than just comparing descriptors, but if that's all you want for now, it would be a straight <a href=\"http://docs.opencv.org/modules/core/doc/operations_on_arrays.html#double%20norm(InputArray%20src1,%20int%20normType,%20InputArray%20mask)\" rel=\"noreferrer\">norm</a>:</p>\n\n<pre><code>   Mat descriptor1, descriptor2;\n   double dist = norm( descriptor1, descriptor2, NORM_HAMMING);\n   // NORM_HAMMING2 or even NORM_L1 would make sense, too.\n   // dist is a double, but ofc. you'd only get integer values in this case.\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Make an weighted histogram using python?",
        "input": "",
        "output": "<p>I came across the same problem when I tried to build a weighted histogram of image's gradients phases: strong gradients should have proportionally more weight than weak gradients. I found out that NumPy had appropriate flag for <code>histogram</code> function:</p>\n\n<p><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html#numpy.histogram\" rel=\"nofollow noreferrer\">https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html#numpy.histogram</a></p>\n\n<p>So I solved my problem with the following code:</p>\n\n<pre><code>import cv2\nimport numpy as np\n\n# see tau manifest\nTAU = np.pi * 2\n\n# load image\n#..\n\n# calculate gradient\ngradX = cv2.Scharr(image, ddepth=cv2.CV_32F, dx=1, dy=0)\ngradY = cv2.Scharr(image, ddepth=cv2.CV_32F, dx=0, dy=1)\n\n# calculate magnitude and phase\nmagnitude = cv2.magnitude(gradX, gradY)\nphase = cv2.phase(gradX, gradY)\n\n# build weighted histogram of phase\nhist, bins = np.histogram(phase, bins=360, range=[0, TAU], weights=magnitude)\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Is it possible to combine HoG and AdaBoost algorithms for tracking?",
        "input": "",
        "output": "<p>AdaBoost is not exactly a face tracking algorithm - it's a method of increasing performance of any learning algorithms. HoG is a specific method of finding shapes and patterns on image (more exactly - describing those). You should read more on that topic:</p>\n\n<p><a href=\"https://www.cs.princeton.edu/~schapire/papers/explaining-adaboost.pdf\" rel=\"nofollow\">https://www.cs.princeton.edu/~schapire/papers/explaining-adaboost.pdf</a> - nice paper on AdaBoost (if you need something more quick and dirty - just look on wikipedia)</p>\n\n<p><a href=\"http://www.cvc.uab.es/~davidm/pdfs/IBPRIA2011.pdf\" rel=\"nofollow\">http://www.cvc.uab.es/~davidm/pdfs/IBPRIA2011.pdf</a> - again wiki could be of much help for you.</p>\n\n<p>There is a well-known technique of face detection which combines haar-like features recognition and Adaboost - see OpenCV library and CascadeClassifier class there (<a href=\"http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html\" rel=\"nofollow\">http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html</a>)</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Stereo Calibration Rotation Matrix",
        "input": "",
        "output": "<p>During rectification and undistortion the image can be translated and/or rotated depending on orientation of the stereo cameras.</p>\n\n<p>Obviously, if you will leave the original resolution for the transformed image - you will lose some information around the border. E.g. consider that there's no translation, but you have to rotate one of the images 45 degrees clockwise and use the same resolution - the corners will be \"blank\".</p>\n\n<p>The common solution is to find a largest \"inscribed\" rectangle and scale it up to the original resolution.</p>\n\n<p>You can use <code>initUndistortRectifyMap</code> for that instead of <code>undistortPoints</code>. </p>\n\n<p>For your <code>stereoRectify</code> call, the calls to the <code>initUndistortRectifyMap</code> will look as follows:</p>\n\n<ul>\n<li>left: <code>initUndistortRectifyMap(cameraMatrixLeft, distCoeffsLeft, R1, P1, cv::Size(sensor_width, sensor_height), CV_32FC1, map_left_1, map_left_2);</code></li>\n<li>right: <code>initUndistortRectifyMap(cameraMatrixRight, distCoeffsRight, R2, P2, cv::Size(sensor_width, sensor_height), CV_32FC1, map_right_1, map_right_2);</code></li>\n</ul>\n\n<p>This will get you the maps for a call to <code>remap</code>:</p>\n\n<ul>\n<li><code>remap(left, left_rectified, map_left_1, map_left_2, INTER_LINEAR, BORDER_CONSTANT);</code></li>\n<li><code>remap(right, right_rectified, map_right_1, map_right_2, INTER_LINEAR, BORDER_CONSTANT);</code></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "what is the best strategy to capture training images from a smartphone for buildings recognition",
        "input": "",
        "output": "<blockquote>\n  <p>How many pictures in average may i need for each interest building?</p>\n</blockquote>\n\n<p>The short answer is \"enough\". Buildings vary in size, shape, detail and whatnot. You should implement some sort of algorithm which detects and registers features and should feed it new data until you don't detect any new features and only old and already detected features, you may have a \"learned\" factor in percent, starting from 0% and all the way to 100% when N new images no longer detect any new feature. Then you may tune that to say 70% or whatever, depending on how much time and resources you want to dedicate to creating a feature profile for a given building.</p>\n\n<p>Then for some building it may take 5 images, for another building it may take 500 images. But you will have an algorithm which knows when it is enough.</p>\n",
        "system": ""
    },
    {
        "instruction": "When training a background subtractor, do the frames need to be in temporal sequence?",
        "input": "",
        "output": "<p>It depends on the history parameter and the frame count that you are going to use for training the model. If your frame count is much more than the history parameter (which is the case usually), then you need to set frames in temporal sequence. Or if you are going to train the background model with just a few frames, then I don't think the order is that important. But as I said, it is not the case most of the time and you need to set frames in temporal sequence.</p>\n",
        "system": ""
    },
    {
        "instruction": "FastCV and Windows Phone 8 (C#)",
        "input": "",
        "output": "<p>I came across over <strong><a href=\"http://fcvnet.codeplex.com/\" rel=\"nofollow\">this</a></strong> CodePlex project. It seems to be doing what you need. If it does not work, you could try the approach below.</p>\n\n<p>One problem is that C# is managed code, while FastCV library is probably native code.</p>\n\n<p>So you would need a wrapper that \"forwards\" the FastCV functions you intend to use. Native Code and Manged C++ interoperability is more easy to have as you can have managed code and native code even in the same file. See <strong><a href=\"http://msdn.microsoft.com/en-us/library/ms173185.aspx\" rel=\"nofollow\">here</a></strong> for more details.</p>\n\n<p>So you need to make a manged C++ library, load it as an assembly from your C# project and consume it from there. The C++ managed library will just call the native FastCV functions.</p>\n\n<blockquote>\n  <p>Are there others computer vision (object detection, text recognition\n  ..) for windows phone 8 - maybe with a better documentation?</p>\n</blockquote>\n\n<p>Yes. <strong>You can have OpenCV for Windows 8 / 8.1.</strong> Check <strong><a href=\"http://msopentech.com/blog/2014/08/20/opencv-support-for-windows-universal-apps-now-available-on-github/\" rel=\"nofollow\">here</a></strong> for more information. It's not guaranteed to be optimized better than FastCV but I have a feeling that it's much more complete than FastCV functions. I would go with OpenCV before FastCV.</p>\n",
        "system": ""
    },
    {
        "instruction": "Vectorizing disparityMap in Matlab",
        "input": "",
        "output": "<p>Three suggestions to try to speed things up:</p>\n\n<ul>\n<li>move the parfor to the outer loop to reduce the overhead of the parallel construct ;</li>\n<li>compute <code>im2.^2</code> once before the loop and save its  value in a temporary variable as it does not depend on the loop variables there is no need to compute it again and again, and actually</li>\n<li>move the whole computation of <code>arg1</code> out of the loops as it only depends on the size of <code>template</code> and not its value, and if I see correctly, the size is constant ;</li>\n<li>replace the <code>[xj]=find(arg==min(arg(:)));</code> construct with something along the lines of <code>[tmp, ind] = min(arg(:)) ; xj=ind2sub(size(arg), ind)</code> to avoid the call to <code>find</code> and rescan the matrix while the indices can be computed simply.</li>\n</ul>\n\n<p>Untested, but it should give you a start</p>\n\n<pre><code>arg1=conv2(im2.^2,ones([2*win1+1, 2*win2+1])/2,'same');\nparfor i=win1+1:1:bottom-win1\n    for j=win2+1:1:right-win2\n        %j=[win2+1:bottom-win2];\n        template=im1(i-win1:i+win1,j-win2:j+win2);\n        arg2=conv2(im2,rot90(template,2),'same');\n        arg=arg1-arg2;\n        [tmp, ind] = min(arg(:)) ; \n        xj=ind2sub(size(arg), ind);\n\n        disparityMap(i,j)=1-xj(1);\n\n    end\nend\n</code></pre>\n\n<p>Also make sure the number of workers is chosen appropriately, and try to compile the code to mex to see if there is improvement.</p>\n",
        "system": ""
    },
    {
        "instruction": "Navigation of maze with a group (cluster) of robots",
        "input": "",
        "output": "<p>A single robot in a maze is called a Braitenberg-vehicle. A group of such robots is a multi-robot-formation, which implies that the agents must coordinate their behavior. In the literature such games are called \u201cSignaling games\u201d, because a sender has private access to an event and must share this information with the group. For example, robot1 has detected a wall and sends the status update to another robot.</p>\n\n<p>In it's easiest form, signaling games are modeled with a lexicon. That is a list of possible messages between the robots. For example: 0=detectwall, 1=walkahead, 2=stop. As a reaction to a received signal, the robot can adapt his behavior and change his map of the maze. Sometimes this idea is called a distributed map building algorithm, because the information is only partial available.</p>\n\n<p>A basic example is the case if two robots are driving against each other (Chicken Game). They must communicate about their evasive strategy to prevent collision. If both robots have decided to drive in the same direction, they will collide as well. The problem is, that they don't know, what the other robot is planning.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to track people across multiple cameras?",
        "input": "",
        "output": "<p>1- blob extraction is not enough to track a specific object, for people case I suggest <a href=\"http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf\" rel=\"nofollow\">HoG</a> - or at least background subtraction before blob extraction, since all of the cameras have still scenes.</p>\n\n<p>2- opencv &lt;=2.4.9 uses pinhole model for stereo vision. so, before any calibration with opencv methods your fisheye images must be <a href=\"http://research.ijcaonline.org/volume51/number10/pxc3881483.pdf\" rel=\"nofollow\">converted to</a> rectilinear images first. You might try calibrating yourself using <a href=\"http://www.ee.oulu.fi/~jkannala/publications/tpami2006.pdf\" rel=\"nofollow\">other approaches</a> too</p>\n\n<p>release 3.0.0 will have support for <a href=\"http://docs.opencv.org/trunk/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#fisheye\" rel=\"nofollow\">fisheye model</a>. It is on alpha stage, you can still download and give it a try. </p>\n",
        "system": ""
    },
    {
        "instruction": "Debug Assertion Failed _pFirstBlock == pHead using Opencv",
        "input": "",
        "output": "<p>I just came across this issue and after serious web trawling, found the answer, at least it worked in my case...</p>\n\n<p>you need to go to your visual studio project settings, to c/c++, to code generation and change the runtime library to Multi-threaded Debug DLL (/MDd).</p>\n\n<p>It seems this error is from a single threaded DLL trying to run in a multi thread project, or something of that nature.</p>\n\n<p>good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Find if apple logo is present in target image",
        "input": "",
        "output": "<p>There are different possible ways. You can use:</p>\n\n<ul>\n<li><p>Surf(Speeded-Up Robust Features) detection</p></li>\n<li><p>Haar-like or LBP features ( &lt;- you need some test images)</p></li>\n</ul>\n\n<p>or take a look at this tutorial-> <a href=\"http://docs.opencv.org/doc/tutorials/features2d/feature_homography/feature_homography.html\" rel=\"nofollow\">Features2D+Homography</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Essential matrix estimation using Matlab and opencv",
        "input": "",
        "output": "<p>The <a href=\"http://docs.opencv.org/trunk/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findessentialmat\" rel=\"nofollow\">function</a> was contributed in <a href=\"https://github.com/Itseez/opencv/pull/266\" rel=\"nofollow\">this pull request</a> which was merged into the master branch (OpenCV 3.x) but <a href=\"http://answers.opencv.org/question/9590/how-to-enable-findessentialmat-in-opencv-249/\" rel=\"nofollow\">not part of</a> OpenCV 2.x</p>\n\n<p><code>mexopencv</code> targets the <a href=\"http://opencv.org/downloads.html\" rel=\"nofollow\">latest stable release</a>, which is as of this moment OpenCV v2.4.10, OpenCV 3.x is still alpha..</p>\n",
        "system": ""
    },
    {
        "instruction": "Running feature extraction on region within a boundary",
        "input": "",
        "output": "<p>Great, now I see the cow! :)</p>\n\n<p>You cannot specify an irregularly-shaped region of interest for the <code>detectSURFFeatures</code> function. However, you can detect the features in the whole image, and then create a binary mask of the region of interest, and use it to exclude keypoints, which are outside it. </p>\n\n<p><strong>Edit:</strong> If your boundary is represented as a polygon, you can use <code>roipoly</code> function to create a binary mask from it.</p>\n\n<p>Having said that, features that are outside your object's boundary can actually be useful, because they capture information about the shape of the object.</p>\n\n<p>Also, what is your final goal?  If you want to recognize individual cows, then local features may not be the best approach. You may do better with a global HOG descriptor (<code>extractHOGFeatures</code>) or with a color histogram, or both.</p>\n",
        "system": ""
    },
    {
        "instruction": "relationship between SIFT keypoint orientation and SIFT description orientation",
        "input": "",
        "output": "<p>I think this is just a matter of visualization used in VLfeat. As described <a href=\"http://www.vlfeat.org/overview/frame.html\" rel=\"nofollow noreferrer\">here</a>\n<a href=\"https://i.sstatic.net/vUUrV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vUUrV.jpg\" alt=\"standard\" /></a><br />\n<sub>(source: <a href=\"http://www.vlfeat.org/demo/frame_standard.jpg\" rel=\"nofollow noreferrer\">vlfeat.org</a>)</sub></p>\n<p>the &quot;standard oriented frame&quot; will be visualized as a circle with a radius pointing downwards.</p>\n<p>The same applies here. If you rotate the frame such that the radius points downwards, then the major gradient direction of the frame should be horizontal, which is agreed in most histograms inside the 4x4 squares.</p>\n<p>I think this convention makes sense, because the radius pointing downwards is <em>aligned</em> with the &quot;main strokes&quot; of the frame (which is visually intuitive), but <em>orthogonal</em> to the major gradient direction.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV detecting multiple objects in one window - not being drawn",
        "input": "",
        "output": "<p>your problem occurs during reading/interpreting the detected objects position:</p>\n\n<pre><code>Point center( firstObject[i].x + firstObject[i].width*0.5, firstObject[i].y + firstObject[i].height*0.5 );\n</code></pre>\n\n<p>while this works fine, you've made a small mistake in the second and third objects interpretation:</p>\n\n<pre><code>Point center( secondObject[k].x + secondObject[k].x + secondObject[k].width*0.5, secondObject[k].y + secondObject[k].y + secondObject[k].height*0.5 );\n</code></pre>\n\n<p>where <strong>secondObject[k].x + secondObject[k].x</strong> might be \"moving\" the detected object out of your image and doesnt make much sense if you want to display the object at the location where it was detected.</p>\n\n<p>If you replace the the line by</p>\n\n<pre><code>Point center( secondObject[k].x + secondObject[k].width*0.5, secondObject[k].y + secondObject[k].y + secondObject[k].height*0.5 );\n</code></pre>\n\n<p>and (for third detector):</p>\n\n<pre><code>Point center( thirdObject[j].x + thirdObject[j].width*0.5, thirdObject[j].y + thirdObject[j].y + thirdObject[j].height*0.5 );\n</code></pre>\n\n<p>everything should be fine.</p>\n",
        "system": ""
    },
    {
        "instruction": "Is &#39;Digit Classification Using HOG Features&#39; matlab example only available in the 2014 version?",
        "input": "",
        "output": "<p>Usually I would consult the <a href=\"http://www.mathworks.com/help/relnotes/index.html\" rel=\"nofollow\"><strong>release notes</strong></a> of the respective toolboxes to find out when a new function was introduced.</p>\n\n<p>For the <a href=\"http://www.mathworks.com/help/vision/examples/digit-classification-using-hog-features.html\" rel=\"nofollow\">digit classification example</a> mentioned, it uses <code>imageSet</code> <a href=\"http://www.mathworks.com/help/vision/release-notes.html#buf_zw7-1\" rel=\"nofollow\">a new feature in R2014b</a>, as well as the <code>extractHOGFeatures</code> function <a href=\"http://www.mathworks.com/help/vision/release-notes.html#btz19jb-1\" rel=\"nofollow\">introduced in R2013b</a>. It also uses <code>fitcecoc</code> from the Statistics toolbox. This <a href=\"http://www.mathworks.com/help/stats/release-notes.html#budsyit-2\" rel=\"nofollow\">a new function in R2014b</a>.</p>\n\n<p>It would be nice if the documentation provided this information in easier way...</p>\n",
        "system": ""
    },
    {
        "instruction": "how to automatically detect SURF feature of ROI of image(frame) in MATLAB?",
        "input": "",
        "output": "<p>Simply crop the image and then use the SURF detector over the cropped image. :-)</p>\n\n<p>In case you need help with cropping, </p>\n\n<p>I_cropped=I_original(200:400,400:700,:);</p>\n\n<p>Now, use the SURF detector on I_cropped.</p>\n",
        "system": ""
    },
    {
        "instruction": "Losing data when converting from UIImage to Mat",
        "input": "",
        "output": "<p>An image pulled from a video is reconstructed in a manner that depends on the video's encoding and compression codec. Short version is that unless you happen to pick out a keyframe (which you generally won't have an api to do, so trying to do this isn't viable), you're getting a image that is reconstructed from the video. </p>\n\n<p>So a image taken from the  video from same time that you took a straight-up image from the camera (assuming you could do both at the same time) would be different. After any sort processing you'll, of course, get different result. </p>\n\n<p>Before applying your threshold, get the raw image for before approaches. Look at them (or a delta of them) and you'll see that just aren't the same image. The second approach will likely have introduced artifacts from being encoded into video from multiple frames, then reconstructed into a single frame image. </p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting irregular lines/polyline in matlab",
        "input": "",
        "output": "<p>for this scenario</p>\n\n<p>after simple binary thresholding, skeletonize the image, label line regions, create straight lines using their end points (an end point of a skeleton has only 1 neighbour)</p>\n\n<p>find max distance from the skeleton to this line, if it is higher than a certain threshold (say, line length/dmax &lt; 20), then that skeleton is an irregular line</p>\n\n<p><a href=\"http://opencv-code.com/quick-tips/implementation-of-guo-hall-thinning-algorithm/\" rel=\"nofollow noreferrer\">http://opencv-code.com/quick-tips/implementation-of-guo-hall-thinning-algorithm/</a></p>\n\n<p><img src=\"https://i.sstatic.net/jwGBl.jpg\" alt=\"dmax\"></p>\n\n<p>for paper map scenario</p>\n\n<p>i guess you want to find specific roads or borders. at least they have different colors and width, you may want to smooth aggressively and apply color threshold after. still you need to work with a thousand different maps to find a generic algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "Indetify people in images",
        "input": "",
        "output": "<p>The most straightforward package is <a href=\"http://scikit-learn.org/stable/datasets/labeled_faces.html\" rel=\"nofollow\">http://scikit-learn.org/stable/datasets/labeled_faces.html</a>. This however is just a sample and you will likely have to extensively train your algorithms to get the desired results.</p>\n",
        "system": ""
    },
    {
        "instruction": "matlab: How to get textured using triangulation points in 3d reconstruction",
        "input": "",
        "output": "<p>Are your cameras calibrated?  If so, then you can rectify the images, and get a dense reconstruction. You can then plot the points using the colors from the RGB image.  See this <a href=\"http://www.mathworks.com/help/vision/examples/stereo-calibration-and-scene-reconstruction.html?refresh=true\" rel=\"nofollow\">example</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "2D Image Convolution - Central Difference on X-gradient",
        "input": "",
        "output": "<p>I think <a href=\"https://en.wikipedia.org/wiki/Sobel_operator\" rel=\"nofollow\">Sobel and Scharr operators</a> and its explanation will clarify this subject to you. If this link will be deleted just google for \"Sobel operator\" - it is well known and described everywhere in the Internet. In two words: Sobel dx operator is combination of blur in oy-direction and simple central ox-difference. Blur performed for reducing artifacts associated with a pure central differences operator. Scharr operator is better optimized for rotational symmetry and \"result from an optimization minimizing weighted mean squared angular error in Fourier domain\" It's theory is a bit more complicated so you'd better visit wiki or read <a href=\"http://archiv.ub.uni-heidelberg.de/volltextserver/962/\" rel=\"nofollow\">Scharr paper</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Non-Maximum Suppression on Detected Windows MATLAB",
        "input": "",
        "output": "<p>Actually for non-maximum suppression you don't need the score associated to each bounding box. You can use the well-known NMS method of Viola and Jones (Boosted cascade of simple features):</p>\n\n<ul>\n<li>cluster all bounding box that have overlap with each other greater than 0.5</li>\n<li>for each cluster calculate the mean bounding box and output it (that is calculate the mean point between all top right corners and all bottom-right corners)</li>\n</ul>\n\n<p>And you have non-maximum suppression.</p>\n\n<p>If you still want to use other routines that require output scores, then just assign to each bounding box the same score.</p>\n",
        "system": ""
    },
    {
        "instruction": "opencv rectangle with dotted or dashed lines",
        "input": "",
        "output": "<pre><code>import cv2\nimport numpy as np\ndef drawline(img,pt1,pt2,color,thickness=1,style='dotted',gap=20):\n    dist =((pt1[0]-pt2[0])**2+(pt1[1]-pt2[1])**2)**.5\n    pts= []\n    for i in  np.arange(0,dist,gap):\n        r=i/dist\n        x=int((pt1[0]*(1-r)+pt2[0]*r)+.5)\n        y=int((pt1[1]*(1-r)+pt2[1]*r)+.5)\n        p = (x,y)\n        pts.append(p)\n\n    if style=='dotted':\n        for p in pts:\n            cv2.circle(img,p,thickness,color,-1)\n    else:\n        s=pts[0]\n        e=pts[0]\n        i=0\n        for p in pts:\n            s=e\n            e=p\n            if i%2==1:\n                cv2.line(img,s,e,color,thickness)\n            i+=1\n\ndef drawpoly(img,pts,color,thickness=1,style='dotted',):\n    s=pts[0]\n    e=pts[0]\n    pts.append(pts.pop(0))\n    for p in pts:\n        s=e\n        e=p\n        drawline(img,s,e,color,thickness,style)\n\ndef drawrect(img,pt1,pt2,color,thickness=1,style='dotted'):\n    pts = [pt1,(pt2[0],pt1[1]),pt2,(pt1[0],pt2[1])] \n    drawpoly(img,pts,color,thickness,style)\n\nim = np.zeros((800,800,3),dtype='uint8')\ns=(234,222)\ne=(500,700)\ndrawrect(im,s,e,(0,255,255),1,'dotted')\n\ncv2.imshow('im',im)\ncv2.waitKey()      \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "MATLAB: 3d reconstruction using eight point algorithm",
        "input": "",
        "output": "<p>From a quick look, it looks correct. Some notes are as follows:</p>\n\n<p>You normalized code in eightpoint() is no ideal.</p>\n\n<p>It is best done on the points involved. Each set of points will have its scaling matrix. That is:</p>\n\n<pre><code>[pts1_n, T1] = normalize_pts(pts1);\n[pts2_n, T2] = normalize-pts(pts2);\n\n% ... code\n% solution\nF = T2' * F * T\n</code></pre>\n\n<p>As a side note (for efficiency) you should do</p>\n\n<pre><code>[~,~,V] = svd(A, 0);\n</code></pre>\n\n<p>You also want to enforce the constraint that the fundamental matrix has rank-2. After you compute F, you can do:</p>\n\n<pre><code>[U,D,v] = svd(F);\nF = U * diag([D(1,1),D(2,2), 0]) * V';\n</code></pre>\n\n<p>In either case, normalization is not the only key to make the algorithm work. You'll want to wrap the estimation of the fundamental matrix in a robust estimation scheme like RANSAC.</p>\n\n<p>Estimation problems like this are very sensitive to non Gaussian noise and outliers. If you have a small number of wrong correspondence, or points with high error, the algorithm will break. </p>\n\n<p>Finally, In 'triangulate' you want to make sure that the points are not at infinity prior to the homogeneous division.</p>\n\n<p>I'd recommend testing the code with 'synthetic' data. That is, generate your own camera matrices and correspondences. Feed them to the estimate routine with varying levels of noise. With zero noise, you should get an exact solution up to floating point accuracy. As you increase the noise, your estimation error increases.</p>\n\n<p>In its current form, running this on real data will probably not do well unless you 'robustify' the algorithm with RANSAC, or some other robust estimator.</p>\n\n<p>Good luck.</p>\n\n<p>Good luck.</p>\n",
        "system": ""
    },
    {
        "instruction": "Language support for Point cloud library API",
        "input": "",
        "output": "<p>compile PCL from source is not easy, but the PCL version for<a href=\"http://www.pointclouds.org/downloads/windows.html\" rel=\"nofollow\">\"All-in-one installers (PCL + dependencies)\"</a> is a little old(only for VS2008, VS2010).\nHere's is the newest PCL1.7.2 for VS2013<a href=\"http://yunpan.cn/cJhEE4eZa5JXv\" rel=\"nofollow\">\"All-in-one installers (PCL + dependencies)\"</a> download(password:706c)</p>\n",
        "system": ""
    },
    {
        "instruction": "Read multiple .mat files and store them in a cell",
        "input": "",
        "output": "<pre><code>ImageCell = {};\nfor i = 270:273\nfilename = strcat('int_f0',int2str(i),'.mat');\nload(filename);\nImageCell{i-269} = C;\nclear C\nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "MATLAB: Segment individual letters from a binary image",
        "input": "",
        "output": "<p>If you want to extract every individual character in your image, you can very easily do that with <a href=\"http://www.mathworks.com/help/images/ref/regionprops.html\" rel=\"noreferrer\"><code>regionprops</code></a>.  Simply use the <code>BoundingBox</code> attribute to extract the bounding box surrounding each character.  After you do this, we can place each character in a <code>cell</code> array for further process.  If you want to store this into a <code>26 x N</code> array, you would need to recognize what each letter was first so that you can choose the slot that the letter is supposed to go in for the first dimension.  Because you want to segment out the characters first, we will focus on that.  As such, let's load in the image into MATLAB.  Note that the original image was in GIF and when I loaded it on my computer... it looked pretty messed up.  I've resaved the image into PNG and it's shown below:</p>\n\n<p><img src=\"https://i.sstatic.net/q7cnA.png\" alt=\"enter image description here\"></p>\n\n<p>Let's read this into MATLAB:</p>\n\n<pre><code>im = imread('https://i.sstatic.net/q7cnA.png');\n</code></pre>\n\n<p>Now, you may notice that there are some discontinuities between some letters.  What we can do is perform a morphological opening to close these gaps.  However, we aren't going to use this image to extract what the actual characters are.  We are only using these to get the bounding boxes for the letters:</p>\n\n<pre><code>se = strel('square', 7);\nim_close = imclose(im, se);\n</code></pre>\n\n<p>Now, you'd call <code>regionprops</code> like this to find all of the bounding boxes in the image (after applying morphology):</p>\n\n<pre><code>s = regionprops(im_close, 'BoundingBox');\n</code></pre>\n\n<p>What is returned in <code>s</code> is a structure where each element in this structure contains a bounding box that encapsulates an object detected in the image.  In our case, this is a single character.  The <code>BoundingBox</code> property for each object is a 4 element array that is formatted like so:</p>\n\n<pre><code>[x y w h]\n</code></pre>\n\n<p><code>(x,y)</code> are the column and row co-ordinates of the upper left corner of the bounding box and <code>w</code> and <code>h</code> are the width and height of the bounding box.  What we will do next is create a 4 column matrix that encapsulates all of these bounding box properties together, where each row denotes a single bounding box:</p>\n\n<pre><code>bb = round(reshape([s.BoundingBox], 4, []).');\n</code></pre>\n\n<p>It's necessary to round the values because if you want to extract the letters from the image, we have to do this in integer co-ordinates as that is how the image is naturally defined.  If you want a good illustration of these bounding boxes, this code below will draw a red box around each character we have detected:</p>\n\n<pre><code>imshow(im);\nfor idx = 1 : numel(s)\n    rectangle('Position', bb(idx,:), 'edgecolor', 'red');\nend\n</code></pre>\n\n<p>This is what we get:</p>\n\n<p><img src=\"https://i.sstatic.net/8tnPD.png\" alt=\"enter image description here\"></p>\n\n<p>The final job is to extract all of the characters and place them into a <code>cell</code> array.  I'm using a <code>cell</code> array because the character sizes are uneven, so putting this into a <code>cell</code> array will accommodate for the different sizes.  As such, simply loop over every bounding box we have, then extract the bounding box of pixels to get each character and place it into a cell array.  Therefore:</p>\n\n<pre><code>chars = cell(1, numel(s));\nfor idx = 1 : numel(s)\n    chars{idx} = im(bb(idx,2):bb(idx,2)+bb(idx,4)-1, bb(idx,1):bb(idx,1)+bb(idx,3)-1);\nend\n</code></pre>\n\n<p>If you want a character, simply do <code>ch = chars{idx};</code> where <code>idx</code> is any number from 1 to as many characters as we have.  You can also see what this character looks like by doing <code>imshow(ch);</code></p>\n\n<p>This should hopefully give you enough to get started.  Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Android: Broadcast Image Continuously from Service",
        "input": "",
        "output": "<p>Instead of broadcast, you may use AIDL interface, once there is data available from camera, you call a interface of VisionService from cameraService, broadcast is not suitable for such job becasue you need to pass large amount of data</p>\n",
        "system": ""
    },
    {
        "instruction": "How to detect text region in image?",
        "input": "",
        "output": "<p>You can use <a href=\"http://www.math.tau.ac.il/~turkel/imagepapers/text_detection.pdf\" rel=\"nofollow noreferrer\">Stroke Width Transform (SWT)</a> to highlight text regions.\nUsing my mex implementation posted <a href=\"https://stackoverflow.com/a/19971599/1714410\">here</a>, you can</p>\n\n<pre><code>img = imread('https://i.sstatic.net/Eyepc.jpg');\n[swt swtcc] = SWT( img, 0, 10 );\n</code></pre>\n\n<p>Playing with internal parameters of the edge-map extraction and image filtering in <code>SWT.m</code> can help you tweak the resulting mask to your needs.</p>\n\n<p>To get this result:\n<img src=\"https://i.sstatic.net/PU2BQ.png\" alt=\"enter image description here\">\nI used these parameters for the edge map computation in <code>SWT.m</code>:</p>\n\n<pre><code>edgeMap = single( edge( img, 'canny', [0.05 0.25] ) );\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Out of memory error in trainCascadeObjectDetector in MATLAB",
        "input": "",
        "output": "<p>Short-term solution: try using LBP or HOG features instead of Haar. Haar features require much more memory for training than the others.</p>\n\n<p>Long-term solution: use a 64-bit machine with a 64-bit operating system. Training a good object detector requires a large amount of training data, which in turn requires a large amount of memory.</p>\n",
        "system": ""
    },
    {
        "instruction": "Ellipse / Circle Detection on Blastomere",
        "input": "",
        "output": "<p>May be this idea will help:</p>\n\n<ul>\n<li>get edges of assumed size (as I can cell edges thickness is more than 1 pixel, I am not sure adaptive threshold is the best solution in this case) using appropriate edge detector parameters,</li>\n<li>reduce noise using morphology operations,</li>\n<li>put one circle in every place and calculate warp-type-distance for this circle, use threshold for this distance to understand where edges seems like circle.</li>\n</ul>\n\n<p>Sure it is calculation-intensive because it is multi-dimensional search for function min and dimensions are:</p>\n\n<ul>\n<li>edge thickness T -- +1</li>\n<li>morphology parameter W (size of window) -- +1 (or it could be function of T)</li>\n<li>X and Y position of circle -- +2</li>\n<li>circle radius R -- +1</li>\n</ul>\n\n<p>So it is warp-distance-function minimum search in 4 or 5-dimensional parallelepiped, where only X and Y position gives ~50*50 = 2500 variants (T may give just 5 variants [1;4], W in [2;5] too, R can have exp steps like 100, 110, 121... etc), and wrap-distance calculation usually is not very fast. But at least this could be parallelized very good and also could be accelerated by GPU. I think it could be done in minutes, not hours. For small images even in seconds.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to perform faster updation of data in my image using matlab?",
        "input": "",
        "output": "<h3>Problem:</h3>\n\n<p>Basically, all frames are added to your figure one on top of the other due to the <code>hold on</code> state your axes handle is in. This causes memory buildup and slows you down.</p>\n\n<h3>Solution:</h3>\n\n<p>You should turn off <code>hold</code> after drawing the lines, so the <code>imshow</code> of the next frame will discard the previous frame.</p>\n\n<pre><code>imshow(currentFrame);\nhold on;\nplot( theLinesThatWereDetected  );\nhold off; %// super critical!\ndrawnow; %// instead of pause\n</code></pre>\n\n<h3>Comments:</h3>\n\n<ol>\n<li>As pointed by <a href=\"https://stackoverflow.com/questions/26588301/how-to-perform-faster-updation-of-data-in-my-image-using-matlab/26589312#comment41793125_26588301\">Ander</a> it is better practice to use <a href=\"http://www.mathworks.com/help/matlab/ref/drawnow.html\" rel=\"nofollow noreferrer\"><code>drawnow</code></a> instead of <code>pause(0.001)</code>.</li>\n<li>Just changing <code>XData</code> and <code>YData</code> of your plot (as suggested by <a href=\"https://stackoverflow.com/questions/26588301/how-to-perform-faster-updation-of-data-in-my-image-using-matlab/26589312#comment41792916_26588301\">Benoit_11</a>) will not be enough, it does not solve the memory waste caused by \"holding\" all the frames in the figure.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Sampson error for five point essential matrix estimation",
        "input": "",
        "output": "<p>The Sampson distance is the first order approximation of geometric distance. It could be understood as follows:</p>\n\n<p>Given a Fundamental matrix F, and a pair of correspondence (x,x') such that x'Fx=e, what is the distance/error of this pair of correspondence? The geometric distance is defined for all correspondence (y,y') such that y'Fy=0, the minimum value of ||x-y||^2+||x'-y'||^2 (in other words, the closest correspondence pair to (x,x') that satisfies the F matrix exactly). And it can be shown that the Sampson error is a first approximation of this minimum distance.</p>\n\n<p>Intuitively, Sampson error can be roughly thought as the squared distance between a point x to the corresponding epipolar line x'F. And in this context, a threshold of 0.01 is way too small (you rarely find a fundamental matrix such that all correspondences are within 0.1 pixel accuracy). The suggested threshold would be somewhere between 1 to 10 (1~3 pixel error), depending on the size/resolution/quality of your image pairs.</p>\n",
        "system": ""
    },
    {
        "instruction": "Stereo image acquisition using bumblebee2",
        "input": "",
        "output": "<p>Matlab cannot read the BumbleBee 2 output directly. To do that you'll have to record the stream and process it offline. I wrote a proprietary recorder based on the code samples in the SDK. You can split the left/right images and record each one in a separate video container (e.g. using OpenCV to write a compressed avi file). Later, you can load these images into memory, and use Triclops to compute disparity maps (or alternatively, use OpenCV to run other algorithms, like semi-global block matching).</p>\n\n<p>Flycapture can capture image series or video clips, but you have less control over what you get. I suggest you use the code samples to write a simple recorder, and then load your output into Matlab in standard ways. Consult the Point Grey tech support.</p>\n",
        "system": ""
    },
    {
        "instruction": "Make a representative HSV image in matlab",
        "input": "",
        "output": "<p>Lets say you quantize the HSV space (0-1) into 256 bins. There will be 256*256*256 possible colors. We could fix a dimension (say saturation) and generate the matrix. Then there will be 256*256 colors.</p>\n\n<pre><code>[x1,x2]=meshgrid(linspace(0,1,256),linspace(0,1,256));\nimg(:,:,1)=x1;\nimg(:,:,2)=1;  %fully saturated colors\nimg(:,:,3)=x2;\n\nimgRGB=hsv2rgb(img); %for display purposes\nimshow(imgRGB,[])\n</code></pre>\n\n<p>It will look different in RGB (that's where you would visualize). It looks similar to your image if you visualize HSV matrix (i.e. without converting it to RGB, but MATLAB doesn't know that its HSV)</p>\n\n<pre><code>imshow(img,[]);\n</code></pre>\n\n<p>The second image you have posted can be obtained with:</p>\n\n<pre><code>[x1,x2]=meshgrid(linspace(0,1,256),linspace(0,1,256));\nimg(:,:,1)=x1;\nimg(:,:,2)=0;\nimg(:,:,3)=x2;\nimshow(img,[]) %visualizing HSV\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Detect number of global thresholds analyzing an image",
        "input": "",
        "output": "<p>I think it might help you if you can detect the modes of the histogram. You'll find mean-shift clustering useful for this purpose. I don't know if that's available in Matlab though. Once you find these modes and sort them, your threshold values should lie in-between two consecutive modes.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Keypoint matching DMatch distance variable",
        "input": "",
        "output": "<p>I have come up with some problem while I was working on real time object matching using SIFT feature detector. Here is my solution on video.</p>\n\n<p>First I have created a struct to store matched keypoints.The struct contains location of keypoint in templateImage,location of keypoint in inputImage and similarity measure.Here I have used cross correlation of vectors as a similarity measure.</p>\n\n<pre><code>struct MatchedPair\n    {\n        Point locationinTemplate;\n        Point matchedLocinImage;\n        float correlation;\n        MatchedPair(Point loc)\n        {\n            locationinTemplate=loc;\n        }\n    }\n</code></pre>\n\n<p>I will select sort the matched keypoints according to their similarity so I will need a helper function that will tell <code>std::sort()</code> to how to compare my <code>MatchedPair</code> objects.</p>\n\n<pre><code>bool comparator(MatchedPair a,MatchedPair b)\n{\n        return a.correlation&gt;b.correlation;\n}\n</code></pre>\n\n<p>Now the main code starts. I have used standard method to detect and descrypt features from both input image and templateImage.After computing features I have implemented my own matching function.<strong>This is the answer you are looking for</strong></p>\n\n<pre><code> int main()\n    {\n        Mat templateImage = imread(\"template.png\",IMREAD_GRAYSCALE); // read a template image\n        VideoCapture cap(\"input.mpeg\"); \n        Mat frame; \n\n        vector&lt;KeyPoint&gt; InputKeypts,TemplateKeypts; \n        SiftFeatureDetector detector;\n        SiftDescriptorExtractor extractor;\n        Mat InputDescriptor,templateDescriptor,result; \n        vector&lt;MatchedPair&gt; mpts; \n        Scalar s;\n        cap&gt;&gt;frame; \n        cvtColor(image,image,CV_BGR2GRAY);\n        Mat outputImage =Mat::zeros(templateImage.rows+frame.rows,templateImage.cols+frame.cols,CV_8UC1);\n        detector.detect(templateImage,TemplateKeypts); // detect template interesting points\n        extractor.compute(templateImage,TemplateKeypts,templateDescriptor); \n\n        while( true) \n        {\n            mpts.clear(); // clear for new frame\n            cap&gt;&gt;frame;  // read video to frame\n            outputImage=Mat::zeros(templateImage.rows+frame.rows,templateImage.cols+frame.cols,CV_8UC1); // create output image \n            cvtColor(frame,frame,CV_BGR2GRAY);\n            detector.detect(frame,InputKeypts);\n            extractor.compute(frame,InputKeypts,InputDescriptor); // detect and descrypt frames features\n\n            /*\n                So far we have computed descriptors for template and current frame using traditional methods\n                From now onward we are going to implement our own match method\n\n     - Descriptor matrixes are by default have 128 colums to hold features of a keypoint.    \n     - Each row in descriptor matrix represent 128 feature of a keypoint.\n\n Match methods are using this descriptor matrixes to calculate similarity.\n\nMy approach to calculate similarity is using cross correlation of keypoints descriptor vector.Check code below to see how I achieved.\n        */\n\n   // Iterate over rows of templateDesciptor ( for each keypoint extracted from     //  template Image)   i keypoints in template,j keypoints in input\n            for ( int i=0;i&lt;templateDescriptor.rows;i++)\n            {\n                mpts.push_back(MatchedPair(TemplateKeypts[i].pt));\n                mpts[i].correlation =0;\n                for ( int j=0;j&lt;InputDescriptor.rows;j++)\n                {\n                    matchTemplate(templateDescriptor.row(i),InputDescriptor.row(j),result,CV_TM_CCOR_NORMED);\n // I have used opencvs built function to calculate correlation.I am calculating // row(i) of templateDescriptor with row(j) of inputImageDescriptor.\n                    s=sum(result); // sum is correlation of two rows\n// Here I am looking for the most similar row in input image.Storing the correlation of best match and matchLocation in input image.\n                    if(s.val[0]&gt;mpts[i].correlation)\n                    {\n                       mpts[i].correlation=s.val[0];\n                       mpts[i].matchedLocinImage=InputKeypts[j].pt;\n                    }\n                }\n\n            }\n\n// I would like to show template,input and matching lines in one output.            templateImage.copyTo(outputImage(Rect(0,0,templateImage.cols,templateImage.rows)));\n            frame.copyTo(outputImage(Rect(templateImage.cols,templateImage.rows,frame.cols,frame.rows)));\n\n  // Here is the matching part. I have selected 4 best matches and draw lines        \u00a0// between them. You should check for correlation value again because there can // be 0 correlated match pairs.\n\n            std::sort(mpts.begin(),mpts.end(),comparator);\n            for( int i=0;i&lt;4;i++)\n            {\n\n                if ( mpts[i].correlation&gt;0.90)\n                {\n// During drawing line take into account offset of locations.I have added \n// template image to upper left of input image in output image.  \n            cv::line(outputImage,mpts[i].locationinTemplate,mpts[i].matchedLocinImage+Point(templateImage.cols,templateImage.rows),Scalar::all(255));\n                }\n            }\n            imshow(\"Output\",outputImage);\n            waitKey(33);\n        }\n\n    }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Making image brightened in python using putdata",
        "input": "",
        "output": "<pre><code>new_pixel= [max(min(channel, 255), 0) for channel in new_pixel]\nnew_image_list.append(new_pixel)\n</code></pre>\n\n<p>You're appending lists to <code>new_image_list</code>, but you should be appending tuples.</p>\n\n<pre><code>new_pixel= [max(min(channel, 255), 0) for channel in new_pixel]\nnew_image_list.append(tuple(new_pixel))\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV OCL Error: GPU API call (-1001) in initializeOpenCLDevices",
        "input": "",
        "output": "<p>The Intel OpenCL runtime is not available from the Ubuntu package manager; you need to download and install it from their website:</p>\n\n<p><a href=\"https://software.intel.com/en-us/articles/opencl-drivers#philinux\" rel=\"nofollow\">https://software.intel.com/en-us/articles/opencl-drivers#philinux</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Why do we need to move the calibration object for pinhole camera calibration?",
        "input": "",
        "output": "<p>I assume you are asking in the context of OpenCV-like camera calibration using images of a planar target. The reference for the algorithm used by OpenCV is <a href=\"http://research.microsoft.com/en-us/um/people/zhang/Papers/TR98-71.pdf\" rel=\"nofollow\">Z. Zhang's now classic paper</a> . The discussion in the top half of page 6 shows that n >= 3 images are necessary for calibrating all 5 parameters of a pinhole camera matrix. Imposing constraints on the parameters reduces the number of needed images to a theoretical minimum of one.</p>\n\n<p>In practice you need more for various reasons, among them:</p>\n\n<ul>\n<li>The need to have enough measurements to overcome \"noise\" and \"random\" corner detection errors, while using a practical target with well-separated corners. </li>\n<li>The difference between measuring data and observing (constraining) model parameters.</li>\n<li>Practical limitations of physical lenses, e.g. depth of field. </li>\n</ul>\n\n<p>As an example for the second point, the ideal target pose for calibrating the nonlinear lens distortion (barrel, pincushion, tangential, etc.) is frontal-facing, covering the whole field of view, because it produces a large number of well-separated and aligned corners over the image, all with approximately the same degree of blur. However, this is exactly the worst pose you can use in order to estimate the field of view / focal length, as for that purpose you need to observe significant perspective foreshortening.</p>\n\n<p>Likewise, it is possible to show that the location of the principal point is well constrained by a set of images showing the vanishing points of multiple pencils of parallel lines. This is important because that location is inherently confused by the component parallel to the image plane of the relative motion between camera and target. Thus the vanishing points help \"guide\" the optimizer's solution toward the correct one, in the common case where the target does translate w.r.t the camera.</p>\n",
        "system": ""
    },
    {
        "instruction": "HoughLinesP with slightly distorted lines in OpenCV",
        "input": "",
        "output": "<p>Did you take a look at Line Segment Detector ? \n <a href=\"http://www.ipol.im/pub/art/2012/gjmr-lsd/\" rel=\"nofollow\">http://www.ipol.im/pub/art/2012/gjmr-lsd/</a></p>\n",
        "system": ""
    },
    {
        "instruction": "optimizing harris corner detector",
        "input": "",
        "output": "<p>The direct combination of kernels to use convolution is not possible. The Ixx = Ix^2, Iyy = Iy^2 and Ixy = Ix*Iy is NOT found by convolution (it is not linear). </p>\n\n<p>Some tricks for optimization can be found in \"LOCOCO: LOW COMPLEXITY CORNER DETECTOR\"</p>\n",
        "system": ""
    },
    {
        "instruction": "how to disable the need of sqsum when calling opencv&#39;s cv::integral() function?",
        "input": "",
        "output": "<p>Could you detail more precisely what didn't work when you set the 3d parameter as NULL ?</p>\n\n<p>I debugged the code using OpenCV 2.4.2 and when the 3rd param is set to NULL _sqsum.needed() returns false which prevents the computation of the sqsum.</p>\n",
        "system": ""
    },
    {
        "instruction": "Find outer checkerboard corners",
        "input": "",
        "output": "<p>In general, if all you have is the detection of some, but not all, the inner corners, the problem cannot be solved. This is because the configuration is invariant to translation - shifting the physical checkerboard by whole squares would produce the same detected corner position on the image, but due to <em>different</em> physical corners. </p>\n\n<p>Further, the configuration is also invariant to rotations by 180 deg in the checkerboard plane and, unless you are careful to distinguish between the colors of the squares adjacent each corner, to rotations by 90 deg and reflections with respect the center and the midlines. </p>\n\n<p>This means that, in addition to detecting the corners, you need to extract from the image some features of the physical checkerboard that can be used to break the above invariance. The simplest break is to detect all 9 corners of one row and one column, or at least their end-corners. They can be used directly to rectify the image by imposing the condition that their lines be at 90 deg angle. However, this may turn out to be impossible due to occlusions or detector failure, and more sophisticated methods may be necessary.</p>\n\n<p>For example, you can try to directly detect the chessboard edges, i.e. the fat black lines at the boundary. One way to do that, for example, would be to detect the letters and numbers nearby, and use those locations to constrain a line detector to nearby areas.</p>\n\n<p>By the way, if the photo you posted is just a red herring, and you are interested in detecting general checkerboard-like patterns, and can control the kind of pattern, there are way more robust methods of doing it. My personal favorite is the <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.4689&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">\"known 2D crossratios\" pattern of Matsunaga and Kanatani.</a> </p>\n",
        "system": ""
    },
    {
        "instruction": "Compute the combined image of SVD perturbations",
        "input": "",
        "output": "<p>This method originated from <a href=\"http://jacs.usv.ro/getpdf.php?paperid=12_8\" rel=\"nofollow noreferrer\">this paper that can be accessed here</a>.  Let's answer your questions in order.</p>\n\n<ol>\n<li><p>If you want to know why this step is useful, you need to know a bit of theory about how the SVD works.  The SVD stands for <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\" rel=\"nofollow noreferrer\">Singular Value Decomposition</a>.  What you are doing with the SVD is that it is transforming your N-dimensional data in such a way where it orders it according to which dimension exhibits the most amount of variation, and the other dimensions are ordered by this variation in decreasing order (SVD experts and math purists... don't shoot me.  This is how I understand the SVD to be).  The singular values in this particular context give you a weighting of how much each dimension of your data contributes to in its overall decomposition.  </p>\n\n<p>Therefore, by applying that particular step (<code>P = U * power(S, i) * V';)</code>, you are giving more emphasis to the \"variation\" in your data so that the most important features in your image will stand out while the unimportant ones will \"fade\" away.  This is really the only rationale that I can see behind why they're doing this.</p></li>\n<li><p>The \"particular\" values are the singular values.  These values are part of the <code>S</code> matrix and those values appear in the diagonals of the matrix.</p></li>\n<li><p>I wouldn't call <code>P</code> the derived image, but an image that locates which parts of the image are more important in comparison to the rest of the image.  By mixing this with the original image, those features that you should concentrate on are more emphasized while the other parts of the image that most people wouldn't pay attention to, the get de-emphasized in the overall result.</p></li>\n</ol>\n\n<hr>\n\n<p>I would recommend you read that paper that you got this algorithm from as it explains the whole process fairly well.</p>\n\n<h1>Some more references for you</h1>\n\n<p>Take a look at this <a href=\"http://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf\" rel=\"nofollow noreferrer\">great tutorial on the SVD here</a>.  Also, <a href=\"https://stackoverflow.com/questions/21302570/image-enhancement-using-combination-between-svd-and-wavelet-transform\">this post</a> may answer more questions regarding the insight of the algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect and count objects in an image",
        "input": "",
        "output": "<p>There many techniques that you can try. But if you really want to make something quick and easy, try to manipulate your image set such that each category has clearly distinctive and unique colours. Thus you can make some decisions of object presence by looking for its colour, or even better count number of contours of specific colour.</p>\n\n<p>The pseudo-code:</p>\n\n<ol>\n<li>pre-process image -- (normalize,threshold only targeted colors by using backprojections function) </li>\n<li>findContours</li>\n<li>for each contour\n<ol>\n<li>check if it is valid \"feature\" contour -- you can later further enhance your program by filtering some noisy regions</li>\n<li>estimate most probable category it belongs to.</li>\n</ol></li>\n</ol>\n\n<p>This should also give you pretty good performance speed. Otherwise you have to make some assumptions on icons' scale, rotation, occlusion/overlapping situations.</p>\n",
        "system": ""
    },
    {
        "instruction": "why pretraining for convolutional neural networks",
        "input": "",
        "output": "<ol>\n<li><p>Pretraining is a regularization technique. It improves generalization accuracy of your model. Since the network is exposed to large amount of data (we have vast amount of unsupervised data in many taks), weight parameters are carried to a space that is more likely to represent the data distribution in overall rather than overfitting a specific subset of underlying data distribution. Neural nets, especially those with high model representation capacity with tons of hidden units, are tend to overfit your data, and vulnerable to random parameter initializations. Also, as initial layers are properly initialized in supervised way, gradient dilution problem is not that severe anymore. This is why pretraining is used as an initial step to supervised task which is generally carried with gradient descent algorithm. </p></li>\n<li><p>CNNs share the same fate with other Neural Nets. There are too many parameters to tune; optimal input patch size, number of hidden layers, number of feature maps per layer, pooling and stride sizes, normalization windows, learning rate and others. Thus, the problem of model selection is relatively harder compared to other ML techniques. Training of large networks are either carried on GPUs or cluster of CPUs. </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Find white region of eye",
        "input": "",
        "output": "<p>I am not sure but you could try to use the following strategy. I have used a similar kind of strategy before to find out something else.</p>\n\n<ol>\n<li>Find out the center of the eyes.</li>\n<li>blur your image and apply canny edge detector.</li>\n<li>Find out the contours in your images obtained through step 2 (Edge image).</li>\n<li>Check the % of white color in each and every contour by calculating a simple histogram.</li>\n<li>Keep only those contours where the % of white color is quite high.</li>\n<li>then check the distance between the center of eyes and the center of each white contour (because there could be some other white contours also in your image e.g. White cloth, paper etc.)</li>\n<li>if the distance between the center of eyes and the center of white contour is less than some threshold value, keep it and discard rest of the contours.</li>\n<li>Draw an outerline across the remaining contours (which should be 2, one for each eye).</li>\n</ol>\n\n<p>I am not sure if the above mentioned technique would work in your case but you can give it a try as it is not too hard.</p>\n\n<p><strong>PS: The things which i have mentioned about blurring, Canny, Countour ...you can have a look at the somehow similar</strong> <a href=\"https://stackoverflow.com/questions/26418558/opencv-thick-frame-glasses-detection\"><strong>SO Post</strong></a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "FInding the Z coordinate using disparity map",
        "input": "",
        "output": "<p>Since, you have found disparity mapping, I assume that you are working with rectified images. In that case, the Z coordinate is given by simple similar triangle formulation, </p>\n\n<p>z=Bf/d, where f if the focal length of the camera used (in pixels), d is the obtained disparity value for the pixel of interest and B is the baseline between the two stereo images. \nNote, the unit of z would be the same as that of B. </p>\n",
        "system": ""
    },
    {
        "instruction": "from heatmap, calculate discrete points",
        "input": "",
        "output": "<p>You could do</p>\n\n<ul>\n<li><p>Apply <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/threshold/threshold.html\" rel=\"nofollow\">Threshold</a> binary on source.</p></li>\n<li><p><a href=\"http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/find_contours/find_contours.html\" rel=\"nofollow\">Find contour</a>.</p></li>\n<li><p>Finally calculate contour mass center using <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/moments/moments.html\" rel=\"nofollow\">Image Moment</a>. </p></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "How to calculate &quot;Average Precision and Ranking&quot; for CBIR system",
        "input": "",
        "output": "<p>Yup that's correct.  You simply add up all of your precision values and average them.  This is the very definition of average precision.</p>\n\n<p>Average precision is simply a single number (usually in percentage) that gives you the overall performance of an image retrieval system.  The higher the value, the better the performance.  Precision-Recall graphs give you more granular detail on how the system is performing, but average precision is useful when you are comparing a lot of image retrieval systems together.  Instead of plotting many PR graphs to try and compare the overall performance of many retrieval systems, you can just have a table that compares all of the systems together with a single number that specifies the performance of each - namely, the average precision.</p>\n\n<p>Also, it doesn't make any sense to plot the average precision.  When average precision is normally reported in scientific papers, there is no plot.... just a single value!  The only way I could see you plotting this is if you had a bar graph, where the <code>y</code>-axis denotes the average precision while the <code>x</code>-axis denotes which retrieval system you are comparing.  The higher the bar, the better the accuracy.  However, a table showing all of the different retrieval systems, each with their average precision is more than suitable.  This is what is customarily done in most CBIR research papers.</p>\n\n<hr>\n\n<p>To address your other question, you calculate the average rank by using the average precision.  Calculate the average precision for all of your retrieval systems you are testing, then sort them based on this average precision.  Systems that have higher average precision will be ranked higher.  </p>\n",
        "system": ""
    },
    {
        "instruction": "Pyramid match kernel , partitioning the feature space",
        "input": "",
        "output": "<p>i may be wrong here, but I guess the intuition is to quantize the feature space. So, you could basically do bag of words with different code book sizes (128,64,32...) and use their kernel to compute similarity between 2 images.</p>\n",
        "system": ""
    },
    {
        "instruction": "Unable to find the difference in these two OpenCV codes?",
        "input": "",
        "output": "<p>Just add</p>\n<pre><code>using namespace cv;\n</code></pre>\n<p>after the <code>include</code> statements and before the <code>main</code> function.</p>\n<p><strong>Why?</strong></p>\n<p>All the OpenCV classes and functions are declared in the namespace <code>cv</code>. Or otherwise, you can also use the scope resolution operator like <em>cv::Mat</em>, <code>cv::imshow</code> etc to access the OpenCV functionality.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding characteres in a game image using Java",
        "input": "",
        "output": "<p>This question is related to computer vision and I would suggest using this java OCR library from here:\n<a href=\"http://ocrsdk.com/documentation/code-samples/\" rel=\"nofollow\">http://ocrsdk.com/documentation/code-samples/</a></p>\n\n<p>This will help you recognize English text present on image. I'm pretty sure that it will give you major texts from image.   </p>\n\n<p>PS: For image processing, using matlab or python would be better as they have good computer vision libraries like \"Computer vision Toolbox for Matlab\" and \"OpenCv for python\". Here are some examples for you task using matlab: <a href=\"http://www.mathworks.com/help/vision/ref/ocr.html\" rel=\"nofollow\">http://www.mathworks.com/help/vision/ref/ocr.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Not sure what this &#39;histogram code&#39; is doing in MATLAB",
        "input": "",
        "output": "<p>He is binning with <code>lb &lt;= x &lt; up</code> and splitting the interval <code>[0,180]</code> in <code>[-10,10), [10, 30), [30,40) ..., [150,170), [170,190)</code>.</p>\n\n<p>Suppose <code>x = 180</code>, then:</p>\n\n<pre><code>bin = floor(180/20-0.5) = floor(9-0.5) = floor(8.5) = 8; \n</code></pre>\n\n<p>while if <code>x = 0</code>:</p>\n\n<pre><code>bin = floor(`0/20-0.5) = floor(-0.5) = floor(-1) = -1; \n</code></pre>\n\n<p>which respectively translate into <code>x1 = 20 * (8+0.5) = 170</code> and <code>x1 = -10</code> which seems like what the function suggests <code>lowerHistBin()</code>.</p>\n\n<p>In the end, <code>w</code> simply measures how far the point <code>x</code> is from the corresponding lower bin <code>x1</code>. Notice that <code>w</code> is in (0,1], with <code>w</code> being 1 when <code>x = x1</code> and approaching 0 when <code>x -&gt; x1+binWidth</code>. So, if x say approaches 170, then <code>w</code> will approach <code>1 - (170-150)/20 = 0</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Not getting what &#39;spatial weights&#39; for HOG are",
        "input": "",
        "output": "<p>The idea here is that each pixel contributes not only to its own histogram cell, but also to the neighboring cell to some degree.  These contributions are weighed differently, depending on how close the pixel is to the edge of the cell. The closer you are to an edge of your cell, the more you contribute to the corresponding neighboring cell, and the less you contribute to your own cell.</p>\n",
        "system": ""
    },
    {
        "instruction": "Similarity measure in classification algorithm",
        "input": "",
        "output": "<p>There a number of possible measures of similarity. Ideally, you should derive one yourself that takes account of the reason why you are doing this classification, so that good similarity scores amount to something that performs well when you use it in practice. Here are a few examples.</p>\n\n<p>1) Cosine similarity. Treat the two sets of percentages as vectors, make them into unit vectors, and take the dot product to give you something between 0 and 1. So in your example you would have (10 * 10 + 20 * 30 + 30 * 20 + 40 * 40) / (sqrt(10 * 10 + 20 * 20 + 30 * 30 + 40 * 40) * sqrt(10 * 10 + 30 * 30 + 20 * 20 + 40 * 40)).</p>\n\n<p>2) If the expert and the classification system classified the same sperm and you kept track of which was which you could work out what percentage the classification system got correct. You didn't do this, but you can work out the maximum possible consistent with the data you have by taking, for each class, the minimum either assigned to this class. In your example, the classification system could have been correct for at most min(10, 10) + min(20, 30) + min(30, 20) + min(40, 40) percent. This score will be somewhere between 0 and 100 percent, with 100 percent for a perfect match.</p>\n\n<p>3) If the result of your classification was used as an input to a diagnostic test (e.g. patient will be infertile if...) instead of comparing the classification output, look at how often the results of your classification produce the same test result as the results of expert classifications - see <a href=\"http://en.wikipedia.org/wiki/Receiver_operating_characteristic\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Receiver_operating_characteristic</a>)</p>\n",
        "system": ""
    },
    {
        "instruction": "What is the difference between ProjectionTransformMatrix of VTK and GL_PROJECTION of OpenGL?",
        "input": "",
        "output": "<p>Well, the documentation you linked to actually explains this (emphasis mine):</p>\n\n<blockquote>\n  <p><code>vtkCamera::GetProjectionTransformMatrix</code>:</p>\n  \n  <p>Return the projection transform matrix, which converts <strong>from camera\n  coordinates to viewport coordinates</strong>. This method computes the aspect,\n  nearz and farz, then calls the more specific signature of\n  Get<strong>Composite</strong>ProjectionTransformMatrix</p>\n</blockquote>\n\n<p>with:</p>\n\n<blockquote>\n  <p><code>vtkCamera::GetCompositeProjectionTransformMatrix</code>:</p>\n  \n  <p>Return the concatenation of the ViewTransform and the\n  ProjectionTransform. This transform will convert world coordinates to\n  viewport coordinates. The 'aspect' is the width/height for the\n  viewport, and the nearz and farz are the Z-buffer values that map to\n  the near and far clipping planes. The viewport coordinates of a point located inside the frustum are in the range\n  ([-1,+1],[-1,+1], <strong>[nearz,farz]</strong>).</p>\n</blockquote>\n\n<p>Note that this neither matches OpenGL's window space nor normalized device space. If find the term \"viewport coordinates\" for this aa poor choice, but be it as it may. What bugs me more with this is that the matrix actually does not transform to that \"viewport space\", but to some clip-space equivalent. Only after the perspective divide, the coordinates will be in the range as given for the above definition of the \"viewport space\".</p>\n\n<blockquote>\n  <p>But is the clip coordinates in OpenGL same as viewport coordinates of\n  VTK?</p>\n</blockquote>\n\n<p>So that answer is a clear <em>no</em>. But it is close. Basically, that projection matrix is just a scaled and shiftet along the z dimension, and it is easy to convert between those two. Basically, you can simply take <code>znear</code> and <code>zfar</code> out of VTK's matrix, and put it into that OpenGL projection matrix formula you linked above, replacing just those two matrix elements. </p>\n",
        "system": ""
    },
    {
        "instruction": "How should I setup my input neurons to recieve my input",
        "input": "",
        "output": "<p>I gave you an example as below. \nIt is a binarized 4x4 image of letter <code>c</code>. You can either concatenate the rows or columns. I am concatenating by columns as shown in the figure. Then each pixel is mapped to an input neuron (totally 16 input neurons). In the output layer, I have 26 outputs, the letters <code>a</code> to <code>z</code>. </p>\n\n<p>Note, in the figure, I did not connect all nodes from layer <code>i</code> to layer <code>i+1</code> for simplicity, which you probably should connect all. </p>\n\n<p>At the output layer, I highlight the node of <code>c</code> to indicate that for this training instance, <code>c</code> is the target label. The expected input and output vector are listed in the bottom of the figure.</p>\n\n<p>If you want to keep the intensity of color, e.g., R/G/B, then you have to triple the number of inputs. Each single pixel is replaced with three neurons. </p>\n\n<p>Hope this helps more. For a further reading, I strongly suggest the deep learning tutorial by Andrew Ng at <a href=\"http://ufldl.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder\" rel=\"nofollow noreferrer\">here</a> - <a href=\"http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial\" rel=\"nofollow noreferrer\">UFLDL</a>. It's the state of art of such image recognition problem. In the exercise with the tutorial, you will be intensively trained to preprocess the images and work with a lot of engineering tricks for image processing, together with the interesting deep learning algorithm end-to-end.</p>\n\n<p><img src=\"https://i.sstatic.net/pcQ2r.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "How to represent object with holes",
        "input": "",
        "output": "<p>Chain codes could still be used if the orientation of the ring is taken into account: for instance a clockwise ring means outer boundary, an anticlockwise one means inner boundary (hole boundary). The signed area of the ring can be used to find its orientation.</p>\n\n<p>If the objects have multiple layers of holes (holes with islands, islands which also have holes, etc...), then some additional encoding convention or marker scheme is needed to find on which layer the next ring to be decoded is.</p>\n",
        "system": ""
    },
    {
        "instruction": "Use image moment as a descriptor for image with hole",
        "input": "",
        "output": "<p>To quote the <a href=\"http://en.wikipedia.org/wiki/Image_moment\" rel=\"nofollow noreferrer\">Wikipedia article</a> :</p>\n<blockquote>\n<p>Adapting this to scalar (greyscale) image with pixel intensities I(x,y), raw image moments Mij are calculated by</p>\n<p><img src=\"https://i.sstatic.net/pHLjo.png\" alt=\"Image moment equation\" /></p>\n</blockquote>\n<p>Where <em><code>I(x,y)</code></em> is the (greyscale) value of the pixel. So the answer to your first question is yes, you can use image moments quite easily on greyscale images, but if you want to treat different objects separately you have to have some way of distinguishing them.</p>\n<p>For your second question, I'm not exactly sure what you're asking. The moment can certainly be calculated for an object that has a hole in it, but specific cases would have to be considered to figure out what the differences in those moments would be.</p>\n",
        "system": ""
    },
    {
        "instruction": "Matching of Correlograms of two image",
        "input": "",
        "output": "<p>A correlogram is also a co-occurrence matrix / histogram.  To answer your question, the simple answer is yes.  Remember, when you're comparing histograms by themselves, you are comparing the grayscale / colour content between two images / patches.  By extending this to correlograms / co-occurrence matrices, you are also comparing <strong>the spatial distributions of the colours</strong> as well, which are handled by the third dimension, the distance, of the histogram.  If you had two images that had the same colour distribution, but the spatial distributions are different, the histogram measures will also take this into account and will report a high dissimilarity / low similarity between them.</p>\n\n<p>As such, you are perfectly fine in using standard histogram comparison measures between two correlograms (and I'm also speaking from experience).  As such, you can simply use any standard techniques that compare histograms together.  Examples include histogram intersection, the <code>L_p</code> norm, the chi-squared distance, the Bhattacharyya distance and so on.  </p>\n\n<p>Take a look at the following link for more details.  There are some great histogram similarity / dissimilarity measures you can use to compare between two histograms, each with their own advantages and disadvantages.  Also, Ander Biguri raised a good point.  Be sure to normalize the contrast between each view to make the content between the histograms somewhat contrast and illumination independent.</p>\n\n<p>Link: <a href=\"http://pi-virtualworld.blogspot.ca/2013/09/histogram-similarity.html\" rel=\"nofollow\">http://pi-virtualworld.blogspot.ca/2013/09/histogram-similarity.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Is it possible to store data on specific images on OpenCV?",
        "input": "",
        "output": "<p>Your question is not clear enough, so I apologize for my poor answer in advance. Anyway, let me try to answer them:</p>\n\n<blockquote>\n  <ol>\n  <li>could I store the data that represents the contours in the specific image?</li>\n  </ol>\n</blockquote>\n\n<p>If you <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#findcontours\" rel=\"nofollow noreferrer\">take a look at those docs</a>, you might notice that <code>findContours()</code> uses one argument as input, and another as output, so you can't pass the input <code>image</code> to this method and also used it to store the output <code>contours</code> because the method will throw an exception (I've tried this in the past).</p>\n\n<blockquote>\n  <ol start=\"2\">\n  <li>could I have another image and detect the contours and store them and then compare the contour data of each image to each other to see if there are objects with related geometric features?</li>\n  </ol>\n</blockquote>\n\n<p>It is possible to analyse 2 contours and compare them to each other. In fact, section <a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_contours/py_contours_more_functions/py_contours_more_functions.html#match-shapes\" rel=\"nofollow noreferrer\"><strong>3. Match Shapes</strong> of this tutorial</a> shares Python code that uses <em>hu-moments</em> to demonstrate how this can be achieved (invariant to translation, rotation and scale):</p>\n\n<p><img src=\"https://i.sstatic.net/0mOop.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code>import cv2\nimport numpy as np\n\nimg1 = cv2.imread('star.jpg',0)\nimg2 = cv2.imread('star2.jpg',0)\n\nret, thresh = cv2.threshold(img1, 127, 255,0)\nret, thresh2 = cv2.threshold(img2, 127, 255,0)\ncontours,hierarchy = cv2.findContours(thresh,2,1)\ncnt1 = contours[0]\ncontours,hierarchy = cv2.findContours(thresh2,2,1)\ncnt2 = contours[0]\n\nret = cv2.matchShapes(cnt1,cnt2,1,0.0)\nprint ret\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Leave one out accuracy for multi class classification",
        "input": "",
        "output": "<p>When using cross validation it is good to keep in mind that it applies to <strong>training</strong> a model, and not usually to the honest-to-god, end-of-the-whole-thing measures of accuracy, which are instead reserved for measures of classification accuracy on a testing set that has not been touched at all or involved in any way during training.</p>\n\n<p>Let's focus just on one single classifier that you plan to build. The \"A vs. rest\" classifier. You are going to separate all of the data into a training set and a testing set, and then you are going to put the testing set in a cardboard box, staple it shut, cover it with duct tape, place it in a titanium vault, and attach it to a NASA rocket that will deposit it in the ice covered oceans of Neptune.</p>\n\n<p>Then let's look at the training set. When we train with the training set, we'd like to leave some of the <em>training</em> data to the side, just for calibrating, <strong>but not</strong> as part of official Neptune ocean test set.</p>\n\n<p>So what we can do is tell every data point (in your case it appears that a data point is a video-valued object) to sit out once. We don't care if it comes from class A or not. So if there are 420 videos which would be used in the <em>training</em> set for just the \"A vs. rest\" classifier, the yeah, you're going to fit 420 different SVMs.</p>\n\n<p>And in fact, if you are tweaking parameters for the SVM, this is where you'll do it. For example, if you're trying to choose a penalty term or a coefficient in a polynomial kernel or something, then you will repeat the <em>entire training process</em> (yep, all 420 different trained SVMs) for all of the combinations of parameters you want to search through. And for each collection of parameters, you will associate with it the sum of the accuracy scores from the 420 LOO trained classifiers.</p>\n\n<p>Once that's all done, you choose the parameter set with the best LOO score, and voila, <em>that</em> is you 'A vs. rest' classifier. Rinse and repeat for \"B vs. rest\" and so on.</p>\n\n<p>With all of this going on, there is rightfully a big worry that you are overfitting the data. Especially if many of the \"negative\" samples have to be repeated from class to class. </p>\n\n<p>But, this is why you sent that testing set to Neptune. Once you finish with all of the LOO-based parameter-swept SVMs and you've got the final classifier in place, now you execute that classifier across you actual <strong>test</strong> set (from Neptune) and that will tell you if the entire thing is showing efficacy in predicting on unseen data.</p>\n\n<p>This whole exercise is obviously computationally expensive. So instead people will sometimes use Leave-P-Out, where P is much larger than 1. And instead of repeating that process until all of the samples have spent some time in a left-out group, they will just repeat it a \"reasonable\" number of times, for various definitions of reasonable. </p>\n\n<p>In the Leave-P-Out situation, there are some algorithms which do allow you sample which points are left out in a way that represents the classes fairly. So if the \"A\" samples make up 40 % of the data, you might want them to take up about 40% of the leave-out set. </p>\n\n<p>This doesn't really apply for LOO, for two reasons: (1) you're almost always going to perform LOO on every training data point, so trying to sample them in a fancy way would be irrelevant if they are all going to end up being used exactly once. (2) If you plan to use LOO for some number of times that is smaller than the sample size (not usually recommended), then just drawing points randomly from the set will naturally reflect the relative frequencies of the classes, and so if you planned to do LOO for K times, then simple taking a random size-K subsample of the training set, and doing regular LOO on those, would suffice.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to store all the pixels within a RotatedRect to another matrix?",
        "input": "",
        "output": "<p>Try this (not sure I undestand the problem perfectly):</p>\n\n<pre><code>#include \"opencv2/opencv.hpp\"\n#include &lt;vector&gt;\nusing namespace std;\nusing namespace cv;\n//----------------------------------------------------------\n//\n//----------------------------------------------------------\nvoid getQuadrangleSubPix_8u32f_CnR( const uchar* src, size_t src_step, Size src_size,\n                                   float* dst, size_t dst_step, Size win_size,\n                                   const double *matrix, int cn )\n{\n    int x, y, k;\n    double A11 = matrix[0], A12 = matrix[1], A13 = matrix[2];\n    double A21 = matrix[3], A22 = matrix[4], A23 = matrix[5];\n\n    src_step /= sizeof(src[0]);\n    dst_step /= sizeof(dst[0]);\n\n    for( y = 0; y &lt; win_size.height; y++, dst += dst_step )\n    {\n        double xs = A12*y + A13;\n        double ys = A22*y + A23;\n        double xe = A11*(win_size.width-1) + A12*y + A13;\n        double ye = A21*(win_size.width-1) + A22*y + A23;\n\n        if( (unsigned)(cvFloor(xs)-1) &lt; (unsigned)(src_size.width - 3) &amp;&amp;\n            (unsigned)(cvFloor(ys)-1) &lt; (unsigned)(src_size.height - 3) &amp;&amp;\n            (unsigned)(cvFloor(xe)-1) &lt; (unsigned)(src_size.width - 3) &amp;&amp;\n            (unsigned)(cvFloor(ye)-1) &lt; (unsigned)(src_size.height - 3))\n        {\n            for( x = 0; x &lt; win_size.width; x++ )\n            {\n                int ixs = cvFloor( xs );\n                int iys = cvFloor( ys );\n                const uchar *ptr = src + src_step*iys;\n                float a = (float)(xs - ixs), b = (float)(ys - iys), a1 = 1.f - a, b1 = 1.f - b;\n                float w00 = a1*b1, w01 = a*b1, w10 = a1*b, w11 = a*b;\n                xs += A11;\n                ys += A21;\n\n                if( cn == 1 )\n                {\n                    ptr += ixs;\n                    dst[x] = ptr[0]*w00 + ptr[1]*w01 + ptr[src_step]*w10 + ptr[src_step+1]*w11;\n                }\n                else if( cn == 3 )\n                {\n                    ptr += ixs*3;\n                    float t0 = ptr[0]*w00 + ptr[3]*w01 + ptr[src_step]*w10 + ptr[src_step+3]*w11;\n                    float t1 = ptr[1]*w00 + ptr[4]*w01 + ptr[src_step+1]*w10 + ptr[src_step+4]*w11;\n                    float t2 = ptr[2]*w00 + ptr[5]*w01 + ptr[src_step+2]*w10 + ptr[src_step+5]*w11;\n\n                    dst[x*3] = t0;\n                    dst[x*3+1] = t1;\n                    dst[x*3+2] = t2;\n                }\n                else\n                {\n                    ptr += ixs*cn;\n                    for( k = 0; k &lt; cn; k++ )\n                        dst[x*cn+k] = ptr[k]*w00 + ptr[k+cn]*w01 +\n                        ptr[src_step+k]*w10 + ptr[src_step+k+cn]*w11;\n                }\n            }\n        }\n        else\n        {\n            for( x = 0; x &lt; win_size.width; x++ )\n            {\n                int ixs = cvFloor( xs ), iys = cvFloor( ys );\n                float a = (float)(xs - ixs), b = (float)(ys - iys), a1 = 1.f - a, b1 = 1.f - b;\n                float w00 = a1*b1, w01 = a*b1, w10 = a1*b, w11 = a*b;\n                const uchar *ptr0, *ptr1;\n                xs += A11; ys += A21;\n\n                if( (unsigned)iys &lt; (unsigned)(src_size.height-1) )\n                    ptr0 = src + src_step*iys, ptr1 = ptr0 + src_step;\n                else\n                    ptr0 = ptr1 = src + (iys &lt; 0 ? 0 : src_size.height-1)*src_step;\n\n                if( (unsigned)ixs &lt; (unsigned)(src_size.width-1) )\n                {\n                    ptr0 += ixs*cn; ptr1 += ixs*cn;\n                    for( k = 0; k &lt; cn; k++ )\n                        dst[x*cn + k] = ptr0[k]*w00 + ptr0[k+cn]*w01 + ptr1[k]*w10 + ptr1[k+cn]*w11;\n                }\n                else\n                {\n                    ixs = ixs &lt; 0 ? 0 : src_size.width - 1;\n                    ptr0 += ixs*cn; ptr1 += ixs*cn;\n                    for( k = 0; k &lt; cn; k++ )\n                        dst[x*cn + k] = ptr0[k]*b1 + ptr1[k]*b;\n                }\n            }\n        }\n    }\n}\n\n//----------------------------------------------------------\n// \n//----------------------------------------------------------\nvoid myGetQuadrangleSubPix(const Mat&amp; src, Mat&amp; dst,Mat&amp; m )\n{\n    CV_Assert( src.channels() == dst.channels() );\n\n    cv::Size win_size = dst.size();\n    double matrix[6];\n    cv::Mat M(2, 3, CV_64F, matrix);\n    m.convertTo(M, CV_64F);\n    double dx = (win_size.width - 1)*0.5;\n    double dy = (win_size.height - 1)*0.5;\n    matrix[2] -= matrix[0]*dx + matrix[1]*dy;\n    matrix[5] -= matrix[3]*dx + matrix[4]*dy;\n\n    if( src.depth() == CV_8U &amp;&amp; dst.depth() == CV_32F )\n        getQuadrangleSubPix_8u32f_CnR( src.data, src.step, src.size(),\n        (float*)dst.data, dst.step, dst.size(),\n        matrix, src.channels());\n    else\n    {\n        CV_Assert( src.depth() == dst.depth() );\n        cv::warpAffine(src, dst, M, dst.size(),\n            cv::INTER_LINEAR + cv::WARP_INVERSE_MAP,\n            cv::BORDER_REPLICATE);\n    }\n}\n//----------------------------------------------------------\n// \n//----------------------------------------------------------\nvoid getRotRectImg(cv::RotatedRect rr,Mat &amp;img,Mat&amp; dst)\n{\n    Mat m(2,3,CV_64FC1);\n    float ang=rr.angle*CV_PI/180.0;\n    m.at&lt;double&gt;(0,0)=cos(ang);\n    m.at&lt;double&gt;(1,0)=sin(ang);\n    m.at&lt;double&gt;(0,1)=-sin(ang);\n    m.at&lt;double&gt;(1,1)=cos(ang);\n    m.at&lt;double&gt;(0,2)=rr.center.x;\n    m.at&lt;double&gt;(1,2)=rr.center.y;\n    myGetQuadrangleSubPix(img,dst,m);\n}\n\n//----------------------------------------------------------\n// \n//----------------------------------------------------------\nint main(int argc, char* argv[])\n{\n    Mat img=imread(\"D:\\\\ImagesForTest\\\\lena.jpg\");\n    img.convertTo(img,CV_32FC3,1.0/255.0);\n\n    cv::RotatedRect rr(cv::Point2f(200,200),Size(50,50),-30);\n\n    // rotated rectangle\n    Point2f rect_points[4];\n    rr.points( rect_points );\n\n    for( int j = 0; j &lt; 4; j++ )\n    {\n        line( img, rect_points[j], rect_points[(j+1)%4], Scalar(0,1,0), 1, CV_AA );\n    }\n\n    imshow(\"colImg\",img);\n    Mat dst(rr.size,CV_32FC3);\n    getRotRectImg(rr,img,dst);\n    imshow(\"rotImg\",dst);\n    cv::waitKey(0);\n    cv::destroyAllWindows();\n    return 0;\n}\n</code></pre>\n\n<p>The result:</p>\n\n<p><img src=\"https://i.sstatic.net/4zqON.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Segmenting characters from Image",
        "input": "",
        "output": "<p>I would like to show <strong>a quick &amp; dirty approach to isolate the letters/numbers</strong> in the plates since the actual segmentation of the characters is not the problem. When these are the input images:</p>\n\n<p><img src=\"https://i.sstatic.net/pzOSy.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/hnEul.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/NXuEg.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/G3Dui.jpg\" alt=\"enter image description here\"></p>\n\n<p>This is what you get at the end of my algorithm:</p>\n\n<p><img src=\"https://i.sstatic.net/f44m6.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/BlqXf.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/yZl2j.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/lGGYq.jpg\" alt=\"enter image description here\"></p>\n\n<p>So what I discuss in this answer will give you some ideas and help you to get rid of the artifacts present at the end of your current segmentation process. Keep in mind that this approach should only work with these types of images, and if you need something more robust you'll need to adjust some things or come up with entirely new ways to do these stuffs.</p>\n\n<ul>\n<li>Given the drastic changes in brightness, it's best to execute <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/histograms/histogram_equalization/histogram_equalization.html\" rel=\"noreferrer\">histogram equalization</a> to improve the contrast and make them more similar to each other so all the other techniques and parameters work with them:</li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/MNQUB.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/Hl8ld.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/5tPkq.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/hvakM.jpg\" alt=\"enter image description here\"></p>\n\n<ul>\n<li>Next, a <a href=\"http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html\" rel=\"noreferrer\">bilateral filter</a> can be used to smooth the images while preserving the edges of the objects, which is something important for the binarization process. This filter costs a little bit more processing power <a href=\"http://eric-yuan.me/bilateral-filtering/\" rel=\"noreferrer\">than others</a>.</li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/suAR2.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/fTyS4.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/2jthO.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/z1UBI.jpg\" alt=\"enter image description here\"></p>\n\n<ul>\n<li>After that the images are ready to be binarized, an <a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html\" rel=\"noreferrer\">adaptive threshold</a> is used to do the trick:</li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/fwFSV.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/ryYjF.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/M3K9I.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/g8KDN.jpg\" alt=\"enter image description here\"></p>\n\n<ul>\n<li>The result of the binarization is similar to what you achieved, so I came up with a way of using <code>findContours()</code> to remove the smaller and larger segments:</li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/oHRlX.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/xtX6s.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/Sb2Nr.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/ZmG19.jpg\" alt=\"enter image description here\"></p>\n\n<ul>\n<li>The result seems a little bit better, but it destroyed important segments of the characters on the plate. However, that's not really a problem right now because we are not worried about recognizing the character: we just want to isolate the area where they are. So the next step is to continue to erase segments, more specifically those that are not aligned with the same Y axis of the digits. The contours that survived this cut process are:</li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/glCGA.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/UQGQZ.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/4HdKc.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/fmxNd.jpg\" alt=\"enter image description here\"></p>\n\n<ul>\n<li>This is much better, and at this point a new <code>std::vector&lt;cv::Point&gt;</code> is created to store all the pixel coordinates needed to draw all these segments. This is necessary to create a <code>cv::RotatedRect</code> which is what allows us to create a <a href=\"https://stackoverflow.com/a/20724615/176769\">bounding box</a> and also <a href=\"https://stackoverflow.com/a/10317919/176769\">crop the image</a>:</li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/f44m6.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/BlqXf.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/yZl2j.jpg\" alt=\"enter image description here\"> <img src=\"https://i.sstatic.net/lGGYq.jpg\" alt=\"enter image description here\"></p>\n\n<p>From this point forward you can use the cropped images to execute your own techniques and easily segment the characters of the plate.</p>\n\n<p><strong>Here is the C++ code</strong>:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;    \n#include &lt;opencv2/highgui/highgui.hpp&gt;\n#include &lt;opencv2/imgproc/imgproc.hpp&gt;\n#include &lt;opencv2/imgproc/imgproc_c.h&gt;\n\n/* The code has an outter loop where every iteration processes one of the four input images */\n\nstd::string files[] = { \"plate1.jpg\", \"plate2.jpg\", \"plate3.jpg\", \"plate4.jpg\" };\ncv::Mat imgs[4];\nfor (int a = 0; a &lt; 4; a++)\n{\n    /* Load input image */\n\n    imgs[a] = cv::imread(files[a]);\n    if (imgs[a].empty())\n    {\n        std::cout &lt;&lt; \"!!! Failed to open image: \" &lt;&lt; imgs[a] &lt;&lt; std::endl;\n        return -1;\n    }\n\n    /* Convert to grayscale */\n\n    cv::Mat gray;\n    cv::cvtColor(imgs[a], gray, cv::COLOR_BGR2GRAY);\n\n    /* Histogram equalization improves the contrast between dark/bright areas */\n\n    cv::Mat equalized;\n    cv::equalizeHist(gray, equalized);\n    cv::imwrite(std::string(\"eq_\" + std::to_string(a) + \".jpg\"), equalized);\n    cv::imshow(\"Hist. Eq.\", equalized);\n\n    /* Bilateral filter helps to improve the segmentation process */\n\n    cv::Mat blur;\n    cv::bilateralFilter(equalized, blur, 9, 75, 75);\n    cv::imwrite(std::string(\"filter_\" + std::to_string(a) + \".jpg\"), blur);\n    cv::imshow(\"Filter\", blur);\n\n    /* Threshold to binarize the image */\n\n    cv::Mat thres;\n    cv::adaptiveThreshold(blur, thres, 255, cv::ADAPTIVE_THRESH_GAUSSIAN_C, cv::THRESH_BINARY, 15, 2); //15, 2\n    cv::imwrite(std::string(\"thres_\" + std::to_string(a) + \".jpg\"), thres);\n    cv::imshow(\"Threshold\", thres);\n\n    /* Remove small segments and the extremelly large ones as well */\n\n    std::vector&lt;std::vector&lt;cv::Point&gt; &gt; contours;\n    cv::findContours(thres, contours, cv::RETR_LIST, cv::CHAIN_APPROX_SIMPLE);\n\n    double min_area = 50;\n    double max_area = 2000;\n    std::vector&lt;std::vector&lt;cv::Point&gt; &gt; good_contours;\n    for (size_t i = 0; i &lt; contours.size(); i++)\n    {\n        double area = cv::contourArea(contours[i]);\n        if (area &gt; min_area &amp;&amp; area &lt; max_area)\n            good_contours.push_back(contours[i]);\n    }\n\n    cv::Mat segments(gray.size(), CV_8U, cv::Scalar(255));\n    cv::drawContours(segments, good_contours, -1, cv::Scalar(0), cv::FILLED, 4);\n    cv::imwrite(std::string(\"segments_\" + std::to_string(a) + \".jpg\"), segments);\n    cv::imshow(\"Segments\", segments);\n\n    /* Examine the segments that survived the previous lame filtering process\n     * to figure out the top and bottom heights of the largest segments.\n     * This info will be used to remove segments that are not aligned with\n     * the letters/numbers of the plate.\n     * This technique is super flawed for other types of input images.\n     */\n\n    // Figure out the average of the top/bottom heights of the largest segments\n    int min_average_y = 0, max_average_y = 0, count = 0;\n    for (size_t i = 0; i &lt; good_contours.size(); i++)\n    {\n        std::vector&lt;cv::Point&gt; c = good_contours[i];\n        double area = cv::contourArea(c);\n        if (area &gt; 200)\n        {\n            int min_y = segments.rows, max_y = 0;\n            for (size_t j = 0; j &lt; c.size(); j++)\n            {\n                if (c[j].y &lt; min_y)\n                    min_y = c[j].y;\n\n                if (c[j].y &gt; max_y)\n                    max_y = c[j].y;\n            }\n            min_average_y += min_y;\n            max_average_y += max_y;\n            count++;\n        }\n    }\n    min_average_y /= count;\n    max_average_y /= count;\n    //std::cout &lt;&lt; \"Average min: \" &lt;&lt; min_average_y &lt;&lt; \" max: \" &lt;&lt; max_average_y &lt;&lt; std::endl;\n\n    // Create a new vector of contours with just the ones that fall within the min/max Y\n    std::vector&lt;std::vector&lt;cv::Point&gt; &gt; final_contours;\n    for (size_t i = 0; i &lt; good_contours.size(); i++)\n    {\n        std::vector&lt;cv::Point&gt; c = good_contours[i];\n        int min_y = segments.rows, max_y = 0;\n        for (size_t j = 0; j &lt; c.size(); j++)\n        {\n            if (c[j].y &lt; min_y)\n                min_y = c[j].y;\n\n            if (c[j].y &gt; max_y)\n                max_y = c[j].y;\n        }\n\n        // 5 is to add a little tolerance from the average Y coordinate\n        if (min_y &gt;= (min_average_y-5) &amp;&amp; (max_y &lt;= max_average_y+5))\n            final_contours.push_back(c);\n    }\n\n    cv::Mat final(gray.size(), CV_8U, cv::Scalar(255));\n    cv::drawContours(final, final_contours, -1, cv::Scalar(0), cv::FILLED, 4);\n    cv::imwrite(std::string(\"final_\" + std::to_string(a) + \".jpg\"), final);\n    cv::imshow(\"Final\", final);\n\n\n    // Create a single vector with all the points that make the segments\n    std::vector&lt;cv::Point&gt; points;\n    for (size_t x = 0; x &lt; final_contours.size(); x++)\n    {\n        std::vector&lt;cv::Point&gt; c = final_contours[x];\n        for (size_t y = 0; y &lt; c.size(); y++)\n            points.push_back(c[y]);\n    }\n\n    // Compute a single bounding box for the points\n    cv::RotatedRect box = cv::minAreaRect(cv::Mat(points));\n    cv::Rect roi;\n    roi.x = box.center.x - (box.size.width / 2);\n    roi.y = box.center.y - (box.size.height / 2);\n    roi.width = box.size.width;\n    roi.height = box.size.height;\n\n    // Draw the box at on equalized image\n    cv::Point2f vertices[4];\n    box.points(vertices);\n    for(int i = 0; i &lt; 4; ++i)\n        cv::line(imgs[a], vertices[i], vertices[(i + 1) % 4], cv::Scalar(255, 0, 0), 1, CV_AA);\n    cv::imwrite(std::string(\"box_\" + std::to_string(a) + \".jpg\"), imgs[a]);\n    cv::imshow(\"Box\", imgs[a]);\n\n    // Crop the equalized image with the area defined by the ROI\n    cv::Mat crop = equalized(roi);\n    cv::imwrite(std::string(\"crop_\" + std::to_string(a) + \".jpg\"), crop);\n    cv::imshow(\"crop\", crop);\n\n    /* The cropped image should contain only the plate's letters and numbers.\n     * From here on you can use your own techniques to segment the characters properly.\n     */\n\n    cv::waitKey(0);\n}\n</code></pre>\n\n<p>For a more complete and robust way of doing license plate recognition with OpenCV, take a look at <a href=\"https://www.packtpub.com/application-development/mastering-opencv-practical-computer-vision-projects\" rel=\"noreferrer\">Mastering OpenCV with Practical Computer Vision Projects</a>, <strong>chapter 5</strong>. <a href=\"https://github.com/MasteringOpenCV/code/tree/master/Chapter5_NumberPlateRecognition\" rel=\"noreferrer\">Source code is available on Github!</a></p>\n",
        "system": ""
    },
    {
        "instruction": "computer vision: segmentation setup. Graph cut potentials",
        "input": "",
        "output": "<p>You could build your <a href=\"https://en.wikipedia.org/wiki/Conditional_random_field\" rel=\"nofollow noreferrer\">CRF</a> model over superpixels, such that a superpixel has a connection to another superpixel if it is a neighbour of it. </p>\n\n<p>For your statistical model <a href=\"http://www.robots.ox.ac.uk/~cbibby/pubs/ECCV08.pdf\" rel=\"nofollow noreferrer\">Pixel Wise Posteriors</a> are simple and cheap to compute. </p>\n\n<p>So, I suggest the following for the unary terms of the CRF:</p>\n\n<ol>\n<li>Build foreground and background histograms over texture per pixel(assuming you have a mask, or reasonable amount of marked foreground pixels(note, not superpixels)).</li>\n<li>For each superpixel, make an independence assumption over pixels within it, such that a superpixels likelihood of being either foreground or background is the product over each observation in the superpixel(in practice, we sum logs). The individual likelihood terms come from the histograms that you generated.</li>\n<li>Compute the posterior for foreground as the cumulative likelihood described above for foreground divided by the sum of the cumulative likelihoods of both. Similar for background.</li>\n</ol>\n\n<p>The pairwise terms between superpixels can be as simple as the difference between the mean observed textures(pixelwise) for each passed through a kernel, such as the <a href=\"https://en.wikipedia.org/wiki/Radial_basis_function_kernel\" rel=\"nofollow noreferrer\">Radial Basis Function</a>.</p>\n\n<p>Alternatively, you could compute histograms over each superpixels observed texture(again, pixel wise) and compute the <a href=\"https://en.wikipedia.org/wiki/Bhattacharyya_distance\" rel=\"nofollow noreferrer\">Bhattacharyya Distance</a> between each neighbouring pair of superpixels.</p>\n",
        "system": ""
    },
    {
        "instruction": "LK Optical flow using OpenGL ES on iOS or Android",
        "input": "",
        "output": "<p>Have you looked at Brad Larson's GPUImage library?\n<a href=\"https://github.com/BradLarson/GPUImage\" rel=\"nofollow\">https://github.com/BradLarson/GPUImage</a></p>\n\n<p>There is no direct method to implement optical flows using this library like cv::calcOpticalFlowPyrLK in openCV, there is a motiontracker that you could use though. Alternatively,You should be able to set up your own pipeline using GPUImage library to do optical flows.\nCheers.</p>\n",
        "system": ""
    },
    {
        "instruction": "Combination of SVD perturbation",
        "input": "",
        "output": "<p>The idea to have the value between one and two is to magnify the singular values to make them invariant to illumination changes.</p>\n\n<p>Refer to this paper: A New Face Recognition Method based on SVD Perturbation for Single Example Image per Person: Daoqiang Zhang,Songcan Chen,and Zhi-Hua Zhou</p>\n\n<p>Note that when n equals to 1, the derived image P is equivalent to the original image I . If we \nchoose n>1, then the singular values satisfying s_i > 1 will be magnified. Thus the reconstructed \nimage P emphasizes the contribution of the large singular values, while restraining that of the \nsmall ones. So by integrating P into I , we get a combined image J which keeps the main \ninformation of the original image and is expected to work better against minor changes of \nexpression, illumination and occlusions. </p>\n\n<p>My take:\nWhen you scale the singular values in the exponent, you are basically introducing a non-linearity, so its possible that for a specific dataset, scaling down the singular values may be beneficial. Its like adjusting the gamma correction factor in a monitor.</p>\n",
        "system": ""
    },
    {
        "instruction": "Using SURF algorithm to match objects on MATLAB",
        "input": "",
        "output": "<p>I would try not doing <code>selectStrongest</code> and not setting <code>MaxRatio</code>.  Just call <code>matchFeatures</code> with the default options and compare the number of resulting matches.</p>\n\n<p>The default behavior of <code>matchFeatures</code> is to use the ratio test to exclude ambiguous matches. So the number of matches it returns may be a good indicator of the presence or absence of the object in the scene.</p>\n\n<p>If you want to try something more sophisticated, take a look at this <a href=\"http://www.mathworks.com/help/vision/examples/image-search-using-point-features.html\" rel=\"nofollow\">example</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Image warping in OpenCV",
        "input": "",
        "output": "<p>Not all the grid_z points are <code>NaN</code> (only some of them). </p>\n\n<p>A bit more detailed printing (instead of just <code>print grid_z</code>) will show that.</p>\n\n<p><code>for i in grid_z:\n    print i</code></p>\n\n<p>Your function interpolates to give an output like </p>\n\n<p><img src=\"https://i.sstatic.net/Itvf7.png\" alt=\"Original code warp\"></p>\n\n<p>You have swapped the source and destination in <code>griddata()</code>. Swapping them back, you get</p>\n\n<pre><code>grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n</code></pre>\n\n<p>The output now is a bunch of straight lines :D.</p>\n\n<p><img src=\"https://i.sstatic.net/ISQrR.png\" alt=\"New output\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Reconstruction 3d on plane from 2d projection with camera calibration parametrs of plane",
        "input": "",
        "output": "<p>There are 4 cases to consider, in order of increasing complexity. The basic goal is to locate in space the plane of interest. After you have it, you can just intersect it with the ray that back-projects your pixel and thus obtain the solution.</p>\n\n<ol>\n<li>The plane of interest contains the checkerboard target AND the image is one of those you used to calibrate the camera. Then the extrinsic parameters [R | T] for that image returned by the calibration routine contain the answer, since the target is the XY plane of the returned world-to-camera coordinate transform at that image, and the world origin is one of the corners (you can see which corner it is by projecting point (0,0,0) into image coordinates. The XY plane (in camera coordinates) is the plane spanned by the first two columns of the rotation matrix R, and its origin is at point T.</li>\n<li>The plane of interest contains the checkerboard target, but its image is NOT one of those used to calibrate. You could just add it to the set, re-calibrate and goto 1., but a faster alternative is to (a) extract the corners as you do for calibration, (b) compute the homography H from their image positions to their \"true\" position on the physical target at Z=0; (c) decompose H into K * [R | T] using the known K from the calibration (you use an algorithm called \"RQ decomposition\" for this purpose, look it up). Then goto 1.</li>\n<li>The calibration target is not in the image, but you can identify in the image at least 4 points on that plane, such that no 3 of them are collinear and they are at known positions with respect to each other. For example, you see a rectangle of known sides in the image. Then, just like in point 2. you can compute the homography between those physical points and their images, and extract the [R|T] transform knowing the camera matrix. Then goto 1.</li>\n<li>None of the above: sorry, you are stuck.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How to rectify a detected ellipse",
        "input": "",
        "output": "<p>A circle is indeed transformed into an ellipse by a perspective transformation, however its axes are not the same as the axes of the initial circle, as shown in this illustration:</p>\n\n<p><a href=\"https://i.sstatic.net/Pq3G0.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Pq3G0.png\" alt=\"Ellipse vs Perspective circle\"></a><br>\n<sub>(source: <a href=\"http://www.brian-curtis.com/text/dfo_second_edition/PowerPoint/cirellipse%2024.png\" rel=\"nofollow noreferrer\">brian-curtis.com</a>)</sub>  </p>\n\n<p>You can refer to <a href=\"https://www.math.utah.edu/~treiberg/Perspect/Perspect.htm#circle\" rel=\"nofollow noreferrer\">this link</a> for a detailled demonstration. As a consequence, the bounding rectangle of the ellipse is not the image of the initial square by the perspective tranformation.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>This means that the center and the axes of the ellipse you observe are not the images, by the perspective mapping, of the center and axes of the original circle. I tried to make a clearer illustration:</p>\n\n<p><img src=\"https://i.sstatic.net/cxj62.png\" alt=\"enter image description here\"></p>\n\n<p>On this image, I drew in green the axes and center of the original circle, after perspective transformation, and the axes and center of the ellipse in red. On this specific example, the vertical axis is not deformed by the perspective mapping, but it would be deformed in the general case. Hence, deforming a circle by a perspective transformation gives an ellipse, but the axes and center that you see are not the axes and center of the original circle.</p>\n\n<p>As a consequence, you cannot simply use the top, bottom, left and right points on the ellipse (the red points, which can easily be detected from the ellipse) to map these onto the top, bottom, left and right points of the circle because they do not correspond under the perspective mapping (the green points do, but they cannot be detected easily from the ellipse).</p>\n\n<p>In the end, I don't think that it is at all possible to estimate the perspective mapping from a single detected ellipse.</p>\n",
        "system": ""
    },
    {
        "instruction": "Automatically Reorient and Crop Similar Images",
        "input": "",
        "output": "<p>I really don't know if writing an automatic software is the way to go, trust me, it would take you far lesser time in manually arranging all documents, than to write the code for it. \nAs far as I can see, some sort of automatic boxing technique could be used, using PCA or something on similar lines. However, if you are not a computer vision student or a prospective learner of the field, I highly recommend the manual method.   </p>\n\n<p>Sorry for the discouraging answer, but sometimes you have to take the sour medicine. :-(</p>\n",
        "system": ""
    },
    {
        "instruction": "Clustering human faces from a video",
        "input": "",
        "output": "<p>I suggest having a look at</p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/abs/1503.03832\" rel=\"nofollow noreferrer\">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></p>\n</blockquote>\n\n<p>My <a href=\"http://www.shortscience.org/paper?bibtexKey=journals/corr/1503.03832\" rel=\"nofollow noreferrer\">shortscience summary</a> (go there if you want to see the Math parts rendered correctly):</p>\n\n<p>FaceNet directly maps face images to $\\mathbb{R}^{128}$ where distances directly correspond to a measure of face similarity. They use a triplet loss function. The triplet is (face of person A, other face of person A, face of person which is not A). Later, this is called (anchor, positive, negative).</p>\n\n<p>The loss function is learned and inspired by LMNN. The idea is to minimize the distance between the two images of the same person and maximize the distance to the other persons image.</p>\n\n<h3>LMNN</h3>\n\n<p>Large Margin Nearest Neighbor (LMNN) is learning a pseudo-metric</p>\n\n<p>$$d(x, y) = (x -y) M  (x -y)^T$$</p>\n\n<p>where $M$ is a positive-definite matrix. The only difference between a pseudo-metric and a metric is that $d(x, y) = 0 \\Leftrightarrow x = y$ does not hold.</p>\n\n<h2>Curriculum Learning: Triplet selection</h2>\n\n<p>Show simple examples first, then increase the difficulty. This is done by selecting the triplets.</p>\n\n<p>They use the triplets which are <em>hard</em>. For the positive example, this means the distance between the anchor and the positive example is high. For the negative example this means the distance between the anchor and the negative example is low.</p>\n\n<p>They want to have</p>\n\n<p>$$||f(x_i^a) - f(x_i^p)||_2^2 + \\alpha &lt; ||f(x_i^a) - f(x_i^n)||_2^2$$</p>\n\n<p>where $\\alpha$ is a margin and $x_i^a$ is the anchor, $x_i^p$ is the positive face example and $x_i^n$ is the negative example. They increase $\\alpha$ over time. It is crucial that $f$ maps the images not in the complete $\\mathbb{R}^{128}$, but on the unit sphere. Otherwise one could double $\\alpha$ by simply making $f' = 2 \\cdot f$.</p>\n\n<h3>Tasks</h3>\n\n<ul>\n<li><strong>Face verification</strong>: Is this the same person?</li>\n<li><strong>Face recognition</strong>: Who is this person?</li>\n</ul>\n\n<h3>Datasets</h3>\n\n<ul>\n<li>99.63% accuracy on Labeled FAces in the Wild (LFW)</li>\n<li>95.12% accuracy on YouTube Faces DB</li>\n</ul>\n\n<h3>Network</h3>\n\n<p>Two models are evaluated: The <a href=\"http://www.shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13\" rel=\"nofollow noreferrer\">Zeiler &amp; Fergus model</a>  and an architecture based on the <a href=\"http://www.shortscience.org/paper?bibtexKey=journals/corr/SzegedyLJSRAEVR14\" rel=\"nofollow noreferrer\">Inception model</a>.</p>\n\n<h2>See also</h2>\n\n<ul>\n<li><a href=\"http://www.shortscience.org/paper?bibtexKey=conf/cvpr/TaigmanYRW14#martinthoma\" rel=\"nofollow noreferrer\">DeepFace</a></li>\n</ul>\n\n<h2>See also</h2>\n\n<ul>\n<li><a href=\"https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf\" rel=\"nofollow noreferrer\">DeepFace: Closing the Gap to Human-Level Performance in Face Verification</a></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "how to read image in Ada?",
        "input": "",
        "output": "<p>You can see two examples for reading a complete file on Rosetta Code: <a href=\"http://rosettacode.org/wiki/Read_entire_file#Ada\" rel=\"nofollow\">http://rosettacode.org/wiki/Read_entire_file#Ada</a></p>\n\n<p>Once you have read the file, it is \"just\" a matter of interpreting the file according to the relevant file format.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to use a 3x3 homography matrix in OpenGL ES shaders?",
        "input": "",
        "output": "<p>I think you just need to do the projection per pixel. Make refTexCoords and curTexCoords at least vec3, then do the /z in the pixel shader before texture lookup. Even better use the textureProj GLSL instruction. </p>\n\n<p>You want to do everything that is linear in the vertex shader, but things like projection need to be done in the fragment shader per pixel.  </p>\n\n<p>This link might help with some background: <a href=\"http://www.reedbeta.com/blog/2012/05/26/quadrilateral-interpolation-part-1/\" rel=\"nofollow\">http://www.reedbeta.com/blog/2012/05/26/quadrilateral-interpolation-part-1/</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How to use image with 4 channels",
        "input": "",
        "output": "<p>this works for me:</p>\n\n<pre><code>#include \"opencv2/opencv.hpp\"\nusing namespace std;\nusing namespace cv;\n//-----------------------------------------------------------------------------------------------------\n//\n//-----------------------------------------------------------------------------------------------------\nvoid Vignette(Mat&amp;img, Mat &amp;out) \n{\n    Mat a, b, c, d, f;\n    double sigma = 280; // vignette 'aperture', the param to play with\n    a = getGaussianKernel(img.cols, sigma, CV_32F);\n    b = getGaussianKernel(img.rows, sigma, CV_32F);\n    c = b * a.t();\n    cv::normalize(c,d,0,1,cv::NORM_MINMAX);\n    cvtColor(d, d, COLOR_GRAY2RGBA);\n    multiply(img, d, out, 1);\n}\n//-----------------------------------------------------------------------------------------------------\n//\n//-----------------------------------------------------------------------------------------------------\nint main ( int argc, char** argv )\n{\n    Mat frame;\n    frame=imread(\"D:/ImagesForTest/lena.jpg\",1);\n    Mat frame_f;\n    cvtColor(frame, frame_f, COLOR_RGB2RGBA);\n    frame_f.convertTo(frame_f,CV_32FC4,1.0/255.0);\n\n    Vignette(frame_f,frame);\n    imshow(\"frame\",frame);\n    imshow(\"frames\",frame_f);\n    cv::waitKey(0); \n}\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/lS9g9.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "3D rendering engine for python?",
        "input": "",
        "output": "<p>If you want interactivity, you can use OpenGL if you install PyOpenGL. OpenGL is pretty low level, and takes a while to learn, but it is quite powerful. It is perfectly possible to render the same scene from two (or two hundred) different cameras using OpenGL, so there's that. However, you'll have to specify all the geometry that you want to render by drawing very simple shapes (really just lines, triangles, and rectangles)</p>\n\n<p>The maker of PyOpenGL also has an additional module called \"OpenGLContext\" which takes care of some of the grungework for you; there are even tutorials at <a href=\"http://pyopengl.sourceforge.net/context/tutorials/index.html\" rel=\"nofollow\">http://pyopengl.sourceforge.net/context/tutorials/index.html</a>, and it includes scene-graph functionality, which will make specifying your scenes easier.</p>\n\n<p>I've heard good things about Panda3D, OpenSceneGraph, and VPython, but haven't used them myself, and I don't know what support they have for stereoscopic rendering.</p>\n\n<p>Good Luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Coordinates of a point with respect to a reference coordinate system",
        "input": "",
        "output": "<p>Assuming you have:</p>\n\n<ul>\n<li>a set of XYZ coordinates that belongs to your object's tracked features wrt to camera.</li>\n<li>the XYZ coordinates of the same features in some CAD (or world) reference frame (design coordinates).</li>\n</ul>\n\n<p>You can obtain the rotation and translation by computing the optimal rotation between these paired set of point coordinates, using the Kabsch algorithm: <a href=\"https://en.wikipedia.org/wiki/Kabsch_algorithm\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Kabsch_algorithm</a></p>\n\n<p>So, basically you:</p>\n\n<ol>\n<li>get the centroid of each coordinate set [XYZ]_i, where i= 1, 2. </li>\n<li>de-centroid each coordinate (subtract each centroid from each [XYZ])</li>\n<li>form a covariance matrix (call it C)</li>\n<li>Apply SVD to the covariance matrix [U,S,V] = svd(C)</li>\n<li>Rotation = V*U', where, ' is the transpose operator</li>\n<li>translation = -R * centroid_of_[XYZ]_1 + centroid_of_[XYZ]_2</li>\n</ol>\n\n<p>There is a nice guide about finding this rotation~translation pair here: <a href=\"http://nghiaho.com/?page_id=671\" rel=\"nofollow\">http://nghiaho.com/?page_id=671</a></p>\n\n<p>I hope it helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "what most candidate Features could you find for Buildings",
        "input": "",
        "output": "<p>Actually, you could choose some simple but effective features(building logo, the foreground of the building)  for recognizing offline. </p>\n\n<p>In order to make the result more accuracy, you cound send the GPS information back to the server.</p>\n\n<p>Do not only recognize the building by analyzing the picture, sometimes ,you can get more information   from the Android.</p>\n\n<p>Good luck.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculating the essential matrix from two sets of corresponding points",
        "input": "",
        "output": "<p>This equation is actually pretty common in geometry algorithms, essentially, you are trying to calculate the matrix <strong><em>X</em></strong> from the equation <strong><em>AXB=0</em></strong>. To solve this, you vectorise the equation, which means, \n<img src=\"https://i.sstatic.net/a0LkQ.png\" alt=\"enter image description here\"></p>\n\n<p>vec() means vectorised form of a matrix, i.e., simply stack the coloumns of the matrix one over the another to produce a single coloumn vector. If you don't know the meaning of the scary looking symbol, its called Kronecker product and you can read it from <a href=\"http://en.wikipedia.org/wiki/Kronecker_product\" rel=\"nofollow noreferrer\">here</a>, its easy, trust me :-)</p>\n\n<p>Now, say I call the matrix obtained by Kronecker product of <strong><em>B^T</em></strong> and <strong><em>A</em></strong> as <strong><em>C</em></strong>. \nThen, <strong><em>vec(X)</em></strong> is the null vector of the matrix <strong><em>C</em></strong> and the way to obtain that is by doing the <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\" rel=\"nofollow noreferrer\">SVD decomposition</a> of <strong><em>C^TC</em></strong> (C transpose multiplied by C) and take the the last coloumn of the matrix <strong><em>V</em></strong>. This last coloumn is nothing but your <strong><em>vec(X)</em></strong>. Reshape X to 3 by 3 matrix. This is you Essential matrix. </p>\n\n<p>In case you find this maths too daunting to code, simply use the following code by  Y.Ma et.al:</p>\n\n<pre><code>%  p are homogenius coordinates of the first image of size 3 by n\n%  q are homogenius coordinates of the second image of size 3 by n\n\nfunction [E]  = essentialDiscrete(p,q)\n\nn = size(p);\nNPOINTS = n(2);\n\n% set up matrix A such that A*[v1,v2,v3,s1,s2,s3,s4,s5,s6]' = 0\nA = zeros(NPOINTS, 9);\n\nif NPOINTS &lt; 9\n     error('Too few mesurements')\n     return;\nend\n\nfor i = 1:NPOINTS\n  A(i,:) = kron(p(:,i),q(:,i))';\nend\nr = rank(A);\n\nif r &lt; 8 \n  warning('Measurement matrix rank defficient')\n  T0 = 0; R = [];\nend;\n\n[U,S,V] = svd(A);\n\n% pick the eigenvector corresponding to the smallest eigenvalue\ne = V(:,9);\ne = (round(1.0e+10*e))*(1.0e-10);\n% essential matrix \nE = reshape(e, 3, 3);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Dense SIFT Keypoints &amp; Descriptor Extraction using vlfeat",
        "input": "",
        "output": "<pre><code>for (int i = 0; i &lt; numKeyPoints; i++) {\n    cout &lt;&lt; vlkeypoints[i].x &lt;&lt; endl;\n    cout &lt;&lt; vlkeypoints[i].y &lt;&lt; endl;\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to use the vision.GeometricShearer command so that it performs for up and down shearing?",
        "input": "",
        "output": "<p>BTW, you have a small typo in your syntax.  <code>values</code> should be capitalized and so it's <code>Values</code>.  This apparently is case-sensitive.... which is a bit ridiculous, but that's the way it is.</p>\n\n<hr>\n\n<p>Back to your post, you need to specify an additional flag to <a href=\"http://www.mathworks.com/help/vision/ref/vision.geometricshearer-class.html\" rel=\"nofollow noreferrer\"><code>vision.GeometricShearer</code></a> that determines the direction of where you want to apply the shear.  Specifically, you need to set the <code>Direction</code> flag, and you set this to either <code>Horizontal</code> or <code>Vertical</code>.  If you omit this, the default is <code>Horizontal</code>.  As such, if you want to shear the last column of your image and move it down by 20 pixels, you would do this:</p>\n\n<pre><code>h = vision.GeometricShearer('Values', [0 20], 'Direction', 'Vertical'); \n</code></pre>\n\n<p>If you want to visualize the results, you would use <code>step</code> and apply it to the image.  As an example, let's load in the <code>checkerboard</code> image that's part of the MATLAB system path, apply the shear, then show both of the results in the same figure:</p>\n\n<pre><code>%// Define vertical shear\nh = vision.GeometricShearer('Values', [0 20], 'Direction', 'Vertical');\nimg = im2single(checkerboard); %// Read in image\noutimg = step(h,img); %// Apply shear\n\n%// Show both results\nsubplot(2,1,1), imshow(img);\ntitle('Original image');\nsubplot(2,1,2), imshow(outimg);\ntitle('Output image');\n</code></pre>\n\n<p>This is what I get:</p>\n\n<p><img src=\"https://i.sstatic.net/ljMYO.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Calculating the gain ratio between two images?",
        "input": "",
        "output": "<p>If the images are of the same scene and only the gain varied then just the ratio of the average pixel value would be enough.</p>\n\n<p>You might want to do this for a range of regions in the image to handle saturated pixels, and take the median of the ratios of each pair of regions.</p>\n",
        "system": ""
    },
    {
        "instruction": "Closest distance between 2 convex hulls",
        "input": "",
        "output": "<p>It\u2019s a bit of an older library - and it\u2019s been a few years since I used it - but I seem to remember trying to do something similar with the <a href=\"http://gts.sourceforge.net\" rel=\"nofollow\">GNU Triangulated Surface</a> library...</p>\n\n<p>In particular, using the function <code>gts_surface_distance</code> to find the distance between two GtsSurfaces (which I think your non-convex meshes can still be represented as).</p>\n\n<p>See <a href=\"http://gts.sourceforge.net/reference/gts-surfaces.html#GTS-SURFACE-DISTANCE\" rel=\"nofollow\">here</a> for more info. </p>\n\n<p>I'm afraid I have no idea if it might be faster for you - but perhaps worth a shot!</p>\n",
        "system": ""
    },
    {
        "instruction": "How do we extract view up vector, view angle and focal point from a view transform matrix?",
        "input": "",
        "output": "<p>Transformation matrices are essentially the base vectors of the destination space as seen from the origin space. So the information you're interested in is available ready to use in the view matrix, the upper left 3\u00d73 sub matrix to be precise, each row or column (depending on which mapping you want; in a orthogonal matrix \u2013 and the upper left 3\u00d73 of a view should be orthogonal the transpose is the inverse, so the rows are the inverse to the columns).</p>\n\n<p>Note that there's no such thing as a \"focal point\" in view transformation, there are just directions Right, Up, and View. But that's exactly what you need.</p>\n",
        "system": ""
    },
    {
        "instruction": "Blobs detection using machine learning?",
        "input": "",
        "output": "<p>This might help. </p>\n\n<ol>\n<li><p>Extract texture features of the boundary of the blobs you want to extract. This can be done using <a href=\"http://www.cse.oulu.fi/CMV/Downloads/LBPMatlab\" rel=\"nofollow\">Local binary patterns</a>. There are many other texture features, you can get a detailed survey <a href=\"http://www.macs.hw.ac.uk/texturelab/files/publications/phds_mscs/MJC/ch4.pdf\" rel=\"nofollow\">here</a>. </p></li>\n<li><p>Then use them to train a binary classifier.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "7x7 Prewitt Kernel",
        "input": "",
        "output": "<p>This does not answer directly your question, but I came across this some time go and it might be of interest as it follows @AnonSubmitter85's comment.</p>\n\n<p>Based on a paper which can be found <a href=\"http://www.umiacs.umd.edu/~raghuram/ENEE731/EdgeDetection/Haralick_ZeroCrossing.pdf\" rel=\"nofollow noreferrer\">here</a>, there are many 'ways' to define a kernel of a given size since they used to describe polynomials (or linear combinations of polynomials) which describe functions from which directional derivatives of an image are computed. I admit that I don't understand everything, anyhow here are the possible windows that can be produced for kernels of size</p>\n\n<p>1) 3x3 </p>\n\n<p><img src=\"https://i.sstatic.net/TN0Bp.jpg\" alt=\"enter image description here\"></p>\n\n<p>or</p>\n\n<p>2) 4x4</p>\n\n<p><img src=\"https://i.sstatic.net/pc0Kv.jpg\" alt=\"enter image description here\"></p>\n\n<p>So I guess there is no single 7x7 kernel that can be computed. Since the Sobel and Prewitt are very miuch alike, there is a submission on the File Exchange which might interest you. Here is the code, which is pretty short. It creates a generalised Sobel filter of any size (I don't have any credit for it, I just paste the code so it's simpler for you to get :).</p>\n\n<pre><code>%Program for creating generalised Sobel operator\n%Authors : Jeny Rajan, K.Kannan\n%Medical Imaging Research Group, NeST, Trivandrum \n%http://jenyrajan.googlepages.com/\n%http://kannan.keizer.googlepages.com/kannankeizer \n\n%This program can be used to generate sobel filter of any order\n% Usage  [E Mx My]=Gsobel(img,Wsize)\n% eg. [E Mx My]=Gsobel(img,5)\n% E - Resultant Edge image generated with sobel filter of window size Wsize\n% Mx &amp; My  - Horizontal and Vertical Masks \n% img - input image\n% Wsize - Filter window size\n\nfunction [E Mx My]= GSobel(img,Wsize)\nfor i=1:Wsize\n    Sx(i)=factorial((Wsize-1))/((factorial((Wsize-1)-(i-1)))*(factorial(i-1)));\n    Dx(i)=Pasc(i-1,Wsize-2)-Pasc(i-2,Wsize-2);\nend\nSy=Sx';\nDy=Dx';\nMx=Sy(:)*Dx;\nMy=Mx';\nEy=imfilter(double(img),My,'symmetric');\nEx=imfilter(double(img),Mx,'symmetric');\nE=sqrt(Ex.^2+Ey.^2);\nfigure,imshow(img,[]),title('Original Image');\nfigure,imshow(E,[]),title('Edge Image');\n\nfunction P=Pasc(k,n)\nif (k&gt;=0)&amp;&amp;(k&lt;=n)\n    P=factorial(n)/(factorial(n-k)*factorial(k));\nelse\n    P=0;\nend\n</code></pre>\n\n<p>Sorry if my answer is quite long and does not really solve your problem, but it was obviously too long for a comment! Hope that helps somehow.</p>\n",
        "system": ""
    },
    {
        "instruction": "Big differences in image filtered with rotated Gabor kernels",
        "input": "",
        "output": "<p>The reason was the PSI param that I set to 0. The problem is immediatly fixed is psi is kept at the default value PI/2.</p>\n",
        "system": ""
    },
    {
        "instruction": "Understanding depth values in 3D point cloud",
        "input": "",
        "output": "<p>In this example the world coordinates are defined by the checkerboard. The checkerboard defines the X-Y plane, and the Z-axis points into the checkerboard, as explained in the <a href=\"http://www.mathworks.com/help/vision/gs/coordinate-systems.html\" rel=\"nofollow noreferrer\">documentation</a>:</p>\n\n<p><img src=\"https://i.sstatic.net/aU2BL.png\" alt=\"enter image description here\"></p>\n\n<p>Since your 3D points are above the checkerboard, they have negative Z-coordinates.</p>\n",
        "system": ""
    },
    {
        "instruction": "Camera Calibration Intrinsic Matrix what do the values represent?",
        "input": "",
        "output": "<p>According to the <a href=\"http://www.mathworks.com/help/vision/ref/cameraparameters-class.html\" rel=\"noreferrer\">official MATLAB documentation</a>, the intrinsic matrix is defined as follows:</p>\n\n<p><img src=\"https://i.sstatic.net/PHrMx.png\" alt=\"enter image description here\"></p>\n\n<p>where <code>fx = F*sx</code>, <code>fy = F*sy</code>, <code>F</code> is the focal length in world units, typically expressed in millimeters and <code>[sx, sy]</code> are the number of pixels per world unit in the x and y direction respectively. <code>fx</code> and <code>fy</code>are expressed in pixels.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV banner matching during sports events",
        "input": "",
        "output": "<p>Try SIFT instead of SURF. Some claim that SURF is more robust, but according to my personal experiences and the recent trend in the CV community SIFT is still the way to go.</p>\n\n<p>Also, you could use some density clustering method to further filter out some outliers, assuming you are only trying to find one object in the target image. A good example of using DBSCAN to improve the detection quality would be </p>\n\n<p>Le, Viet Phuong, et al. \"Improving Logo Spotting and Matching for Document Categorization by a Post-Filter Based on Homography.\" Document Analysis and Recognition (ICDAR), 2013 12th International Conference on. IEEE, 2013.</p>\n\n<p>Their method is very straightforward and easy to implement, I've implemented it myself and found it quite useful for certain tasks. It is just one or two hundred lines of code.</p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab Camera Calibration: saving chessboard calibration images",
        "input": "",
        "output": "<p>You generate the calibration images by taking pictures of the chessboard with your camera.</p>\n\n<p>Also, if you have a recent version of MATLAB with the Computer Vision System Toolbox, try the <a href=\"http://www.mathworks.com/help/vision/ug/find-camera-parameters-with-the-camera-calibrator.html\" rel=\"nofollow\">Camera Calibrator app</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Draw a parabola in opencv",
        "input": "",
        "output": "<p>Something like that should do the trick</p>\n\n<pre><code>int width = 500;\nfloat miny = -1, maxy = 1;\nMat image = Mat::zeros(width,width,CV_8UC3);\nvector&lt;Point2f&gt; list_point(width);\nfor(int i = 0; i &lt; width; i++){\n   list_point[i].x = i;\n   float real_y = miny + ((maxy-miny)*i)/width;\n   list_point[i].y = real_y*real_y;\n}\n//Draw the curve\nfor(int i = 1; i &lt; width; i++) line(image,list_point[i-1],list_point[i],Scalar(255,255,255));\nimshow(\"image\",image);\nwaitKey();\n</code></pre>\n\n<p>Here is the result with miny = -20 and maxy = 20 in green and miny = -10, maxy = 30 in blue</p>\n\n<p><img src=\"https://i.sstatic.net/nZdUV.png\" alt=\"Result of the algorithm\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Rectification of an image by using texture projection",
        "input": "",
        "output": "<p>OK, so the size is not a problem. Also to do rectification, once you have the homography that provides the rectification, use it in the vertex shader to multiply all of the initial 2D homogeneous coordinates.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to obtain the index value from an indexed image in MATLAB?",
        "input": "",
        "output": "<p>You can access the Index value using the output you get when you load the image, for instance if you store the image data in a structure called s:</p>\n\n<pre><code>s = load('clown')\n\ns = \n\n          X: [200x320 double]\n        map: [81x3 double]\n    caption: [2x1 char]\n</code></pre>\n\n<p>Pixel values are stored in <code>X</code>, whereas the associated colormap is stored in <code>map</code>.</p>\n\n<p>Since indices in Matlab are [row,column], the X and Y  values you are looking for can be accessed in X like this:</p>\n\n<pre><code>s.X(139,175)\n</code></pre>\n\n<p>which gives 45.</p>\n\n<p>More information can be found <a href=\"http://blogs.mathworks.com/steve/2006/02/03/all-about-pixel-colors-part-2/\" rel=\"nofollow\">here.</a></p>\n",
        "system": ""
    },
    {
        "instruction": "3d point reconstruction from depth map (with camera parameters)",
        "input": "",
        "output": "<p>When you rectify the images, you define new intrinsics for both. From that you get a homogeneous 4x4 matrix Q, which relates the image coordinates and disparity [x y d] to the world coordinates [X Y Z]. The details are in the \"Learning OpenCV\" book.</p>\n",
        "system": ""
    },
    {
        "instruction": "Content-Based Image Retrieval and Precision-Recall graphs using Color Histograms in MATLAB",
        "input": "",
        "output": "<p>The code that you have written is comparing the histogram between images, provided that they're grayscale.  If you want to do this for RGB images, you need to determine how many bins you want per plane.   Once you do this, for each RGB colour triplet that you have, you would determine a linear 1D index so that this would essentially act like a normal 1D histogram.  Once you do this, you can use the above code in the same fashion as you have specified above.  As such, let's create a function <code>imcolourhist</code> that takes in the image, and the total number of red, green and blue bins you want.  Bear in mind that you <strong>can't</strong> specify 256 bins per dimension.  Not only would this be too granular to have any discriminative power, but you would need <code>2^24 = 16777216</code> memory locations, and MATLAB will surely give you an out of memory error.</p>\n\n<p>The general procedure would be to determine which bin each colour independently belongs to.  Once you do this, you would then create a linear 1D index which is essentially a bin for a 1D histogram, and then you increment that value in that location.  I'm going to use <a href=\"http://www.mathworks.com/help/matlab/ref/accumarray.html\" rel=\"noreferrer\"><code>accumarray</code></a> to calculate the histogram for me.  Once we're done, this will essentially replace your <code>imhist</code> call and you would perform histogram intersection on this histogram that is output from <code>imcolourhist</code> instead.</p>\n\n<pre><code>function [out] = imcolourhist(im, num_red_bins, num_green_bins, num_blue_bins)\n\n    im = double(im); %// To maintain precision\n\n    %// Compute total number of bins\n    total_bins = num_red_bins*num_green_bins*num_blue_bins;\n\n    %// Figure out threshold between bins\n    red_level = 256 / num_red_bins;\n    green_level = 256 / num_green_bins;\n    blue_level = 256 / num_blue_bins;\n\n    %// Calculate which bins for each colour plane\n    %// each pixel belongs to\n    im_red_levels = floor(im(:,:,1) / red_level);\n    im_green_levels = floor(im(:,:,2) / green_level);\n    im_blue_levels = floor(im(:,:,3) / blue_level);\n\n    %// Compute linear indices\n    ind = im_blue_levels*num_red_bins*num_green_bins + im_green_levels*num_red_bins + im_red_levels;\n    ind = ind(:); %// Make column vector for accumarray\n\n    %// Determine 1D histogram - Ensure that every histogram\n    %// generated has the same size of total_bins x 1\n    out = accumarray(ind+1, 1, [total_bins 1]);\nend\n</code></pre>\n\n<hr>\n\n<p>Take this code, copy and paste it into a new file, then save it as <code>imcolourhist.m</code>.  Make sure you save this code in the same directory as where the above code you showed us is.  Take note that in <code>accumarray</code>, I offset the linear indices by 1, as the linear indices that I generate will start from <code>0</code>, but MATLAB starts indexing at <code>1</code>.  Now, all you have to do now is replace your <code>imhist</code> calls with <code>imcolourhist</code>.  I would recommend you choose the bins per colour channel to be 8 for now (i.e. <code>num_red_bins = num_green_bins = num_blue_bins = 8</code>.  You'll have to play around with this to get good results. </p>\n\n<p>As such, you would change your code where you're computing the histogram for <code>A</code> as:</p>\n\n<pre><code>Inp1=imread('D:\\visionImages\\c1\\1.ppm');  \nnum_red_bins = 8;\nnum_green_bins = 8;\nnum_blue_bins = 8;\nnum_bins = num_red_bins*num_green_bins*num_blue_bins;\nA = imcolourhist(Inp1, num_red_bins, num_green_bins, num_blue_bins);\n</code></pre>\n\n<p>Note that I'm reading in the images as <strong>colour</strong>, so the <code>rgb2gray</code> call is removed.  Similarly, for <code>B</code>, you would do:</p>\n\n<pre><code>B = zeros(num_bins, 30); \nptr=1;\nfor i = 1 : length(srcFiles)\n    filename = strcat('D:\\visionImages\\c1\\',srcFiles(i).name);\n    I = imread(filename);\n    B(:,ptr) = imcolourhist(I, num_red_bins, num_green_bins, num_blue_bins); \n    ptr=ptr+1;                                                      \nend\n</code></pre>\n\n<p>Note that I can't guarantee good results here.  Because you're only using the colour histogram as a method for image retrieval, you could have a query image and a database image that may have the same colour distributions, but look completely different in terms of texture and composition.  If both of these images have the same colour distribution, these would be considered as high similarity, even though they look nothing like each other.</p>\n\n<hr>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Otsu&#39;s Thresholding Implementation not working properly",
        "input": "",
        "output": "<p>There are a few issues with your code, and I'll outline where it's wrong:</p>\n\n<ol>\n<li>I'm simply nitpicking, but you can count the total number of pixels (<code>N</code>) by using <a href=\"http://www.mathworks.com/help/matlab/ref/numel.html\" rel=\"nofollow\"><code>numel</code></a>... it's just cleaner :)</li>\n<li>In your original code, you are checking for the right threshold between 1 and 254.  You really should be checking from 0 to 255, as there are 256 possible intensities in your image. </li>\n<li>You also need to change your <code>sigma</code> declaration so that there are 256 elements, not 255.  Remember, there are 256 possible intensities in your image.</li>\n<li>Within your <code>for</code> loop for checking each intensity, when you are calculating the probability of class occurrences, you need to check for intensity 0 as well.  Because of the fact that MATLAB starts indexing arrays at 1, you'll need to offset your access index so that you're starting at 1.</li>\n<li>Your definition of the variance between the object and background is slightly off.  You need to also calculate the probability of the class mean for the object as well.  You can check the code for more details.</li>\n<li>Your probability of class mean definitions are slightly inaccurate.  You need to divide by <code>qb</code> and <code>qo</code>, not <code>N</code>.</li>\n<li>You are using <code>m</code> to calculate then mean when you should be storing it in <code>muT</code>.</li>\n<li>Finally, when you find the maximum of the variance between the object and background, you need to <strong>subtract</strong> by 1, as this will provide an intensity between 0 and 255.  </li>\n</ol>\n\n<p>As such, this is what your code looks like.  Note that I have omitted the code that thresholds your image.  I'm only providing the code that calculates the threshold for your image.</p>\n\n<pre><code>hst=imhist(im1);\nsigma = zeros(256,1); %// Change\nN = numel(im1); %// Change\n\nfor T=0:255 %// Change\n    qb=0;\n    muT=0;\n    qo=0;\n    muQ=0; %// Change\n    for i=0:T %// Change\n        qb=qb+(hst(i+1)./N); % probability of class occurence (background)\n    end    \n    for i=T+1:255 \n        qo=qo+(hst(i+1)./N);% probability of class occurence (object)\n    end\n    for i=0:T%// Change        \n        muT=muT+((i.*hst(i+1))./qb);% probability of class mean (background)    \n    end\n    for i=T+1:255 %// Change\n        muQ=muQ+((i.*hst(i+1))./qo);% probability of class mean (object)        \n    end   \n    sigma(T+1) = qb*qo*((muT-muQ)^2); %// Change\nend\n[Y,T] = max(sigma);\nT = T-1; %// Change - For 0 to 255\n</code></pre>\n\n<hr>\n\n<p>This code should now work.  I ran this code with my own implementation of Otsu, and I get the same calculated threshold.  To be honest, I find this code to be rather inefficient due to the many <code>for</code> loops.  What I would personally do is vectorize it, but I'll leave that to you as a learning exercise :)</p>\n\n<hr>\n\n<h1>Edit</h1>\n\n<p>OK, I'll give in.  Here's some code that I wrote for Otsu that is more vectorized.  This essentially performs what you are doing above, but in a more vectorized manner.  You're more than welcome to use it for your own purposes, but do cite me if you intend to use it of course :)</p>\n\n<pre><code>%// Function that performs Otsu's adaptive bimodal thresholding\n%// Written by Raymond Phan - Version 1.0\n%// Input - im - Grayscale image\n%// Output - out - Thresholded image via Otsu\n\nfunction [out] = otsu(im)\n\n%// Compute histogram\nJ = imhist(im);\n\n%// Total number of intensities\nL = length(J);\n\n%// Some pre-processing stuff\n%// Determine total number of pixels\nnum_pixels = sum(J(:));\n\n%// Determine PDF\npdf = J / num_pixels;\n\n%// Storing between-class variances for each intensity\ns_b = zeros(1,L);\n\nfor idx = 1 : L\n    %// Calculate w_0\n    w_0 = sum(pdf(1:idx));\n\n    %// Calculate w_1\n    w_1 = 1 - w_0;\n\n    %// Calculate u_0\n    u_0 = sum((0:idx-1)'.*pdf(1:idx)) / w_0;\n\n    %// Calculate u_1\n    u_1 = sum((idx:L-1)'.*pdf(idx+1:L)) / w_1;\n\n    % // Calculate \\sigma_b^{2}\n    s_b(idx) = w_0*w_1*((u_1 - u_0)^2);    \nend\n\n%// Find intensity that provided the biggest variance\n[max_s_b T] = max(s_b);\nT = T - 1; %// Must subtract by 1, since the index starts from 1\n\n%// Now create an output image that thresholds the input image\nout = im &gt;= T;\n\nend\n</code></pre>\n\n<hr>\n\n<h1>Edits by Divakar</h1>\n\n<p>Divakar (thanks!) has created vectorized code to replace the loop portion of the above function code and this essentially gets rid of the pre-allocation for <code>s_b</code>:</p>\n\n<pre><code>w_0 = cumsum(pdf);\nw_1 = 1 - w_0;\nu_0 = cumsum((0:L-1)'.*pdf)./w_0;\nu_1 = flipud([0 ; cumsum((L-1:-1:1)'.*pdf((L:-1:2)))])./w_1;\ns_b = w_0.*w_1.*((u_1 - u_0).^2); \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "algorithm for finding closest images based on jitter / translation",
        "input": "",
        "output": "<p>You can calculate the difference between your images.<br>\nThe higher the intensity values of the difference image, the more they are different.<br>\nSo, if you have two exactly the same images and subtract them, there will be a \"black\" difference image.<br>\nYou can simply use the overloaded <code>operator-()</code> of <code>Mat</code>-class.</p>\n",
        "system": ""
    },
    {
        "instruction": "Geodesic computation on triangle meshes?",
        "input": "",
        "output": "<p>There are many implementation for computing geodesic distance on triangle mesh. Some are approximate and some are exact.</p>\n\n<p>1.Fast Marching method. This method is approximate and in practice the average error is below 1%. You can refer to Gabriel Peyre's implementation of fast marching method in matlab. \n<a href=\"http://www.mathworks.com/matlabcentral/fileexchange/6110-toolbox-fast-marching\">http://www.mathworks.com/matlabcentral/fileexchange/6110-toolbox-fast-marching</a></p>\n\n<ol start=\"2\">\n<li><p>MMP method proposed by [1] and implemented in [2]. This method is exact and the code is in <a href=\"https://code.google.com/p/geodesic/\">https://code.google.com/p/geodesic/</a>    . Same as the comment by Ante. A disadvantage is that when the mesh is larege, MMP method will consume a lot of memory, O(n^2), n is the number of vertices.</p></li>\n<li><p>CH method proposed in [3] and improved and implemented in [4]. This method is exact and consume less memory than MMP method. The code is in <a href=\"https://sites.google.com/site/xinshiqing/knowledge-share\">https://sites.google.com/site/xinshiqing/knowledge-share</a></p></li>\n<li><p>Heat method proposed in [5]. One implementation is in <a href=\"https://github.com/dgpdec/course\">https://github.com/dgpdec/course</a>\nThis method is approximated and require a preprocessing process. It's faster than Fast Marching method.</p></li>\n</ol>\n\n<p>[1] Joseph S. B. Mitchell, David M. Mount, and Christos H. Papadimitriou. 1987. The discrete geodesic problem. SIAM J. Comput. 16, 4 (August 1987), 647-668. </p>\n\n<p>[2] Vitaly Surazhsky, Tatiana Surazhsky, Danil Kirsanov, Steven Gortler, Hugues Hoppe. Fast exact and approximate geodesics on meshes. ACM Trans. Graphics (SIGGRAPH), 24(3), 2005.</p>\n\n<p>[3] Chen, J. and Han, Y.1990. Shortest paths on a polyhedron. InSCG '90: Proceedings of the sixth annual symposium on Computational geometry. ACM Press, New York, NY, USA, 360{369</p>\n\n<p>[4] Shi-Qing Xin and Guo-Jin Wang. 2009. Improving Chen and Han's algorithm on the discrete geodesic problem. ACM Trans. Graph. 28, 4, Article 104 (September 2009), 8 pages. </p>\n\n<p>[5] Crane K, Weischedel C, Wardetzky M. Geodesics in heat: a new approach to computing distance based on heat flow[J]. ACM Transactions on Graphics (TOG), 2013, 32(5): 152.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to compute difference between rotation matrices?",
        "input": "",
        "output": "<p>When working with rotation matrices, you have to be extra careful about the source coordinate frame and the destination coordinate frame.</p>\n\n<p>Considering two coordinate frames R1 and R2, you can denote the rotation matrix transforming a point M<sub>R1</sub>, expressed in R1, to the corresponding point M<sub>R2</sub>, expressed in R2, by R<sub>R2&lt;-R1</sub> such that :</p>\n\n<p>M<sub>R2</sub> = R<sub>R2&lt;-R1</sub> * M<sub>R1</sub></p>\n\n<p>This notation is very useful and has two nice properties :</p>\n\n<p>(R<sub>R2&lt;-R1</sub>)<sup>-1</sup> = R<sub>R1&lt;-R2</sub></p>\n\n<p>R<sub>R3&lt;-R2</sub>* R<sub>R2&lt;-R1</sub> = R<sub>R3&lt;-R1</sub></p>\n\n<p>Now, with this in mind you should get the answers to your question quite easily. Let's use the following notations regarding your particular problem:</p>\n\n<p>R<sup>0</sup><sub>1</sub> = R<sup>0</sup><sub>cam&lt;-world</sub> : rotation matrix world coordinates to camera coordinates at frame 0</p>\n\n<p>R<sup>0</sup><sub>2</sub> = R<sup>0</sup><sub>sensor&lt;-world</sub> : rotation matrix world coordinates to sensor coordinates at frame 0</p>\n\n<p>R<sup>t</sup><sub>1</sub> = R<sup>t</sup><sub>cam&lt;-world</sub> : rotation matrix world coordinates to camera coordinates at frame t</p>\n\n<p>R<sup>t</sup><sub>2</sub> = R<sup>t</sup><sub>sensor&lt;-world</sub> : rotation matrix world coordinates to sensor coordinates at frame t</p>\n\n<p>First, you want to find the rotation matrix transforming points in the camera coordinate frame to points in the sensor coordinate frame (i.e. \"orientation of real webcam coordinate frame with respect to sensor coordinate frame\"). </p>\n\n<p>R<sup>0</sup><sub>12</sub> = R<sup>0</sup><sub>sensor&lt;-cam</sub> = R<sup>0</sup><sub>sensor&lt;-world</sub> * R<sup>0</sup><sub>world&lt;-cam</sub> = R<sup>0</sup><sub>2</sub> * (R<sup>0</sup><sub>1</sub>)<sup>-1</sup></p>\n\n<p>Then, you want to find the rotation matrix of the web cam in subsequent frames, knowing the rotation matrix of the sensor for this frame and assuming R<sup>t</sup><sub>12</sub> = R<sup>0</sup><sub>12</sub>. </p>\n\n<p>R<sup>t</sup><sub>1</sub> = R<sup>t</sup><sub>cam&lt;-world</sub> = R<sup>t</sup><sub>cam&lt;-sensor</sub> * R<sup>t</sup><sub>sensor&lt;-world</sub> = (R<sup>t</sup><sub>sensor&lt;-cam</sub>)<sup>-1</sup> * R<sup>t</sup><sub>sensor&lt;-world</sub> = (R<sup>0</sup><sub>12</sub>)<sup>-1</sup> * R<sup>t</sup><sub>2</sub></p>\n",
        "system": ""
    },
    {
        "instruction": "Characterizing my RAW camera Output",
        "input": "",
        "output": "<p>The evidence that you have, first that each byte has the upper 4 bits as 0 and the fact that you can see a proper color picture from Irfanview, tells me that the employee was incorrect and the format isn't actually YUY2 - it's GRGB.</p>\n\n<p>This code should pull out the green pixels and double them up so you can get an idea of what the image looks like. Note that the even and odd lines will alternate between starting with G and (R or B).</p>\n\n<pre><code>with open('test.raw', 'r+b') as f:\n    raw_g = []\n    for y in range(720):\n        for x in range(1280//4):\n            if y % 2:\n                r1, r0, g1, g0, b1, b0, g3, g2 = f.read(8)\n            else:\n                g1, g0, r1, r0, g3, g2, b1, b0 = f.read(8)\n            g0 = g0 &lt;&lt; 8 | g1\n            g2 = g2 &lt;&lt; 8 | g3\n            raw_g.extend([g0, g0, g2, g2])\n</code></pre>\n\n<p>You'll probably want to right-shift these values <code>&gt;&gt; 4</code> before you write them out to a gray-scale file.</p>\n\n<p><strong>Edit:</strong> The code had one remaining bug (an extra read), and I didn't realize the values were little-endian instead of big-endian. Those changes are already applied to the code sample above.</p>\n\n<p>Since the values seemed a little dark, I also decided to apply a gamma correction - instead of <code>n &gt;&gt; 4</code> I used <code>int(255.999 * (n / 4095.0)**(1/2.2))</code></p>\n\n<p><img src=\"https://i.sstatic.net/b6tuY.png\" alt=\"green channel with gamma correction\"></p>\n\n<p>I'm afraid de-Bayering is a little beyond what I can pull off in a short StackOverflow answer.</p>\n",
        "system": ""
    },
    {
        "instruction": "Coffee beans separation algorithm",
        "input": "",
        "output": "<p>This approach is a <a href=\"https://stackoverflow.com/a/14617359/176769\">spin-off from mmgp's answer</a> that explains in detail <strong>how the watershed algorithm works</strong>. Therefore, if you need some explanation on what the code does, please check his answer.</p>\n\n<p>The code can be played with in order to improve the rate of detection. Here it is:</p>\n\n<pre><code>import sys\nimport cv2\nimport numpy\nfrom scipy.ndimage import label\n\ndef segment_on_dt(a, img):\n    border = cv2.dilate(img, None, iterations=3)\n    border = border - cv2.erode(border, None)\n    cv2.imwrite(\"border.png\", border)\n\n    dt = cv2.distanceTransform(img, 2, 5)    \n    dt = ((dt - dt.min()) / (dt.max() - dt.min()) * 255).astype(numpy.uint8)\n    _, dt = cv2.threshold(dt, 135, 255, cv2.THRESH_BINARY)\n    cv2.imwrite(\"dt_thres.png\", dt)    \n</code></pre>\n\n<p><strong>border</strong> (left), <strong>dt</strong> (right):</p>\n\n<p><img src=\"https://i.sstatic.net/PQoIG.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/3trhu.png\" alt=\"enter image description here\"></p>\n\n<pre><code>    lbl, ncc = label(dt)\n    lbl = lbl * (255/ncc)      \n    # Completing the markers now. \n    lbl[border == 255] = 255\n\n    lbl = lbl.astype(numpy.int32)\n    cv2.imwrite(\"label.png\", lbl)\n</code></pre>\n\n<p><strong>lbl</strong>:</p>\n\n<p><img src=\"https://i.sstatic.net/Q2TIp.png\" alt=\"enter image description here\"></p>\n\n<pre><code>    cv2.watershed(a, lbl)\n\n    lbl[lbl == -1] = 0\n    lbl = lbl.astype(numpy.uint8)\n    return 255 - lbl\n\n# Application entry point\nimg = cv2.imread(\"beans.png\")\nif img == None:\n    print(\"!!! Failed to open input image\")\n    sys.exit(0)\n\n# Pre-processing.\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    \n_, img_bin = cv2.threshold(img_gray, 128, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\ncv2.imwrite(\"img_bin.png\", img_bin)\n\nimg_bin = cv2.morphologyEx(img_bin, cv2.MORPH_OPEN, numpy.ones((3, 3), dtype=int))\ncv2.imwrite(\"img_bin_morphoEx.png\", img_bin)\n</code></pre>\n\n<p><strong>img_bin</strong> (left) before and after (right) a morphology operation:</p>\n\n<p><img src=\"https://i.sstatic.net/rPFks.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/mZ8Eu.png\" alt=\"enter image description here\"></p>\n\n<pre><code>result = segment_on_dt(img, img_bin)\ncv2.imwrite(\"result.png\", result)\n\nresult[result != 255] = 0\nresult = cv2.dilate(result, None)\nimg[result == 255] = (0, 0, 255)\ncv2.imwrite(\"output.png\", img)\n</code></pre>\n\n<p><strong>result</strong> (left) of watershed segmentation, followed by <strong>output</strong> (right):</p>\n\n<p><img src=\"https://i.sstatic.net/5hlVh.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/Fta6p.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    }
]