[
    {
        "instruction": "Fixed Number of Keypoints for similar Image detection",
        "input": "",
        "output": "<p>So \"match1\" represents the directed matches of one of the database images and \"match2\" a query image, \"finalMatch\" are all the matches between those images and \"finalMatch.get(i).distance\" is some kind of mean value between the two directed distances.</p>\n\n<p>So what you do is, you calculate the mean of the sum of the distances and scale them by the number of interest points you have. The goal I assume is to have a good meassure of how well the overall images match. </p>\n\n<p>I am pretty sure the distance you calculate doesn't reflect that similarity very well. Dividing the sum of the distances by the number of matches makes some sense and this might give you an idea of similarity when compared to other query images, but scaling this value with the number of interest points just doesn't do anything meaningful.</p>\n\n<p>First of all I would suggest that you get rid of this scaling. I'm not sure what your brute force matching does exactly, but additionally to your symmetry test, you should discard matches where the ratio of the first and the second candidate is to high (if I remember right, Lowe suggest a threshold of 0.8). Then, if it is a rigid scene, I would suggest that you apply some kind of fundamental matrix estimation (8 point algorithm + RANSAC) and filter the result using epipolar geometry. I'm pretty sure the mean discriptor distance of the \"real\" matches will give you a good idea about the \"similarity\" of the database image and the query.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to draw different shapes on an image in openCV?",
        "input": "",
        "output": "<p>If you have the coordinates it is easy. First create your cv::Mat and set it all white.</p>\n\n<pre><code>cv::Mat image;\nimage.setTo(cv::Scalar(255,255,255));\n</code></pre>\n\n<p>Then if you have the begin and the end coordinates you can draw a line using opencv <a href=\"http://docs.opencv.org/modules/core/doc/drawing_functions.html#line\" rel=\"nofollow\">line</a> function.</p>\n\n<pre><code>cv::line(image, cv::Point(initial_coords.x, initial_coords.y), cv::Point(end_coords.x, end_coords.y), cv::Scalar(0,0,255));\n</code></pre>\n\n<p>Finally to do the w use <a href=\"http://docs.opencv.org/modules/core/doc/drawing_functions.html#puttext\" rel=\"nofollow\">puttext function</a> </p>\n\n<pre><code>cv::putText(image, \"text\", cv::Point(coords.x, coords.y), cv::FONT_HERSHEY_SCRIPT_SIMPLEX, 2, Scalar::all(255), 3,8);\n</code></pre>\n\n<p>If you need to erase the window before adding new things use again the </p>\n\n<pre><code>image.setTo(cv::Scalar(255,255,255));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Remove background and get deer as a fore ground?",
        "input": "",
        "output": "<p>Here's what I would do: </p>\n\n<p>As was commented to your question, you can detect the dear and then perform grabcut to segment it from the picture.</p>\n\n<p>To detect the dear, I would couple a classifier with a sliding window approach. That would mean that you'll have a classifier that given a patch (can be a large patch) in the image, output's a score of how much that patch is similar to a dear. The sliding window approach means that you loop on the window size and then loop on the window location. For each position of the window in the image, you should apply the classifier on that window and get a score of how much that window \"looks like\" a dear. Once you've done that, threshold all the scores to get the \"best windows\", i.e. the windows that are most similar to a dear. The rational behind this is that if we a dear is present at some location in the image, the classifier will output a high score at all windows that are close/overlap with the actual dear location. We would like to merge all that locations to a single location. That can be done by applying the functions groupRectangles from OpenCV:</p>\n\n<p><a href=\"http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html#grouprectangles\" rel=\"nofollow\">http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html#grouprectangles</a></p>\n\n<p>Take a look at some face detection example from OpenCV, it basically does the same (sliding window + classifier) where the classifier is a Haar cascade.</p>\n\n<p>Now, I didn't mention what that \"dear classifier\" can be. You can use HOG+SVM (which are both included in OpenCV) or use a much powerful approach of running a deep convulutional neural network (deep CNN). Luckily, you don't need to train a deep CNN. You can use the following packages with their \"off the shelf\" ImageNet networks (which are very powerful and might even be able to identify a dear without further training):</p>\n\n<p>Decaf- which can be used only for research purposes:\n<a href=\"https://github.com/UCB-ICSI-Vision-Group/decaf-release/\" rel=\"nofollow\">https://github.com/UCB-ICSI-Vision-Group/decaf-release/</a></p>\n\n<p>Or Caffe - which is BSD licensed:</p>\n\n<p><a href=\"http://caffe.berkeleyvision.org/\" rel=\"nofollow\">http://caffe.berkeleyvision.org/</a></p>\n\n<p>There are other packages of which you can read about here:\n<a href=\"http://deeplearning.net/software_links/\" rel=\"nofollow\">http://deeplearning.net/software_links/</a></p>\n\n<p>The most common ones are Theano, Cuda ConvNet's and OverFeat (but that's really opinion based, you should chose the best package from the list that I linked to).</p>\n\n<p>The \"off the shelf\" ImageNet network were trained on roughly 10M images from 1000 categories. If those categories contain \"dear\", that you can just use them as is. If not, you can use them to extract features (as a 4096 dimensional vector in the case of Decaf) and train a classifier on positive and negative images to build a \"dear classifier\".</p>\n\n<p>Now, once you detected the dear, meaning you have a bounding box around it, you can apply grabcut:</p>\n\n<p><a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_grabcut/py_grabcut.html\" rel=\"nofollow\">http://docs.opencv.org/trunk/doc/py_tutorials/py_imgproc/py_grabcut/py_grabcut.html</a></p>\n\n<p>You'll need an initial scribble on the dear to perform grabcu. You can just take a horizontal line in the middle of the bounding box and hope that it will be on the dear's torso. More elaborate approaches would be to find the symmetry axis of the dear and use that as a scribble, but you would have to google, research an implement some method to extract symmetry axis from the image.</p>\n\n<p>That's about it. Not straightforward, but so is the problem.</p>\n\n<p>Please let me know if you have any questions.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to open just 10% magnified jpeg image in Python?",
        "input": "",
        "output": "<p>You can do this with <a href=\"http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html#resize\" rel=\"nofollow\">resize()</a>.</p>\n\n<pre><code>import cv2\nimport numpy as np\n\nimage = cv2.imread('image.jpg')\nrows, cols = image.shape[:2]\n\nresized = cv2.resize(image, (cols / 10, rows / 10), interpolation = cv2.INTER_CUBIC)\n</code></pre>\n\n<p>You can display <code>resized</code> and allow the user to select a region on it, then map the coordinate of that region back to the original image so you can do the actual processing on the full size image.</p>\n",
        "system": ""
    },
    {
        "instruction": "2D map translation computation",
        "input": "",
        "output": "<p><strong>Cheap and easy idea:</strong><br>\nA cheaper approach than shifting the entire second image -9 to +9 might be to take a single row in the second image and find the best matching row in the first image. Do the same for a single column. You can compute row/column similarity using something simple like using the absolute difference of the row/column vectors. This might give you an easy translation between the images.</p>\n\n<p><strong>Some more advanced alternatives:</strong><br>\nYou might be able to estimate some type of transformation between the images (e.g., affine transformation) using <a href=\"http://en.wikipedia.org/wiki/Scale-invariant_feature_transform\" rel=\"nofollow noreferrer\">SIFT</a> or <a href=\"http://en.wikipedia.org/wiki/SURF\" rel=\"nofollow noreferrer\">SURF</a> image features. See this: <a href=\"https://stackoverflow.com/questions/13758570/estimating-an-affine-transform-between-two-images\">Estimating an Affine Transform between Two Images</a></p>\n\n<p>More resources: <br>\n<a href=\"http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.html\" rel=\"nofollow noreferrer\">http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/warp_affine/warp_affine.html</a></p>\n\n<p>Another idea is to use <a href=\"http://en.wikipedia.org/wiki/Optical_flow\" rel=\"nofollow noreferrer\">optical flow</a> to find a pixel-based mapping between the images.</p>\n\n<p>More resources:<br>\n<a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html\" rel=\"nofollow noreferrer\">http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html</a></p>\n\n<p>These two approaches might be overkill since you seem to be dealing with a transformation of a static image. The SIFT/SURF features might make it easy to learn the transformation though.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is the significance of the eigenvalues of an autocorrelation matrix in image processing?",
        "input": "",
        "output": "<p>The eigenvalues of the autocorrelation matrix tell you what <strong>kind</strong> of feature you are looking at. The autocorrelation you are computing is based on an image patch you are looking at in the image.</p>\n\n<p>Actually, what you're computing is the <a href=\"http://en.wikipedia.org/wiki/Structure_tensor\" rel=\"noreferrer\">structure tensor</a>.  The Harris corner detector algorithm commonly refers to this matrix as the autocorrelation matrix, but it is really just a sum of squared differences.  The structure tensor is a 2 x 2 matrix of sum of squared differences between two image patches within the same image. </p>\n\n<p>This is how you'd compute the structure tensor in a Harris corner detector sense:</p>\n\n<p><img src=\"https://upload.wikimedia.org/math/b/d/2/bd2cf04bb7c3e4f9e53a73b4a3ad6057.png\" alt=\"blah\"></p>\n\n<p>Given an image patch in your image, <code>Ix</code> and <code>Iy</code> represent the partial derivatives of the image patch in the horizontal and vertical directions.  You can use any standard convolution operation to achieve these partial derivative images, like using a Prewitt or Sobel operator.</p>\n\n<p>After you compute this matrix, there are three situations that you need to take a look at when looking at the autocorrelation matrix in the Harris corner detector.  Note that this is a 2 x 2 matrix, and so there are two eigenvalues for this matrix.</p>\n\n<ol>\n<li>If both eigenvalues are close to 0, then there is no feature point of interest in the image patch you're looking at.</li>\n<li>If one of the eigenvalues is larger and the other is close to 0, this tells you that you are lying on an <strong>edge</strong>.  </li>\n<li>If <strong>both</strong> of the eigenvalues are large, that means the feature point we are looking at is a <strong>corner</strong>.</li>\n</ol>\n\n<p>However, it has been noted that calculating eigenvalues is a very computationally expensive operation, even if it's just for a 2 x 2 matrix.  Therefore, Harris came up with an interest point measure instead of computing the eigenvalues to determine whether or not something is <strong>interesting</strong>.  Basically, when you compute this measure, if it surpasses some set threshold, then what you have is a corner point within the centre of this patch.  If it doesn't, then there is no corner point.</p>\n\n<p><img src=\"https://upload.wikimedia.org/math/2/0/5/205bb6f246c54d741267eec95c898d92.png\" alt=\"blah\"></p>\n\n<p><code>Mc</code> is the \"score\" that is for a particular image patch to see if we have a corner point.  <code>det</code> is the determinant of the matrix, which is just <code>ad - bc</code>, given that your 2 x 2 matrix is in the form of <code>[a b; c d]</code>, and the <code>trace</code> is just the sum of the diagonals or <code>a + d</code>, given that the matrix is of the same form: <code>[a b; c d]</code>. <code>kappa</code> is a tunable parameter that usually ranges between 0.04 and 0.15.  The threshold that you set to see whether or not we have an interesting point or an edge highly depends on your image, so you'll have to play around with this.</p>\n\n<p>If you want to avoid using <code>kappa</code>, there is another way to estimate calculating the eigenvalues using Noble's corner measure:</p>\n\n<p><img src=\"https://upload.wikimedia.org/math/8/b/5/8b58eaf1ec3c177c06c1ce0b759b3666.png\" alt=\"blah\"></p>\n\n<p><code>epsilon</code> is some small constant, like <code>0.0001</code>.\nAgain, to figure out whether or not you have an interesting point depends on your image.  After you find all of the corner points in your image, people usually perform non-maximum suppression to suppress false positives.  What this means is that you examine a neighbourhood of corner points that surround the centre of a particular corner point.  If this centre corner point <strong>does not have the highest score</strong>, then this corner point is dropped.  This is also performed because if you were to detect corner points using a sliding window approach, it is highly probable that you would have multiple corner points within a small vicinity of the valid one when only <strong>one or a few</strong> would suffice.</p>\n\n<hr>\n\n<p>Basically, the point of looking at the eigenvalues is to check to see whether or not you are looking at an edge, a corner point, or nothing at all.</p>\n",
        "system": ""
    },
    {
        "instruction": "Create depth map from 3d points",
        "input": "",
        "output": "<p>Assuming you've corrected for camera tilt (a simple matrix multiplication if you know the angle), you can probably just follow <a href=\"http://www.mathworks.com/help/matlab/ref/griddata.html\" rel=\"nofollow\">this example</a></p>\n\n<pre><code>X = data(:,1);\nY = data(:,1);\nZ = data(:,1);\n\n%// This bit requires you to make some choices like the start X and Z, end X and Z and resolution (X and Z) of your desired depth map\n[Xi, Zi] = meshgrid(X_start:X_res:X_end, Z_start:Z_res:Z_end);\n\ndepth_map = griddata(X,Z,Y,Xi,Zi)\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Assertion Failed error: (-215) scn == 3 || scn == 4 in function cv::cvtColor works ALTERNATE times",
        "input": "",
        "output": "<p>At least I do not find any major problem in your code, i.e. \"should work\". The problem seems to be in the camera driver. Cameras are different, and camera drivers are different (a.k.a. buggy).</p>\n\n<p>Unfortunately, debugging the camera driver is not a very easy mission. The odd behaviour is most likely bound to the specific camera, operating system, OpenCV, and camera driver version. It is not very likely the driver can be fixed. Just try to keep everything up to date.</p>\n\n<p>However, as your camera can capture every second image, there are things to do.</p>\n\n<p>First, verify that the problem really is in the camera driver by replacing:</p>\n\n<pre><code>cam = create_capture(video_src, fallback='synth:bg=../cpp/lena.jpg:noise=0.05')\n</code></pre>\n\n<p>by</p>\n\n<pre><code>cam = create_capture('synth:bg=../cpp/lena.jpg:noise=0.05')\n</code></pre>\n\n<p>As is probably evident form the code, this forces the camera to be simulated. Function <code>create_capture</code> is only a wrapper around <code>VideoCapture</code> to provide this functionality. If your code runs fine with this, the problem is in the video driver.</p>\n\n<p>After verifying that, you could try to run the following code:</p>\n\n<pre><code>import cv2\n\ncam = cv2.VideoCapture(0)\ncam.open(0)\nresults = [ cam.read()[0] for i in range(100) ]\nprint results\n</code></pre>\n\n<p>This should create a list of 100 Trues, and the process should take a few seconds, as the camera should capture 100 consecutive images.</p>\n\n<p>In your code you do not seem to use the first value in the return tuple of <code>cam.read</code> (<code>ret</code> in your code). It is <code>True</code>if the image is really captured. Also, <code>cam.read</code> should block until there is an image available, no need to add any delays.</p>\n\n<p>Most probably you will get a list <code>[True, False, True, False, ...]</code> because the video driver does something odd. There is no beautiful way to fix this, but there is an ugly one. Replace your capture line by:</p>\n\n<pre><code># max. 10 retries\nfor i in range (10):\n    ret, img = cam.read()\n    if ret:\n        break\nelse:\n    # capture failed even after 10 tries\n    raise MyExceptiom(\"Video driver does not like me.\")\n</code></pre>\n\n<p>Of course, the driver may be so broken that you have to release and reopen it once in a while. Even uglier but doable, as well.</p>\n\n<p>Summary: \"Most probably it cannot be cured, it does not kill you, and there are medicines to alleviate the symptoms.\"</p>\n",
        "system": ""
    },
    {
        "instruction": "CV: Difference between MATLAB and OpenCV camera calibration techniques",
        "input": "",
        "output": "<p>Your adviser is correct in that both MATLAB and OpenCV use essentially the same calibration algorithm.  However, MATLAB uses the Levenberg-Marquardt non-linear least squares algorithm for the optimization (see <a href=\"http://www.mathworks.com/help/vision/ug/find-camera-parameters-with-the-camera-calibrator.html\" rel=\"nofollow\">documentation</a>), whereas OpenCV uses gradient descent. I would guess that this accounts for most of the difference in the reprojection errors.</p>\n\n<p>Additionally, MATLAB and OpenCV use different algorithms for checkerboard detection.</p>\n",
        "system": ""
    },
    {
        "instruction": "background extraction and update from video using matlab",
        "input": "",
        "output": "<p>You can maintain a buffer of <code>B</code> frames for a dynamic estimation of backgound</p>\n\n<pre><code>buff = NaN( [vidHeight, vidWidth, 3, B] ); % allocate room for buffer\n\n% process the video\nfor fi = 1:nFrames\n    % read current frame\n    thisframe = double(read(vob, k)) / 255; % convert to [0..1] range \n\n    % update background model\n    buff(:, :, :, mod( fi, B ) + 1 ) = thisframe;\n    background_L1 = nanmedian( buff, 4 ); % I think this is better than `mean` - try it!\n    background_L2 = nanmean( buff, 4 );\n\n    % do whatever processing you need with fi-th frame \n    % and the current background mode...\n    % ...\nend\n</code></pre>\n\n<p>Note that if <code>fi</code> &lt; <code>B</code> (i.e., you processed less than <code>B</code> frames) the background model is not stable. I am using <code>NaN</code>s as default values for the buffer and these values are ignored when backgound model is estimated -- this is the reason why I use <a href=\"http://www.mathworks.com/help/stats/nanmedian.html\" rel=\"nofollow\"><code>nanmedian</code></a> and <a href=\"http://www.mathworks.com/help/stats/nanmean.html\" rel=\"nofollow\"><code>nanmean</code></a> instead of simply <code>median</code> and <code>mean</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to calculate camera orientation using one point in large distance (using opencv)?",
        "input": "",
        "output": "<p>You need to specify an additional constraint - rotating the camera from its current pose to one that aligns the optical axis with an arbitrary ray leaves the camera free to rotate about the ray itself (i.e. it leaves the \"roll\" angle unspecified).</p>\n\n<p>Let's assume that you want the roll to be zero, i.e. that you want the motion to be a pure pan-tilt. This has a unique solution as long as the ray you want to align to is not parallel to the vertical image axis (in which case pan and roll are the same motion).</p>\n\n<p>Then the solution is computed as follows.  Let's use the OpenCV camera frame: Z=[0,0,1]' (, where \" ' \" means transpose) be the camera focal axis, oriented going out of the lens, Y=[0,1,0]' the vertical axis going down, and X = Z x Y (where 'x' is the cross product) the horizontal camera axis going toward the right of the image. So \"pan\" is a rotation about Y, \"tilt\" is a rotation about X.</p>\n\n<p>Let U = [u1, u2, u3]', with || u || = 1 be the ray you want to rotate to. You want to apply a pan that brings Z onto the plane Puy defined by the vectors u and Y, then apply a tilt that brings Z onto u.</p>\n\n<p>The angle of the first rotation is (angle between Z and Puy) = [90 deg - (angle between Z and Y x U)]. this is because Y x U is orthogonal to Puy. Look up the expressions for computing the angle between vectors on Wikipedia or elsewhere online. Once you have the angle (or its cosine and sine), the rotation about Y can be expressed as a standard rotation matrix Ry. </p>\n\n<p>The angle of the second rotation, about X after once Z is onto Puy, is the angle between vector Z and U after Ry is applied to Z, or equivalently, between Z and inv(Ry) * U. Compute the angle between the vector, and use to build a standard rotation matrix about X, Rx</p>\n\n<p>The final transformation is then Rx * Ry.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting the National ID card and getting the details",
        "input": "",
        "output": "<p>Assuming these IDs are prepared according to a standard template having specific widths, heights, offsets, spacing etc., you can try a template based approach. </p>\n\n<p>MRZ would be easy to detect. Once you detect it in the image, find the transformation that maps the MRZ in your template to it. When you know this transformation you can map any region on your template (for example, the photo of the individual) to the image and extract that region.</p>\n\n<p>Below is a very simple program that follows a happy path. You will have to do more processing to locate the MRZ in general (for example, if there are perspective distortions or rotations). I prepared the template just by measuring the image, and it won't work for your case. I just wanted to convey the idea. Image was taken from <a href=\"http://en.wikipedia.org/wiki/File:Czech_passport_2006_MRZ_data.jpg\" rel=\"noreferrer\">wiki</a></p>\n\n<pre><code>    Mat rgb = imread(INPUT_FILE);\n    Mat gray;\n    cvtColor(rgb, gray, CV_BGR2GRAY);\n\n    Mat grad;\n    Mat morphKernel = getStructuringElement(MORPH_ELLIPSE, Size(3, 3));\n    morphologyEx(gray, grad, MORPH_GRADIENT, morphKernel);\n\n    Mat bw;\n    threshold(grad, bw, 0.0, 255.0, THRESH_BINARY | THRESH_OTSU);\n\n    // connect horizontally oriented regions\n    Mat connected;\n    morphKernel = getStructuringElement(MORPH_RECT, Size(9, 1));\n    morphologyEx(bw, connected, MORPH_CLOSE, morphKernel);\n\n    // find contours\n    Mat mask = Mat::zeros(bw.size(), CV_8UC1);\n    vector&lt;vector&lt;Point&gt;&gt; contours;\n    vector&lt;Vec4i&gt; hierarchy;\n    findContours(connected, contours, hierarchy, CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE, Point(0, 0));\n\n    vector&lt;Rect&gt; mrz;\n    double r = 0;\n    // filter contours\n    for(int idx = 0; idx &gt;= 0; idx = hierarchy[idx][0])\n    {\n        Rect rect = boundingRect(contours[idx]);\n        r = rect.height ? (double)(rect.width/rect.height) : 0;\n        if ((rect.width &gt; connected.cols * .7) &amp;&amp; /* filter from rect width */\n            (r &gt; 25) &amp;&amp; /* filter from width:hight ratio */\n            (r &lt; 36) /* filter from width:hight ratio */\n            )\n        {\n            mrz.push_back(rect);\n            rectangle(rgb, rect, Scalar(0, 255, 0), 1);\n        }\n        else\n        {\n            rectangle(rgb, rect, Scalar(0, 0, 255), 1);\n        }\n    }\n    if (2 == mrz.size())\n    {\n        // just assume we have found the two data strips in MRZ and combine them\n        CvRect max = cvMaxRect(&amp;(CvRect)mrz[0], &amp;(CvRect)mrz[1]);\n        rectangle(rgb, max, Scalar(255, 0, 0), 2);  // draw the MRZ\n\n        vector&lt;Point2f&gt; mrzSrc;\n        vector&lt;Point2f&gt; mrzDst;\n\n        // MRZ region in our image\n        mrzDst.push_back(Point2f((float)max.x, (float)max.y));\n        mrzDst.push_back(Point2f((float)(max.x+max.width), (float)max.y));\n        mrzDst.push_back(Point2f((float)(max.x+max.width), (float)(max.y+max.height)));\n        mrzDst.push_back(Point2f((float)max.x, (float)(max.y+max.height)));\n\n        // MRZ in our template\n        mrzSrc.push_back(Point2f(0.23f, 9.3f));\n        mrzSrc.push_back(Point2f(18.0f, 9.3f));\n        mrzSrc.push_back(Point2f(18.0f, 10.9f));\n        mrzSrc.push_back(Point2f(0.23f, 10.9f));\n\n        // find the transformation\n        Mat t = getPerspectiveTransform(mrzSrc, mrzDst);\n\n        // photo region in our template\n        vector&lt;Point2f&gt; photoSrc;\n        photoSrc.push_back(Point2f(0.0f, 0.0f));\n        photoSrc.push_back(Point2f(5.66f, 0.0f));\n        photoSrc.push_back(Point2f(5.66f, 7.16f));\n        photoSrc.push_back(Point2f(0.0f, 7.16f));\n\n        // surname region in our template\n        vector&lt;Point2f&gt; surnameSrc;\n        surnameSrc.push_back(Point2f(6.4f, 0.7f));\n        surnameSrc.push_back(Point2f(8.96f, 0.7f));\n        surnameSrc.push_back(Point2f(8.96f, 1.2f));\n        surnameSrc.push_back(Point2f(6.4f, 1.2f));\n\n        vector&lt;Point2f&gt; photoDst(4);\n        vector&lt;Point2f&gt; surnameDst(4);\n\n        // map the regions from our template to image\n        perspectiveTransform(photoSrc, photoDst, t);\n        perspectiveTransform(surnameSrc, surnameDst, t);\n        // draw the mapped regions\n        for (int i = 0; i &lt; 4; i++)\n        {\n            line(rgb, photoDst[i], photoDst[(i+1)%4], Scalar(0,128,255), 2);\n        }\n        for (int i = 0; i &lt; 4; i++)\n        {\n            line(rgb, surnameDst[i], surnameDst[(i+1)%4], Scalar(0,128,255), 2);\n        }\n    }\n</code></pre>\n\n<p>Result: photo and surname regions in orange. MRZ in blue.\n<img src=\"https://i.sstatic.net/e6cwG.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "How to extract specific area of image",
        "input": "",
        "output": "<p>You have a couple of mistakes.\nFirstly you seem to be blending using the color to choose between the screen and multiply blends, but you should use the intensity. I think photoshop might do the blend in hsv colorspace, but in this case rgb seems to work as longs as you use L=(r+g+b)/3 for the intensity.</p>\n\n<p>Also your code was blending the image with and alpha blend of the vignette and the image (did the code in you question match the image generated in your question?). Instead you want a \"mask\" that equals the vignette in the areas where you want the vignette applied, and equals 0.5 in areas where you don't want it applied.</p>\n\n<p>So I take the vignette you provided (far left) which has an alpha channel (2nd from left) and\ndo an alpha blend with gray (second from right) to get an image to use as the top image in a blend overlay  (far right).  Where the top image is gray, when is blended with the other image, the bottom will show though unchanged.  This is so because in these two lines of code:</p>\n\n<pre><code>_result[j][c] = ((1 - (1 - 2 * (target - 0.5)) * (1 - blend)));\n_result[j][c] = 2 * target * blend;\n</code></pre>\n\n<p>if blend = 0.5, it works out that result is set to target. </p>\n\n<pre><code>     Vignette              Alpha                 Gray              Blend Image (top)\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/vdutN.png\" alt=\"enter image description here\"></p>\n\n<p>I have included the image generated, and the code to do it below.  The required image is shown on left, and the generated image is shown on the right. As far as I can see they are the same.\nAn improvement in accuracy could be obtained by not converting to <code>CV_UC3</code> in the middle, but passing <code>FC3</code> arguments in <code>blend_overlay()</code>.</p>\n\n<pre><code>               Required                                 Generated\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/FSh3z.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code>#include &lt;opencv2/core/core.hpp&gt;\n#include &lt;opencv2/highgui/highgui.hpp&gt;\n#include &lt;opencv2/imgproc/imgproc.hpp&gt;\n#include &lt;iostream&gt;     // std::cout\n#include &lt;vector&gt;       // std::vector\nusing namespace std;\nusing namespace cv;\n\nvoid blending_overlay2(Mat&amp; bot, Mat&amp; top, Mat&amp; out)\n{\n    Mat result(bot.size(), CV_32FC3);\n\n   // Extract the calculate I = (r + g + b) / 3\n    Mat botf;\n    bot.convertTo(botf, CV_32FC3, 1.0 / 255.0);\n    std::vector&lt;cv::Mat&gt; planes(3);\n    cv::split(botf, planes); \n\n    cv::Mat intensity_f((planes[0] + planes[1] + planes[2]) / 3.0f);\n    cv::Mat L;\n    intensity_f.convertTo(L, CV_8UC1, 255.0);\n    //cv::imshow(\"L\", L);\n\n\n    for(int i = 0; i &lt; bot.size().height; ++i)\n    {\n        // get pointers to each row\n        cv::Vec3b* _bot = bot.ptr&lt;cv::Vec3b&gt;(i);\n        cv::Vec3b* _top = top.ptr&lt;cv::Vec3b&gt;(i);\n        cv::Vec3f* _result = result.ptr&lt;cv::Vec3f&gt;(i);\n        uchar* _L = L.ptr&lt;uchar&gt;(i);\n\n        // now scan the row\n        for(int j = 0; j &lt; bot.size().width; ++j)\n        {   \n            for (int c=0; c &lt; bot.channels(); c++)\n            {\n                float target = float(_bot[j][c]) / 255.0f;\n                float blend = float(_top[j][c]) / 255.0f;\n\n                if(_L [j] &gt; 128)\n                {\n                    _result[j][c] = 2 * (blend + target - target * blend) - 1;\n                    // Why isn't the below line simplified like above?\n                    //_result[j][c] = ((1 - (1 - 2 * (target - 0.5)) * (1 - blend)));\n                }\n                else\n                {\n                    _result[j][c] = 2 * target * blend;\n                }\n            }\n        } \n    }\n    result.convertTo(out, CV_8UC3, 255);\n}\n\nint main( int argc, char** argv )\n{\n    Mat Img1=cv::imread(\"kqw0D.png\",-1); // the vignete in the question\n\n    Mat Img2=cv::imread(\"i.png\"); // the iamge in the question\n    Mat image = Img2.clone();\n    cv::resize(Img1,Img1,Img2.size());\n    Img1.convertTo(Img1,CV_32FC4,1.0/255.0);\n    Img2.convertTo(Img2,CV_32FC3,1.0/255.0);\n\n    // split off the alpha channel from the vignette\n    vector&lt;Mat&gt; ch; \n    split(Img1,ch);\n    Mat alpha = ch[3].clone();              // here's the vignette\n    Mat alpha_u;\n    alpha.convertTo(alpha_u,CV_8UC1,255);\n    imshow(\"alpha\",alpha);\n\n    // drop the alpha channel from vignette\n    ch.resize(3);\n\n    // pre-mutiply each color channel by the alpha\n    // and merge premultiplied color channels into 3 channel vignette I1\n    Mat I1;\n    cv::multiply(alpha, ch[0], ch[0]);\n    cv::multiply(alpha, ch[1], ch[1]);\n    cv::multiply(alpha, ch[2], ch[2]);\n    merge(ch, I1);\n\n    // Now make the vignette = 0.5 in areas where it should not be \"applied\"\n    Mat I2;\n    vector&lt;Mat&gt; ch2;\n    cv::Mat half = cv::Mat::ones(Img2.rows, Img2.cols, CV_32FC1) * 0.5;\n    cv::multiply(1.0f - alpha, half, half);\n    ch2.push_back(half);\n    ch2.push_back(half);\n    ch2.push_back(half);\n    //split(Img2, ch2);\n    //cv::multiply(1.0f - alpha, ch2[0], ch2[0]);\n    //cv::multiply(1.0f - alpha, ch2[1], ch2[1]);\n    //cv::multiply(1.0f - alpha, ch2[2], ch2[2]);\n    merge(ch2, I2);\n\n    // result = alpha * vignette + (1 - alpha) * gray;\n    Mat top;\n    top=I1+I2; \n\n\n    //  make I1 8 bit images again\n    I1.convertTo(I1,CV_8UC3,255);\n    I2.convertTo(I2,CV_8UC3,255);\n    top.convertTo(top,CV_8UC3,255);\n    //imshow(\"I1\",I1);\n    //imshow(\"I2\",I2);\n    //imshow(\"top\",top);\n\n    Mat result2;\n    blending_overlay2(image,top, result2);\n    imshow(\"Voila!\", result2);\n    imwrite(\"voila.png\", result2);\n    waitKey(0);\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How could I reads the image in subfolders in folder by using MATLAB",
        "input": "",
        "output": "<p>you can list the content of a folder using <code>dir</code> command</p>\n\n<pre><code>fldrs = dir( oneFolderName );  % list all sub folders of oneFolderName\nfor ii = 1:numel( fldrs )\n    if fldrs(ii).name(1) == '.'\n       continue; % skip '.' and '..' asuuming all other sub folders do not start with .\n    end\n    if ~fldrs(ii).isdir\n       continue; % skip non subfolders entries\n    end\n    fls = dir( fullfile(oneFolderName, fldrs(ii).name, '*.jpg' ) ); % list all jpg files in subfolder\n    for jj = 1:numel( fls )\n        img = imread( fullfile( oneFolderName, fldrs(ii).name, fls(ii).name ) ); % read image\n        % do your processing here...\n    end\nend \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Planar Homographies in matlab",
        "input": "",
        "output": "<p>You can not do it with just one transformation.\nBut if you know the exact equations of the three planes (i.e. the ground, the left wall, and the right wall) relative to your camera position, and also your camera orientation, then you can do simulate the scene from any given point by applying three different and separate homographies on the three planes.</p>\n\n<p>To do this, at first you need to extract each of the three planes from the image (e.g. img1=extracted ground, img2=extracted left wall, img3=extracted right wall).\nThen apply one separate homography to each one of them according to the formula:</p>\n\n<pre><code>H=K*R*inv(K) - 1/d * K*t*n'/K\n</code></pre>\n\n<p>You will have three homographies (H1, H2, and H3) corresponding to the three planes.</p>\n\n<p>Note that for each plane you have to set the the n and d according to that plane (The plane is defined as n'*x=d. and x=[x1,x2,x3]' and n=[a,b,c]'). Also take into consideration that t (=translation) and R (=rotation matrix) are adjusted according to the relative position and orientation of the the camera in the current scene and that of camera in the desired (to be simulated) state.</p>\n\n<p>Finally you can render the new image by transforming each plane (img1, img2, and img3) with its own H (H1, H2, and H3 respectively) and then attaching the three images.</p>\n\n<p>You can take a look at my answer in the following question. It explains how to implement the mentioned homography between two planes:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/18346525/homographic-image-transformation-distortion-issue/28331106#28331106\">homographic-image-transformation-distortion-issue</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How to get text label appearing in MATLAB in Computer Vision Toolbox",
        "input": "",
        "output": "<p>The easy way is to use the <code>insertObjectAnnotation</code> function, which draws a bounding box together with a label into the image.</p>\n",
        "system": ""
    },
    {
        "instruction": "Gradient Direction in Canny Edge Detection",
        "input": "",
        "output": "<p>At first glance, <code>find((iy&lt;=0 &amp; ix&gt;-iy)  | (iy&gt;=0 &amp; ix&lt;-iy));</code> returns the indices of all pixels where</p>\n\n<ul>\n<li><code>(iy&lt;=0 &amp; ix&gt;-iy)</code>, so \n<ul>\n<li>the y derivative is less than zero, so the edge is downward, between 90\u00b0 and 270\u00b0</li>\n<li>the x derivative is greater than zero, so the edge is leftward, between 180\u00b0 and 360\u00b0</li>\n<li>the magnitude of ix is greater than iy, so the edge is predominantly tilted vertically, not horizontally</li>\n<li>resulting in an edge between 180\u00b0 and 225\u00b0</li>\n</ul></li>\n<li>or <code>(iy&gt;=0 &amp; ix&lt;-iy))</code>, so\n<ul>\n<li>the y derivative is greater than zero, so the edge is upward, between 270\u00b0 and 90\u00b0</li>\n<li>the x derivative is less than zero, so the edge is rightward, between 0\u00b0 and 180\u00b0</li>\n<li>the magnitude of ix is greater than iy, so the edge is tilted vertically</li>\n<li>resulting in an edge between 0\u00b0and 45\u00b0</li>\n</ul></li>\n</ul>\n\n<p>Assuming that the pixels are ordered from up to down and from left to right, and an exactly vertical edge (black on the left, white on the right) is defined as 0\u00b0</p>\n\n<p>The other 3 terms of the switch case are analogous.</p>\n\n<p>This is not directly related to non-maximum suppression. I assume this is part of a canny edge filter or something similar, in which case the next step would be to find the local maximum in the just determined direction of the edge. This is done by comparing each pixel with its local neighbours in the edge direction, and erasing all but the maximum.</p>\n",
        "system": ""
    },
    {
        "instruction": "Can&#39;t install Vision Workbench",
        "input": "",
        "output": "<p>try running </p>\n\n<pre><code>./configure --with-boost=\"path/to/boost\"\n</code></pre>\n\n<p>if you have more than one version of boost installed, you may need to also set environment variables </p>\n\n<pre><code>HAVE_PKG_BOOST=yes\nPKG_BOOST_CPPFLAGS=/path/to/boost\nPKG_BOOST_LDFLAGS=/path/to/boost\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Gaussian blurring with OpenCV: only blurring a subregion of an image?",
        "input": "",
        "output": "<p>To blur the whole image, assuming you want to overwrite the original (<em>In-place filtering is supported</em> by <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html#gaussianblur\">cv::GaussianBlur</a>), you will have something like</p>\n\n<pre><code> cv::GaussianBlur(image, image, Size(0, 0), 4);\n</code></pre>\n\n<p>To blur just a region use <a href=\"http://docs.opencv.org/modules/core/doc/basic_structures.html#id6\">Mat::operator()(const Rect&amp; roi)</a> to extract the region:</p>\n\n<pre><code> cv::Rect region(x, y, w, h);\n cv::GaussianBlur(image(region), image(region), Size(0, 0), 4);\n</code></pre>\n\n<p>Or if you want the blurred output in a separate image:</p>\n\n<pre><code> cv::Rect region(x, y, w, h);\n cv::Mat blurred_region;\n cv::GaussianBlur(image(region), blurred_region, Size(0, 0), 4);\n</code></pre>\n\n<p>The above uses the default <code>BORDER_CONSTANT</code> option that just assumes everything outside the image is 0 when doing the blurring. \n<em>I am not sure what it does with pixels at the edge of a region</em>. You can force it to ignore pixels outside the region (BORDER_CONSTANT|BORDER_ISOLATE). SO it think it probably does use the pixels outside the region. You need to compare the results from above with:</p>\n\n<pre><code> const int bsize = 10;\n cv::Rect region(x, y, w, h);\n cv::Rect padded_region(x - bsize, y - bsize, w + 2 * bsize, h + 2 * bsize)\n cv::Mat blurred_padded_region;\n cv::GaussianBlur(image(padded_region), blurred_padded_region, Size(0, 0), 4);\n\n cv::Mat blurred_region = blurred_padded_region(cv::Rect(bsize, bsize, w, h));\n // and you can then copy that back into the original image if you want: \n blurred_region.copyTo(image(region));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "BOW prediction of cluster for training data",
        "input": "",
        "output": "<p>In order to assign the descriptors to clusters, you have to choose a distance metric. A simple choice would be the Euclidean distance. Then you need to compute the distance from the training descriptors to each cluster centroid, and assign them to the cluster whose centroid is closer to the descriptor than the centroids of any other cluster.</p>\n\n<p>After you've done that for all training descriptors on the image, you count the number of descriptors assigned to each cluster, and then build a histogram. The procedure must be repeated for all images in the training set, though. Since the number of descriptors might vary with the image, the histograms are usually normalized before being fed into the classifier as feature vectors for training.</p>\n",
        "system": ""
    },
    {
        "instruction": "Face morphing using opencv",
        "input": "",
        "output": "<p>I recently wrote an article on face morphing using OpenCV. I have shared the code in C++ / Python. You can find the details here </p>\n\n<p><a href=\"http://www.learnopencv.com/face-morph-using-opencv-cpp-python/\" rel=\"noreferrer\">http://www.learnopencv.com/face-morph-using-opencv-cpp-python/</a></p>\n\n<p>but the basic idea is as follows. </p>\n\n<ol>\n<li>Find Point Correspondences in the two input images. </li>\n<li>Do Delaunay triangulation on the points. </li>\n<li>The amount of morphing is controlled by a parameter alpha. E.g .for alpha = 0, you will get Ted Cruz in the example below, and for alpha  = 1.  you will get Hillary Clinton. For any other alpha, it will be the blend between the two. Use alpha to calculate the location of the points in the output ( morphed ) image by taking a weighted average of the two input image points. </li>\n<li>Calculate the affine transform between every triangle in the input images and the destination image ( morphed image ). </li>\n<li>Warp triangle from each input images to the output image, and blend the pixels based on alpha. Do this for every triangle and you get the output morphed image. </li>\n</ol>\n\n<p>Hope this helps. </p>\n\n<p><a href=\"https://i.sstatic.net/oRVgx.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/oRVgx.jpg\" alt=\"enter image description here\"></a></p>\n",
        "system": ""
    },
    {
        "instruction": "Any online community sharing Simulink models for computer vision?",
        "input": "",
        "output": "<p>Well Peter Corke provides a handy pair of Matlab/Simulink toolboxes for <a href=\"http://petercorke.com/Robotics_Toolbox.html\" rel=\"nofollow\">Robotic Control</a> and <a href=\"http://www.petercorke.com/Machine_Vision_Toolbox.html\" rel=\"nofollow\">Machine Vision</a>. It might not be exactly what you need, but I have found them extremely useful.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting frames for which a face appears in a video",
        "input": "",
        "output": "<p>You can use the <code>vision.CascadeObjectDetector</code> object to detect faces in any particular frame. If it does not detect any faces, its <code>step</code> method will return an empty array.  The problem is that the face detection algorithm is not perfect. Sometimes it detects false positives, i. e. detects faces where there are none.  You can try to mitigate that my setting the <code>MinSize</code> and <code>MaxSize</code> properties, assuming you know what size faces you expect to find.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to test the head Pose code",
        "input": "",
        "output": "<p>Check out <a href=\"http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html\" rel=\"nofollow\">PRIMA head pose estimation database</a>. It is freely available. PRIMA Head Pose Estimation Database consists of 2790 face images of 15 people with variation of yaw and pitch from -90 to +90 degrees. People in the dataset wear glasses or not and have various skin color. Background is neutral and face has a visible contrast from the background. Although the resolution of images is quite small - 384 x 288.</p>\n\n<p>In case of PRIMA, each image is labeled with yaw and pitch angles, but no roll. You can read those two values by parsing image file name, e.g. personne01157+15-30.jpg has a face oriented in such way that pitch = +15 degress, yaw = -30 degrees. <a href=\"http://www-prima.inrialpes.fr/perso/Gourier/Faces/displayDatabaseOpenCV.c\" rel=\"nofollow\">Here</a> you have an example how to parse database file names (although is old OpenCV API, you can use just the part which extracts and parses file names).</p>\n\n<p>I have used this database for research purposes in my master thesis, you only have to cite their paper in your work if you would like to do that.</p>\n",
        "system": ""
    },
    {
        "instruction": "Running the code of J-Linkage for model fitting",
        "input": "",
        "output": "<p>The source code of the function is included in the folder 'CSources'. It is coded using a mex function so you need to compile in Matlab. Refer to the function 'mex'</p>\n",
        "system": ""
    },
    {
        "instruction": "How to draw a line on a video frame in MATLAB",
        "input": "",
        "output": "<p>Unfortunately, you cannot use handle graphics commands on a <code>vision.VideoPlayer</code>.  However, there is a function <code>insertShape</code>, which lets you draw directly into the image, before you display it.</p>\n",
        "system": ""
    },
    {
        "instruction": "Camera projection matrix: why transpose rotation matrix?",
        "input": "",
        "output": "<p>It depends in which direction <code>R</code> was determined. I.e. is it a transformation of the camera in the global reference frame, or is it a transformation of the points in the local camera's reference frame.</p>\n\n<p>The true answer is: Don't worry just check that what you've got is right.</p>\n",
        "system": ""
    },
    {
        "instruction": "SIFT feature detection with heavy vignette images",
        "input": "",
        "output": "<ul>\n<li><p>I don't think vignetting is your problem.</p></li>\n<li><p>If \"remapping\" based on your calibration is supposed to account for lens distoritions, this can of course produce problems if the parameters are estimated wrong. Also, if distorition is very strong, the sampling during remapping might introduce problems. Additionally, if you use an epipolar matrix for outlier filtering, all distortions have to be accounted for.</p></li>\n<li><p>There seems to be some blur that might come from the remapping or camera motion. This can definitely mess up the results. Comparing the background structures of Image 22 and Image 9 I wonder what exactly is to be matched there. It doesn't look at all like a translation, more like some kind of random illumination. Maybe you can give some insight on what exactly the images show.</p></li>\n</ul>\n\n<p>Cheers,\nJo</p>\n",
        "system": ""
    },
    {
        "instruction": "some videos don&#39;t work with matlab with the next error",
        "input": "",
        "output": "<p>It seems like your face detector could not find any face for the specific frame <code>videoFrame</code>: the bounding box you got (<code>bbox</code>) is empty:</p>\n\n<blockquote>\n  <p><code>size(bbox)=[0,4]</code></p>\n</blockquote>\n\n<p>You might want to add a condition before cropping that <code>numel(bbox) &gt; 0</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Basler Pylon 4 SDK and OPENCV 2.4.9, CPylonImage to Mat",
        "input": "",
        "output": "<p>You have to make sure that the pixel types match. In your code example you use <code>PixelType_RGB8packed</code> for the camera image, and <code>CV_8UC1</code> as the Mat pixel type. you should use <code>CV_8UC3</code> instead. Also I would use <code>PixelType_BGR8packed</code> instead of <code>PixelType_RGB8packed</code>, because BGR is compatible with Windows bitmaps. I am assuming you use Windows. </p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Circle/Contour Detection Python",
        "input": "",
        "output": "<p>To me it looks like the output has the format of (x,y,radius), where (x,y) is the center of each circle.</p>\n",
        "system": ""
    },
    {
        "instruction": "Where can find image bank of car plates?",
        "input": "",
        "output": "<p>It's perfectly legal to do so, as long as the images are CC (Creative Commons) licensed, or you have permission of the website owner to do.</p>\n\n<p>A quick search for <a href=\"http://www.google.gg/search?q=ANPR%20Numberplates&amp;oq=ANPR%20Numberplates&amp;aqs=chrome..69i57j0l4.9024j0j4&amp;sourceid=chrome&amp;es_sm=122&amp;ie=UTF-8&amp;gws_rd=ssl&amp;surl=1&amp;safe=active#q=number%20plate%20image%20database&amp;safe=active&amp;surl=1\" rel=\"nofollow\"><code>number plate image database</code></a> yields some results:</p>\n\n<ul>\n<li><p><a href=\"http://www.zemris.fer.hr/projects/LicensePlates/english/images.html\" rel=\"nofollow\">Examples of test images</a></p></li>\n<li><p><a href=\"http://www.isical.ac.in/~scc/seminars/SumanMitra_ISI_CRSC_Presentation.pdf\" rel=\"nofollow\">Academic Document (More examples of number plates)</a></p></li>\n<li><a href=\"http://www.numberplates.com/pr-number-plate-gallery.asp\" rel=\"nofollow\">Good small library of number plates.</a></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Weak vs Strong descriptors - machine vision",
        "input": "",
        "output": "<p>A weak descriptor would be something which is not too refined or tuned (eg: haar features, edge maps etc). A strong descriptor(SIFT/SURF/MSER) would be something which is accurate, has high repeatability under blur, viewpoint/illumination change, JPEG compression. A boosting method would perform better for weak descriptors and SVM would be suitable for a strong descriptor. This is because the idea of boosting is to combine a lot of weak classifiers to learn a classifier. In the case of haar like features, adaboost combines many such weak features to learn a strong classifier. SVM tries to fit a hyperplane between the most confusing features between the two classes, so for SVM to perform better, the confusion between the classes should be less and features should be robust and accurate.</p>\n",
        "system": ""
    },
    {
        "instruction": "Simulating Camera Movement in MATLAB",
        "input": "",
        "output": "<p>What you need is a <a href=\"http://en.wikipedia.org/wiki/3D_projection\" rel=\"nofollow\">3D projection</a> between 2 coordinate spaces (camera and world) and obtain a <strong>view matrix</strong> which will represent the transformation of your view after the translation on y axis. </p>\n\n<p>Since there is no rotation involved, your task should not be so hard.\nHere are the steps you should follow:</p>\n\n<ul>\n<li>Set up the camera space by positioning and orienting the camera and derive a matrix from it(camera space)</li>\n<li>Construct a matrix that maps from world space(you already have) into camera space</li>\n<li>Map given vertexes(your image pixel positions here) from world space to camera space</li>\n<li>Translate those positions using the translation matrix</li>\n</ul>\n\n<p>In your case this should be:</p>\n\n<pre><code>       [ 1  0  0 ]  # notice that y axis is\nT =    [ 0 -2  0 ]  # translated while the \n       [ 0  0  1 ]  # other two stay the same\n</code></pre>\n\n<ul>\n<li>Map the translated points back into the world space and enjoy your new view</li>\n</ul>\n\n<p>This link is useful <a href=\"http://schabby.de/view-matrix/\" rel=\"nofollow\">http://schabby.de/view-matrix/</a> but you can also find a lot of other similar resources explaining the process. It's probably already in your reference text book for this particular class you are taking.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can I determine the angle a line found by HoughLines function using OpenCV?",
        "input": "",
        "output": "<p>If you use  HoughLines function, it will provide you lines already defined by two parameters: theta and rho, as</p>\n\n<pre><code>vector&lt;Vec2f&gt; lines;\n// detect lines\nHoughLines(image, lines, 1, CV_PI/180, 150, 0, 0 );\n\n// get lines\nfor( size_t i = 0; i &lt; lines.size(); i++ )\n{\n    float rho = lines[i][0], theta = lines[i][1];\n   ....\n}\n</code></pre>\n\n<p>Or\nif you apply HoughLinesP function, you will get lines defined by two points, you just need to calculate the angle of line between two points with regard to the image, as:</p>\n\n<pre><code>vector&lt;Vec4i&gt; lines;\n// detect the lines\nHoughLinesP(image, lines, 1, CV_PI/180, 50, 50, 10 );\nfor( size_t i = 0; i &lt; lines.size(); i++ )\n{\n    Vec4i l = lines[i];\n    // draw the lines\n\n    Point p1, p2;\n    p1=Point(l[0], l[1]);\n    p2=Point(l[2], l[3]);\n    //calculate angle in radian,  if you need it in degrees just do angle * 180 / PI\n    float angle = atan2(p1.y - p2.y, p1.x - p2.x);\n  .......\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Deskew bounding box",
        "input": "",
        "output": "<p>By eroding and boxing the resulting points, you can calculate the angle:</p>\n\n<pre><code>double deskew(const char* filename)\n{\n    cv::Mat img = cv::imread(filename, 0);\n\n    cv::Mat element = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(5, 3));\n    cv::erode(img, img, element);\n\n    std::vector&lt;cv::Point&gt; points;\n    cv::Mat_&lt;uchar&gt;::iterator it = img.begin&lt;uchar&gt;();\n    cv::Mat_&lt;uchar&gt;::iterator end = img.end&lt;uchar&gt;();\n\n    for (; it != end; ++it)\n       if (*it)\n          points.push_back(it.pos());\n\n    cv::RotatedRect box = cv::minAreaRect(cv::Mat(points));\n\n    double angle = box.angle;\n    if (angle &lt; -45.)\n       angle += 90.;\n\n    return angle;\n}\n</code></pre>\n\n<p><a href=\"http://felix.abecassis.me/2011/10/opencv-bounding-box-skew-angle/\" rel=\"nofollow\">OpenCV Bouding Box &amp; Skew Angle</a></p>\n\n<p>Then you can deskew with:</p>\n\n<pre><code>void deskew(cv::Mat img, double angle)\n{\n  cv::bitwise_not(img, img);\n\n  std::vector&lt;cv::Point&gt; points;\n  cv::Mat_&lt;uchar&gt;::iterator it = img.begin&lt;uchar&gt;();\n  cv::Mat_&lt;uchar&gt;::iterator end = img.end&lt;uchar&gt;();\n  for (; it != end; ++it)\n    if (*it)\n      points.push_back(it.pos());\n\n  cv::RotatedRect box = cv::minAreaRect(cv::Mat(points));\n\n  cv::Mat rot_mat = cv::getRotationMatrix2D(box.center, angle, 1);\n\n  cv::Mat rotated;\n  cv::warpAffine(img, rotated, rot_mat, img.size(), cv::INTER_CUBIC);\n}\n</code></pre>\n\n<p><a href=\"http://felix.abecassis.me/2011/10/opencv-rotation-deskewing/\" rel=\"nofollow\">OpenCV Rotating/Deskewing</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How do I enhance an image then convert it to a binary image using Python and OpenCV?",
        "input": "",
        "output": "<p>Converting <code>enhanced_im</code> to the appropriate type for <code>threshold</code> will make it work. That can be done with:</p>\n\n<pre><code>enhanced_im  = numpy.array(enhanced_im, numpy.uint8)\n</code></pre>\n\n<p>Here is your code, modified to work:</p>\n\n<pre><code>import cv2\nfrom scipy.signal import wiener\nimport numpy\n#---------------------------------------------------------------------\n# functions\n#---------------------------------------------------------------------\ndef enhance_image(input_image):\n    my_im = input_image.copy()\n    my_im = wiener(my_im)\n    return my_im\n\n#---------------------------------------------------------------------\n# Main\n#---------------------------------------------------------------------\nfilename=\"/home/ryan/OpenCV/opencv-2.3.4.7/samples/cpp/baboon.jpg\"\n\nim=cv2.imread(filename)\ncv2.imshow('Original',im)\n\ngray_im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\ncv2.imshow('Gray',gray_im)\n\nenhanced_im=enhance_image(gray_im)\ncv2.imshow('Enhanced',enhanced_im)\n#print type(enhanced_im)\n#enhanced_im.astype(numpy.uint8)\nenhanced_im  = numpy.array(enhanced_im, numpy.uint8)\n#print enhanced_im\n# this fails\n#ret,thresh = cv2.threshold(imgray,self.t1,225, cv2.THRESH_BINARY)\nthresh, bw_im = cv2.threshold(enhanced_im, 128, 255, cv2.THRESH_BINARY)\ncv2.imshow('Black and White',bw_im)\n\nkey = cv2.waitKey()\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Is edge points of object represent an ellipse?",
        "input": "",
        "output": "<p>As long as you have more than 2 points you could try linear fitting by using least squares:</p>\n\n<p>See here:<a href=\"https://math.stackexchange.com/a/153150/104118\">https://math.stackexchange.com/a/153150/104118</a></p>\n\n<p>See section 7 <strong>Fitting an Ellipse to 2D Points</strong> in the actual link: <a href=\"http://www.geometrictools.com/Documentation/LeastSquaresFitting.pdf\" rel=\"nofollow noreferrer\">http://www.geometrictools.com/Documentation/LeastSquaresFitting.pdf</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Brisk (binary robust invariant scalable keypoints )",
        "input": "",
        "output": "<p>The expression means that you are looking at a pair of pixels (pi, pj), such that both pixels belong to the region R2 x R2, and the two pixels cannot be the same.</p>\n\n<p>Gradient is a vector (Ix, Iy), where Ix is the first derivative in the x direction, and Iy is the first derivative in the y direction. This vector is defined at a point, so gradient is local by definition.  I don't know what global gradient means. More context may help here.</p>\n",
        "system": ""
    },
    {
        "instruction": "Object Detection software",
        "input": "",
        "output": "<p><code>vision.CascadeObjectDetector</code> is for detecting particular object categories, such as faces, eye, noses, etc.  For motion detection you should use <code>vision.ForegroundDetector</code>, which will give you a binary mask labeling pixels as background (stationary) or foreground (moving). However, this will only work if your camera does not move.</p>\n\n<p>See this <a href=\"http://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html\" rel=\"nofollow\">example</a> for more details.</p>\n\n<p>See this <a href=\"http://www.mathworks.com/help/vision/examples/video-display-in-a-custom-user-interface.html\" rel=\"nofollow\">example</a> for how to put vision.VideoPlayer inside a custom UI.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect black dots from color background",
        "input": "",
        "output": "<p>I was able to get some pretty nice first pass results by converting to HSV color space with <a href=\"http://www.mathworks.com/help/matlab/ref/rgb2hsv.html\" rel=\"noreferrer\"><code>rgb2hsv</code></a>, then using the Image Processing Toolbox functions <a href=\"http://www.mathworks.com/help/images/ref/imopen.html\" rel=\"noreferrer\"><code>imopen</code></a> and <a href=\"http://www.mathworks.com/help/images/ref/imregionalmin.html\" rel=\"noreferrer\"><code>imregionalmin</code></a> on the value channel:</p>\n\n<pre><code>rgb = imread('6abIc.jpg');\nhsv = rgb2hsv(rgb);\nopenimg = imopen(hsv(:, :, 3), strel('disk', 11));\nmask = imregionalmin(openimg);\nimshow(rgb);\nhold on;\n[r, c] = find(mask);\nplot(c, r, 'r.');\n</code></pre>\n\n<p>And the resulting images (for the image in the question and one chosen from your link):</p>\n\n<p><img src=\"https://i.sstatic.net/ElIhL.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/hBnlV.png\" alt=\"enter image description here\"></p>\n\n<p>You can see a few false positives and missed dots, as well as some dots that are labeled with multiple points, but a few refinements (such as modifying the structure element used in the opening step) could clean these up some.</p>\n",
        "system": ""
    },
    {
        "instruction": "Image segmentation with maxflow",
        "input": "",
        "output": "<p>I recognize that source very well.   That's the Boykov-Kolmogorov Graph Cuts library.  What I would recommend you do first is <a href=\"http://lvelho.impa.br/ip13/reading/ijcv06.pdf\">read their paper</a>.</p>\n\n<p>Graph Cuts is an <strong>interactive</strong> image segmentation algorithm.  You mark pixels in your image on what <strong>you believe</strong> belong to the object (a.k.a. foreground) and what don't belong to the object (a.k.a the background).  That's what you need first.  Once you do this, the Graph Cuts algorithm <strong>best guesses</strong> what the labels of the other pixels are in the image.  It basically goes through each of the other pixels that are not labeled and figures out whether or not they belong to foreground and background.</p>\n\n<p>The whole premise behind Graph Cuts is that image segmentation is akin to energy minimization.  Image segmentation can be formulated as a cost function with a summation of two terms:</p>\n\n<ol>\n<li>Self-Penalty: This is the cost of assigning each pixel as either foreground or background.  This is also known as a <strong>data cost</strong>.</li>\n<li>Neighbouring Penalties: This enforces that neighbouring pixels more or less should share the same classification label.  This is also known as a <strong>smoothness cost</strong>.</li>\n</ol>\n\n<p>This kind of formulation is well known as the <strong>Maximum A Posteriori Markov Random Field</strong> classification problem (MAP-MRF).  The goal is to minimize that cost function so that you achieve the best image segmentation possible.  This is actually an <strong>NP-Hard</strong> problem, and is actually one of the problems that is up for money from the Clay Math Institute.</p>\n\n<p>Boykov and Kolmogorov theoretically proved that the MAP-MRF problem can be translated into graph theory, and solving the MAP-MRF problem is akin to taking your image and forming it into a graph with source and sink links, as well as links that connect neighbouring pixels together.  To solve the MAP-MRF, you perform the maximum-flow/minimum-cut algorithm.  There are many ways to do this, but Boykov / Kolmogorov find a more efficient way that is much faster than more established algorithms, such as Push-Relabel, Ford-Fulkenson, etc.</p>\n\n<p>The self penalties are what are known as <strong>t</strong> links, while the neighbouring penalties are what are known as <strong>n</strong> links.  You should read up the paper to figure out how these are computed, but the <strong>t</strong> links describe the classification penalty.  Basically, how much it would cost to classify each pixel as belonging to the foreground or the background.  These are usually based on the negative log probability distributions of the image.  What you do is you create a histogram of the distribution of what was classified as foreground and a histogram of what was classified as background.</p>\n\n<p>Usually, a uniform quanitization of each colour channel for both foreground and background suffices.  You then turn these into PDFs but dividing by the total number of elements in each histogram, then when you calculate the t-links for each pixel, you access the colour, then see where it lies in the histogram, then take the negative log.  This will tell you how much it will cost to classify that pixel to be either foreground or background.</p>\n\n<p>The neighbouring pixel costs are more intuitive.  People usually just take the Euclidean distance between one pixel and a neighbouring pixel and apply this distance to a Gaussian.  To make things simple, a 4 pixel neighbourhood is what is usually used (North, South, East and West).</p>\n\n<p>Once you figure out how to compute the cost, you follow this procedure:</p>\n\n<ol>\n<li>Mark pixels as foreground or background.</li>\n<li>Create a graph structure using their library</li>\n<li>Compute the histograms of the foreground and background pixels</li>\n<li>Calculate t-links and add to the graph</li>\n<li>Calculate n-links and add to the graph</li>\n<li>Invoke the <code>maxflow</code> routine on the graph to segment the image</li>\n<li>Go through each pixel and figure out whether or not the pixel belongs to foreground or background.</li>\n<li>Create a binary map that reflects this, then copy over image pixels where the binary map is true, and don't do this when it's false.</li>\n</ol>\n\n<p>The original source of <code>maxflow</code> can be found here: <a href=\"http://pub.ist.ac.at/~vnk/software/maxflow-v3.03.src.zip\">http://pub.ist.ac.at/~vnk/software/maxflow-v3.03.src.zip</a></p>\n\n<p>It also has a README so you can see how the library is supposed to work given some example images.</p>\n\n<p>You have a lot to digest, but Graph Cuts is one of the most powerful interactive segmentation tools out there.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Ensure constant Mat size OpenCV",
        "input": "",
        "output": "<p>You can use <a href=\"https://stackoverflow.com/a/23984222/661211\"><code>cv:resize</code></a> to do it. Unlike in <a href=\"https://stackoverflow.com/a/23984222/661211\">the other answer</a>, You provide a desired-size <a href=\"http://docs.opencv.org/modules/core/doc/basic_structures.html#size\" rel=\"nofollow noreferrer\"><code>cv::Size</code></a> structure and leave the scale factor arguments. </p>\n\n<pre><code>cv::resize(theSourceOfAllEvil,myAwesomeMatrixResized,cv::Size(width,heigth));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Hough circle detection accuracy very low",
        "input": "",
        "output": "<p>I've adjusted my RANSAC algorithm from this answer: <a href=\"https://stackoverflow.com/questions/20698613/detect-semi-circle-in-opencv/20734263#20734263\">Detect semi-circle in opencv</a></p>\n\n<p>Idea:</p>\n\n<ol>\n<li>choose randomly 3 points from your binary edge image</li>\n<li>create a circle from those 3 points</li>\n<li>test how \"good\" this circle is</li>\n<li><p>if it is better than the previously best found circle in this image, remember</p></li>\n<li><p>loop 1-4 until some number of iterations reached. then accept the best found circle.</p></li>\n<li><p>remove that accepted circle from the image</p></li>\n<li><p>repeat 1-6 until you have found all circles</p></li>\n</ol>\n\n<p>problems: </p>\n\n<ol>\n<li>at the moment you must know how many circles you want to find in the image</li>\n<li>tested only for that one image.</li>\n<li>c++ code</li>\n</ol>\n\n<p>result:</p>\n\n<p><img src=\"https://i.sstatic.net/NjyeR.png\" alt=\"enter image description here\"></p>\n\n<p>code:</p>\n\n<pre><code>    inline void getCircle(cv::Point2f&amp; p1,cv::Point2f&amp; p2,cv::Point2f&amp; p3, cv::Point2f&amp; center, float&amp; radius)\n    {\n      float x1 = p1.x;\n      float x2 = p2.x;\n      float x3 = p3.x;\n\n      float y1 = p1.y;\n      float y2 = p2.y;\n      float y3 = p3.y;\n\n      // PLEASE CHECK FOR TYPOS IN THE FORMULA :)\n      center.x = (x1*x1+y1*y1)*(y2-y3) + (x2*x2+y2*y2)*(y3-y1) + (x3*x3+y3*y3)*(y1-y2);\n      center.x /= ( 2*(x1*(y2-y3) - y1*(x2-x3) + x2*y3 - x3*y2) );\n\n      center.y = (x1*x1 + y1*y1)*(x3-x2) + (x2*x2+y2*y2)*(x1-x3) + (x3*x3 + y3*y3)*(x2-x1);\n      center.y /= ( 2*(x1*(y2-y3) - y1*(x2-x3) + x2*y3 - x3*y2) );\n\n      radius = sqrt((center.x-x1)*(center.x-x1) + (center.y-y1)*(center.y-y1));\n    }\n\n\n\n    std::vector&lt;cv::Point2f&gt; getPointPositions(cv::Mat binaryImage)\n    {\n     std::vector&lt;cv::Point2f&gt; pointPositions;\n\n     for(unsigned int y=0; y&lt;binaryImage.rows; ++y)\n     {\n         //unsigned char* rowPtr = binaryImage.ptr&lt;unsigned char&gt;(y);\n         for(unsigned int x=0; x&lt;binaryImage.cols; ++x)\n         {\n             //if(rowPtr[x] &gt; 0) pointPositions.push_back(cv::Point2i(x,y));\n             if(binaryImage.at&lt;unsigned char&gt;(y,x) &gt; 0) pointPositions.push_back(cv::Point2f(x,y));\n         }\n     }\n\n     return pointPositions;\n    }\n\n\n    float verifyCircle(cv::Mat dt, cv::Point2f center, float radius, std::vector&lt;cv::Point2f&gt; &amp; inlierSet)\n    {\n     unsigned int counter = 0;\n     unsigned int inlier = 0;\n     float minInlierDist = 2.0f;\n     float maxInlierDistMax = 100.0f;\n     float maxInlierDist = radius/25.0f;\n     if(maxInlierDist&lt;minInlierDist) maxInlierDist = minInlierDist;\n     if(maxInlierDist&gt;maxInlierDistMax) maxInlierDist = maxInlierDistMax;\n\n     // choose samples along the circle and count inlier percentage\n     for(float t =0; t&lt;2*3.14159265359f; t+= 0.05f)\n     {\n         counter++;\n         float cX = radius*cos(t) + center.x;\n         float cY = radius*sin(t) + center.y;\n\n         if(cX &lt; dt.cols)\n         if(cX &gt;= 0)\n         if(cY &lt; dt.rows)\n         if(cY &gt;= 0)\n         if(dt.at&lt;float&gt;(cY,cX) &lt; maxInlierDist)\n         {\n            inlier++;\n            inlierSet.push_back(cv::Point2f(cX,cY));\n         }\n     }\n\n     return (float)inlier/float(counter);\n    }\n\n    float evaluateCircle(cv::Mat dt, cv::Point2f center, float radius)\n    {\n\n        float completeDistance = 0.0f;\n        int counter = 0;\n\n        float maxDist = 1.0f;   //TODO: this might depend on the size of the circle!\n\n        float minStep = 0.001f;\n        // choose samples along the circle and count inlier percentage\n\n        //HERE IS THE TRICK that no minimum/maximum circle is used, the number of generated points along the circle depends on the radius.\n        // if this is too slow for you (e.g. too many points created for each circle), increase the step parameter, but only by factor so that it still depends on the radius\n\n        // the parameter step depends on the circle size, otherwise small circles will create more inlier on the circle\n        float step = 2*3.14159265359f / (6.0f * radius);\n        if(step &lt; minStep) step = minStep; // TODO: find a good value here.\n\n        //for(float t =0; t&lt;2*3.14159265359f; t+= 0.05f) // this one which doesnt depend on the radius, is much worse!\n        for(float t =0; t&lt;2*3.14159265359f; t+= step)\n        {\n            float cX = radius*cos(t) + center.x;\n            float cY = radius*sin(t) + center.y;\n\n            if(cX &lt; dt.cols)\n                if(cX &gt;= 0)\n                    if(cY &lt; dt.rows)\n                        if(cY &gt;= 0)\n                            if(dt.at&lt;float&gt;(cY,cX) &lt;= maxDist)\n                            {\n                                completeDistance += dt.at&lt;float&gt;(cY,cX);\n                                counter++;\n                            }\n\n        }\n\n        return counter;\n    }\n\n\n    int main()\n    {\n    //RANSAC\n\n    cv::Mat color = cv::imread(\"HoughCirclesAccuracy.png\");\n\n    // convert to grayscale\n    cv::Mat gray;\n    cv::cvtColor(color, gray, CV_RGB2GRAY);\n\n    // get binary image\n    cv::Mat mask = gray &gt; 0;\n\n    unsigned int numberOfCirclesToDetect = 2;   // TODO: if unknown, you'll have to find some nice criteria to stop finding more (semi-) circles\n\n    for(unsigned int j=0; j&lt;numberOfCirclesToDetect; ++j)\n    {\n        std::vector&lt;cv::Point2f&gt; edgePositions;\n        edgePositions = getPointPositions(mask);\n\n        std::cout &lt;&lt; \"number of edge positions: \" &lt;&lt; edgePositions.size() &lt;&lt; std::endl;\n\n        // create distance transform to efficiently evaluate distance to nearest edge\n        cv::Mat dt;\n        cv::distanceTransform(255-mask, dt,CV_DIST_L1, 3);\n\n\n\n        unsigned int nIterations = 0;\n\n        cv::Point2f bestCircleCenter;\n        float bestCircleRadius;\n        //float bestCVal = FLT_MAX;\n        float bestCVal = -1;\n\n        //float minCircleRadius = 20.0f; // TODO: if you have some knowledge about your image you might be able to adjust the minimum circle radius parameter.\n        float minCircleRadius = 0.0f;\n\n        //TODO: implement some more intelligent ransac without fixed number of iterations\n        for(unsigned int i=0; i&lt;2000; ++i)\n        {\n            //RANSAC: randomly choose 3 point and create a circle:\n            //TODO: choose randomly but more intelligent,\n            //so that it is more likely to choose three points of a circle.\n            //For example if there are many small circles, it is unlikely to randomly choose 3 points of the same circle.\n            unsigned int idx1 = rand()%edgePositions.size();\n            unsigned int idx2 = rand()%edgePositions.size();\n            unsigned int idx3 = rand()%edgePositions.size();\n\n            // we need 3 different samples:\n            if(idx1 == idx2) continue;\n            if(idx1 == idx3) continue;\n            if(idx3 == idx2) continue;\n\n            // create circle from 3 points:\n            cv::Point2f center; float radius;\n            getCircle(edgePositions[idx1],edgePositions[idx2],edgePositions[idx3],center,radius);\n\n            if(radius &lt; minCircleRadius)continue;\n\n\n            //verify or falsify the circle by inlier counting:\n            //float cPerc = verifyCircle(dt,center,radius, inlierSet);\n            float cVal = evaluateCircle(dt,center,radius);\n\n            if(cVal &gt; bestCVal)\n            {\n                bestCVal = cVal;\n                bestCircleRadius = radius;\n                bestCircleCenter = center;\n            }\n\n            ++nIterations;\n        }\n        std::cout &lt;&lt; \"current best circle: \" &lt;&lt; bestCircleCenter &lt;&lt; \" with radius: \" &lt;&lt; bestCircleRadius &lt;&lt; \" and nInlier \" &lt;&lt; bestCVal &lt;&lt; std::endl;\n        cv::circle(color,bestCircleCenter,bestCircleRadius,cv::Scalar(0,0,255));\n\n        //TODO: hold and save the detected circle.\n\n        //TODO: instead of overwriting the mask with a drawn circle it might be better to hold and ignore detected circles and dont count new circles which are too close to the old one.\n        // in this current version the chosen radius to overwrite the mask is fixed and might remove parts of other circles too!\n\n        // update mask: remove the detected circle!\n        cv::circle(mask,bestCircleCenter, bestCircleRadius, 0, 10); // here the radius is fixed which isnt so nice.\n    }\n\n    cv::namedWindow(\"edges\"); cv::imshow(\"edges\", mask);\n    cv::namedWindow(\"color\"); cv::imshow(\"color\", color);\n\n    cv::imwrite(\"detectedCircles.png\", color);\n    cv::waitKey(-1);\n    return 0;\n    }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Extracting Stone from Image",
        "input": "",
        "output": "<p>you could try this, <a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html\" rel=\"nofollow\">http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html</a></p>\n\n<p>and then choose a threshold what you desire, it is very good for grouping and segmentation, code is also publicly available. </p>\n\n<p>It may be better if you place the stone on some other surface which is of a different color, may be placing a white sheet below would help(if that is possible)</p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab: translation vector in the &#39;extrinsics&#39; function",
        "input": "",
        "output": "<p>I would rather use the Hartley and Zisserman de facto way of defining/understanding this more precisely. Such a definition underpins the Matlab library (indeed, see the references at the end of your Matlab reference URL!).</p>\n\n<p>Thus, define the image point x and the scene point X, such that:</p>\n\n<pre><code>x = P X\n</code></pre>\n\n<p>Where P is the projection matrix that incorporates the camera calibration matrix K (intrinsics) and the external transformation matrix (extrinsics) [R | t], such that:</p>\n\n<pre><code>P = K [R | t]\n</code></pre>\n\n<p>Here is the same in a <a href=\"http://users.cecs.anu.edu.au/~hartley/Papers/CVPR99-tutorial/tut_4up.pdf\" rel=\"nofollow\">free tutorial paper</a>. Notice the first couple of slides.</p>\n\n<p>I hope that helps!</p>\n",
        "system": ""
    },
    {
        "instruction": "face recognition using PCA-matlab",
        "input": "",
        "output": "<p>Please see the below link:</p>\n\n<p><a href=\"http://www.cs.ait.ac.th/~mdailey/matlab/\" rel=\"nofollow\">http://www.cs.ait.ac.th/~mdailey/matlab/</a></p>\n\n<p>It explains step by step applying PCA on images for face recognition including matlab code. </p>\n",
        "system": ""
    },
    {
        "instruction": "Optical Flow class in opencv(CalcOpticalFlowPyrLK) Parameters",
        "input": "",
        "output": "<p>The connection of these two values is that you first have to check the status array. The status array says if a feature point was found or not. This \"check\" has a higher priority then the error. The documentation says: \"if the flow wasn\u2019t found then the error is not defined\". But the memory has to get alloceted anyway to keep the index correspondences. In C++ you have to initalize values, if this not happend there will be some random value. So I think OpenCV is not fill the array with zeros or something before. So that is why it can happend that the error is really small, but there is no status bit!</p>\n\n<p><a href=\"https://stackoverflow.com/a/14367563/3578196\">Here is some explenation how it Lucas-Kanada Method works.</a></p>\n\n<p>When I was implementing Optical Flow in OpenCV i found <a href=\"https://github.com/royshil/SfM-Toy-Library/blob/master/SfMToyLib/OFFeatureMatcher.cpp\" rel=\"nofollow noreferrer\">this</a> nice link. </p>\n\n<p>Also the book <a href=\"https://www.packtpub.com/application-development/mastering-opencv-practical-computer-vision-projects\" rel=\"nofollow noreferrer\">Mastering OpenCV with Practical Computer Vision Projects</a> has a usefull chapter about that.</p>\n",
        "system": ""
    },
    {
        "instruction": "Class aware visual dictionary creation",
        "input": "",
        "output": "<p>As @old-ufo commented, you simply train two dictionaries.  If you want k-dimensional feature vector, you train 2 * k/2 dictionaries, one for each class.  To put into an SVM vector for training your classifier, you build two histograms of k /2 dimensions, one from each dictionary, and then you concatenate them into one feature vector of size k.  </p>\n\n<p>So in OpenCV just do the same as you have already done, but do it twice, once for each class, so you have two dictionaries: bowDE1 and bowDE2. Then you calculate the descriptor for each one, and then concatenate them into a single descriptor for svm training.  </p>\n\n<p>Cheers</p>\n",
        "system": ""
    },
    {
        "instruction": "Essential Matrix from Fundamental Matrix in OpenCV",
        "input": "",
        "output": "<p>I don't know where you got that formulae, but the correct one is\n<code>E = K'^T . F . K</code>  (see Hartley &amp; Zisserman, \u00a79.6, page 257 of second edition)</p>\n\n<p><code>K</code> is the intrinsic camera parameters, holding scale factors and positions of the center of the image, expressed in pixel units.</p>\n\n<pre><code>    | \\alpha_u     0     u_0 |\nK = |    0      \\alpha_u v_0 |\n    |    0         0      1  |\n</code></pre>\n\n<p>(sorry, Latex not supported on SO)</p>\n\n<p><strong>Edit</strong> : To get those values, you can either:</p>\n\n<ul>\n<li>calibrate the camera</li>\n<li>compute an approximate value if you have the manufacturer data. If the lens is correctly centered on the sensor, then u_0 and v_0 are the half of, respectively, width and height of image resolution. And  <code>alpha = k.f</code> with f: focal length (m.), and k the pixel scale factor: if you have a pixel of, say, 6 um, then <code>k=1/6um</code>.\nExample, if the lens is 8mm and pixel size 8um, then <code>alpha=1000</code></li>\n</ul>\n\n<p><strong>Computing E</strong></p>\n\n<p>Sure, there are several of ways to compute E, for example, if you have strong-calibrated the rig of cameras, then you can extract R and t (rotation matrix and translation vector) between the two cameras, and E is defined as the product of the skew-symmetric matrix t and the matrix R.</p>\n\n<p>But if you have the book, all of this is inside.</p>\n\n<p><strong>Edit</strong> Just noticed, there is even a <a href=\"http://en.wikipedia.org/wiki/Essential_matrix\" rel=\"noreferrer\">Wikipedia page</a> on this topic!</p>\n",
        "system": ""
    },
    {
        "instruction": "directional edge detection in OpenCV",
        "input": "",
        "output": "<p>1) \n Canny edge detector produces thin edges because of non-maxima supression along the neighbours.\nIn order to mimic that, you need to choose edge pixels with maximimum edge response along that direction. So blobs of points can be prevented this way.</p>\n\n<p>As you can probably guess, the weaker images in the grid can be suppressed with threshold defined by you.</p>\n\n<p>2) I can't give a definite answer to that sadly. For the angel given, the kernels might be limited by\ndiscretization. So for many different angles, this approach 'should'  be better. </p>\n",
        "system": ""
    },
    {
        "instruction": "Assign specific RGB colours to 3d mesh/surface/points",
        "input": "",
        "output": "<p>I would suggest this:</p>\n\n<pre><code>n=50000; % chose something appropriate\n[C,map] = rgb2ind(FaceImageRGB,n);\n</code></pre>\n\n<p>To map the color in your RGB image into a linear index. <strong>Make sure the mesh and the RGB image have the same x-y dimensions</strong>.</p>\n\n<p>Then use <code>surf</code> to plot the surface with the indexed values for color (should be in the form <code>surf(X,Y,Z,C)</code>) and the <code>map</code> as color map.</p>\n\n<pre><code>surf(3dmesh, C), shading flat;\ncolormap(map);\n</code></pre>\n\n<p><strong>Edit:</strong> a working example (with a <em>colorful</em> image this time...):</p>\n\n<pre><code>rgbim=imread('http://upload.wikimedia.org/wikipedia/commons/0/0d/Loriculus_vernalis_-Ganeshgudi,_Karnataka,_India_-male-8-1c.jpg');\n\nn=50000; % chose something apropriate\n[C,map] = rgb2ind(rgbim,n);    \n\n% Creation of mesh with the same dimensions as the image:\n[X,Y] = meshgrid(-floor(size(rgbim, 2)/2):floor(size(rgbim, 2)/2), -floor(size(rgbim, 1)/2):floor(size(rgbim, 1)/2));\n\n% An arbitrary function for Z:\nZ=-(X.^2+Y.^2);\n\n% Display the surface with the image as color value:\nsurf(X, Y, Z, double(C)), shading flat\ncolormap(map);\n</code></pre>\n\n<p>Result:</p>\n\n<p><img src=\"https://i.sstatic.net/mg7E1.png\" alt=\"mapped parrot\"></p>\n",
        "system": ""
    },
    {
        "instruction": "How to determine isotropicity of a given edge detector",
        "input": "",
        "output": "<p>Hum, you know, there is this thing called the scientific method: you formulate a hypothesis, design an experiment, perform it, and see how well your hypothesis explains its outcome. </p>\n\n<p>In this particular case, anything wrong with using an appropriate natural or synthetic test image, and see how your detector responds on it? </p>\n\n<p>This may be a good image to start: <a href=\"http://lowcadence.com/wp-content/uploads/2013/08/YinYang.jpg\" rel=\"nofollow\">http://lowcadence.com/wp-content/uploads/2013/08/YinYang.jpg</a>  Don't forget to test at multiple resolutions, and after mirroring it. A light blur may help too.   </p>\n",
        "system": ""
    },
    {
        "instruction": "Arrow recognition in video",
        "input": "",
        "output": "<p>I managed to solve the problem by using canny edge detection and HoughLinesP. The system works pretty well but has a limited rotation range at which it will detect the direction correctly (approx 15 degrees).</p>\n\n<p>basically I first performed colour detection to detect the arrow, then used houghlinesp to find its outline. Out of these lines, I eliminated all those which are horizontal or vertical, leaving just the ones at the tip as shown in red. I then used the end points of each line to determine the direction.</p>\n\n<p><img src=\"https://i.sstatic.net/bHC4w.png\" alt=\"enter image description here\"> </p>\n",
        "system": ""
    },
    {
        "instruction": "What methods and algorithms now using for Naughty Detection?",
        "input": "",
        "output": "<p>Seminal work in this field was done in <a href=\"https://courses.cs.washington.edu/courses/cse455/10au/notes/forsyth.pdf\" rel=\"nofollow\">Fleck, Margaret M., David A. Forsyth, and Chris Bregler. \"Finding naked people.\" Computer Vision\u2014ECCV'96. Springer Berlin Heidelberg, 1996. 593-602.</a>. The approach detects skin-colored regions and then determines whether or not the regions match predefined human shapes. More on their skin detection algorithm here: <a href=\"http://www.cs.hmc.edu/~fleck/naked-skin.html\" rel=\"nofollow\">http://www.cs.hmc.edu/~fleck/naked-skin.html</a> .</p>\n\n<p>More recent papers with summaries of current methods are available: </p>\n\n<ol>\n<li><a href=\"http://iseclab.org/people/cplatzer/papers/sfcs05-platzer.pdf\" rel=\"nofollow\">http://iseclab.org/people/cplatzer/papers/sfcs05-platzer.pdf</a> </li>\n<li><a href=\"http://arxiv.org/abs/1402.5792\" rel=\"nofollow\">http://arxiv.org/abs/1402.5792</a></li>\n</ol>\n\n<p>You may also take a look at: <a href=\"https://stackoverflow.com/questions/713247/what-is-the-best-way-to-programatically-detect-porn-images\">What is the best way to programatically detect porn images?</a></p>\n\n<p><strong>update 2016:</strong> use a convnet. They are far better at building high resolution filters. I wrote about it in more detail here </p>\n\n<ul>\n<li><p><a href=\"http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity/\" rel=\"nofollow\">http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity/</a></p></li>\n<li><p><a href=\"https://www.slideshare.net/mobile/RyanCompton1/what-convnets-look-at-when-they-look-at-nudity\" rel=\"nofollow\">https://www.slideshare.net/mobile/RyanCompton1/what-convnets-look-at-when-they-look-at-nudity</a></p></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Mat::data is zero after image operation",
        "input": "",
        "output": "<p>I suppose you have checked that your image.png (after having been converted to grayscale) contains values higher than 220?</p>\n\n<p>You may want to try to perform <code>cvtColor</code> as a non-in-place operation, as it might not properly support in-place conversion (see <a href=\"https://stackoverflow.com/a/15345586/19254\">https://stackoverflow.com/a/15345586/19254</a>). This you would do for example like this:</p>\n\n<pre><code>Mat dst;\ncvtColor(src, dst, CV_RGB2GRAY);\n</code></pre>\n\n<p>... and then do <code>threshold</code> on <code>dst</code>. Alternatively, you could read the image into memory in grayscale mode to begin with (second parameter to <code>imread</code> can be <code>CV_LOAD_IMAGE_GRAYSCALE</code>), in which case the <code>cvtColor</code> call is not necessary.</p>\n\n<p>Between each call, you can <code>imwrite</code> your image to disk in order to find out which step is not working as expected.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to estimate/determine surface normals and tangent planes at points of a depth image?",
        "input": "",
        "output": "<p>So there are a couple of things that are undefined in your question, but I'll do my best to outline an answer.</p>\n\n<p>The basic idea for what you want to do is to take the gradient of the image, and then apply a transformation to the gradient to get the normal vectors. Taking the gradient in matlab is easy:</p>\n\n<pre><code>[m, g] = imgradient(d);\n</code></pre>\n\n<p>gives us the magnitude (<code>m</code>) and the direction (<code>g</code>) of the gradient (relative to the horizontal and measured in degrees) of the image at every point. For instance, if we display the magnitude of the gradient for your image it looks like this:</p>\n\n<p><img src=\"https://i.sstatic.net/1Lcrq.png\" alt=\"enter image description here\"></p>\n\n<p>Now, the harder part is to take this information we have about the gradient and turn it into a normal vector. In order to do this properly we need to know how to transform from image coordinates to world coordinates. For a CAD-generated image like yours, this information is contained in the projection transformation used to make the image. For a real-world image like one you'd get from a Kinect, you would have to look up the spec for the image-capture device. </p>\n\n<p>The key piece of information we need is this: just how wide is each pixel in real-world coordinates? For non-orthonormal projections (like those used by real-world image capture devices) we can approximate this by assuming each pixel represents light within a fixed angle of the real world. If we know this angle (call it <code>p</code> and measure it in radians), then the real-world distance covered by a pixel is just <code>sin(p) .* d</code>, or approximately <code>p .* d</code> where <code>d</code> is the depth of the image at each pixel.</p>\n\n<p>Now if we have this info, we can construct the 3 components of the normal vectors:</p>\n\n<pre><code>width = p .* d;\ngradx = m .* cos(g) * width;\ngrady = m .* sin(g) * width;\n\nnormx = - gradx;\nnormy = - grady;\nnormz = 1;\n\nlen = sqrt(normx .^ 2 + normy .^ 2 + normz .^ 2);\nx = normx ./ len;\ny = normy ./ len;\nz = normz ./ len;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "General Tips to Improve Haar Vision Training Data Results [openCV]",
        "input": "",
        "output": "<p>From my experience based on training hand(palm) detector based on haar features, first thing you have to ask yourself is <em>whether the objects I would like to detect is susceptible to haar features detection</em>. Before I explain this let me remind what haar features actually are - they are just (or at least) able to detect <em>gradient change features</em> - by those I mean e.g. any kind of vertical and horizontal line combinations (single line, lines between two others etc.) And yes, that's basically it. Gradient change. If you think about it, some objects have such feature that there exists some kind of substructure in them that makes them easy to learn which haar features \"exists\" in the objects. For example, human's face is the best known one. It has several keypoints that might and are considered as an existence of haar featurtes. And that's the description of what I called <em>susceptible to haar features</em>. You have to answer this question before you start to train classifier. If the answer is negative, just don't try to find another solution. </p>\n\n<p>For example, I considered detection open palm gesture. I thought what kind of gesture can be detected by haar method. I thought one solution will be to restrict gesture to palm with open fingers, straight,crowded. This simple trick leads to better haar detection and learning because of existence of additional \"edges\" between fingers that touch each other. So apart from these edges. And that edges are visible even though the background is similar to the skin color. And I trained classifier, based mainly of those edges between fingers (because you cannot rely on edges around the hand, because it depends of the background - in some cases those edges can even disappear). I used this command:</p>\n\n<blockquote>\n  <p>opencv_traincascade -data HAAR -vec samples.vec -bg negative.dat\n  -numStages 20 -minHitRate 0.999 -numPos 700 -numNeg 2500 -w 20 -h 11 -mode ALL -featureType HAAR</p>\n</blockquote>\n\n<p>As you can see, I used 700 positive and 2500 negative samples. Please notice -w and -h parameters. It is a size of detector  windows. All cropped regions from positive samples are scaled to this size, so remember that the regions you crop must  have the same aspect ratio. Finally, I successfully created a haar classifier able to detect specific hand gesture - palm with fingers open, straight, and packed next to each other.</p>\n\n<p><strong>EDIT:</strong> You can check log from training <a href=\"http://pastebin.com/dr5cfX29\" rel=\"noreferrer\">here</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculate similarity score between scene and template object",
        "input": "",
        "output": "<p>A similarity score is greater if the object and scene are more similar (as opposed to a dissimilarity score, where a higher score means they are more dissimilar). Since you are using distances with FLANN (which I assume is giving you approximate euclidean distances between descriptors) a dissimilarity score is easier to generate, since euclidean distance is greater if descriptors are further apart in the descriptor space, and small if they are close together.</p>\n\n<p>One simple way to generate a dissimilarity score would be to: \n1. For each descriptor in the object image: calculate the minimum distance to each descriptor in the scene image. \n2. Sum the (minimum) distances, and normalize by the number of descriptors in the object image. </p>\n\n<p>Then you will have a single score quantifying the match between the object and the scene. </p>\n",
        "system": ""
    },
    {
        "instruction": "Some function of opencv showing error on some specific scenario",
        "input": "",
        "output": "<p>I had the same issues. For some reason, ndk and jni doesn't work well with vector indexing. You need to create a temporary instance of that object and assign it and then proceed to do further processing.</p>\n\n<p>I think this will solve your problem.</p>\n\n<pre><code>split(image,ch);\nMat ch0 = ch[0];\nMat ch1 = ch[1];\nMat ch2 = ch[2];\n\nmultiply(alpha, ch0, ch0);\nmultiply(alpha, ch1, ch1);\nmultiply(alpha, ch2, ch2);\n\nvector&lt;Mat&gt; newVec;\nnewVec.push_back(ch0);\nnewVec.push_back(ch1);\nnewVec.push_back(ch2);\n\nmerge(newVec, image);\n</code></pre>\n\n<p>For resize and clone, you can't use any of these functions on vector index. You have to create a temporary instance of Mat and use cv::functions over it.</p>\n\n<pre><code>Mat alpha1 = ch[3];\nMat alpha =  alpha1.clone();\n</code></pre>\n\n<p>To resize,</p>\n\n<pre><code>Mat alpha1 = ch[3];\nMat alhpa1_resized;\ncv::resize(alpha1, alpha1_resized, cv::Size(new_cols, new_rows));\n\n// or\ncv::resize(alpha1, alpha1_resized, cv::Size(), new_x_scale, new_y_scale);\n</code></pre>\n\n<p>Don't multiply your alpha channel. CH[3]. Keep it as it is and merge. You could also try addWeighted. I think you're doing the same thing.</p>\n",
        "system": ""
    },
    {
        "instruction": "opencv estimateRigidTransform: How to get global scale?",
        "input": "",
        "output": "<p>Ok, after additional search I found a great answer here: <a href=\"https://math.stackexchange.com/questions/13150/extracting-rotation-scale-values-from-2d-transformation-matrix\">https://math.stackexchange.com/questions/13150/extracting-rotation-scale-values-from-2d-transformation-matrix</a></p>\n\n<p><img src=\"https://i.sstatic.net/rhsh8.png\" alt=\"\"></p>\n\n<p>Where the \"zooming\" is represented by s_x and s_y. These two values will mostly be equal (since zoom in video captured by cameras is equal in both directions), so I can take either one.</p>\n",
        "system": ""
    },
    {
        "instruction": "Haar classifier for segmenting lips",
        "input": "",
        "output": "<p>If there are no Haar cascades in the OpenCV distribution that currently implement how to segment out lips, consider building your own.  Take a look at the guide to building Haar cascades from the OpenCV project for more details:</p>\n\n<p><a href=\"http://docs.opencv.org/trunk/doc/user_guide/ug_traincascade.html\" rel=\"nofollow\">http://docs.opencv.org/trunk/doc/user_guide/ug_traincascade.html</a></p>\n\n<p>I am assuming you already have positive and negative examples of what you want to classify, and so you can build your own cascades using your own ground truth data.  The above guide will get you started on creating your own Haar cascades.</p>\n\n<p><strong>NB:</strong> I am usually <em>against</em> deferring people to external links without some sort of closure in my posts, but the process to do this is quite involved, and I can't invest the effort in repeating that information here.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: HSV inRange returns binary image squashed and repeated",
        "input": "",
        "output": "<p>As @Nallath comments, this is apparently a channel issue. According to the documentation, the output of <a href=\"http://docs.opencv.org/modules/core/doc/operations_on_arrays.html?highlight=inrange#inrange\" rel=\"nofollow\"><code>inRange()</code></a> should be a 1-channel <code>CV_8U</code> image which is the logical <code>AND</code> of all channel inclusives.  </p>\n\n<p>Your result means that somewhere along the way <code>threshold</code> is being treated like a 3-channel plane-order image.</p>\n\n<p>What version of OpenCV are you using?  </p>\n\n<p>I suggest that you show <code>threshold</code> between every step to find the place where this conversion happens. This might be a bug that should be <a href=\"http://code.opencv.org/projects/opencv/wiki#Reporting-bugs\" rel=\"nofollow\">reported</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Changing widthstep of image opencv",
        "input": "",
        "output": "<ol>\n<li><p>Since you are using term widthStep I guess you are using IplImage. IplImage was taken from Intel Performance Primitives (IPP) library. In order to have good performance it is required  that widthStep of each row should be multiple of 4. To enforce this condition rows are padded with addition bytes. So as long as you are using IplImage you won't be able to have widthStep equal to 750 which is not multiple of 4.</p></li>\n<li><p>OpenCV 1 was based on IplImage, but OpenCV 2 is based on Mat. Its been a years since IplImage was deprecated.</p></li>\n<li><p>Mat has no such limitation. By default its step will be 750.</p></li>\n</ol>\n\n<p><strong>After edit of the question</strong>:</p>\n\n<p>colRange(1,250) means 249 columns, not 250. Same for rowRange(1,250). When size of the image being copied is different from size of target image, target image is reallocated. But since colRange and rowRange return constant temporary image it can't be reallocated and the program crashes.</p>\n",
        "system": ""
    },
    {
        "instruction": "MATLAB : epipolar lines doesn&#39;t intersect with the points (fundamental matrix)",
        "input": "",
        "output": "<p>This is perfectly normal. Estimation of the fundamental matrix is sensitive to noise, so the corresponding points rarely end up exactly on the epipolar line.  Typically, you introduce some tolerance threshold to eliminate the bad matches that are too far from the corresponding epipolar lines.</p>\n\n<p>You may be able to get a better result by starting out with more good matches. You may want to try a different interest point detector and/or a different descriptor. <code>extractFeatures</code> uses the FREAK descriptor with Harris corners by default. You can make it use a different descriptor by setting the 'Method' parameter.</p>\n",
        "system": ""
    },
    {
        "instruction": "Fit image over a set of points (shape)",
        "input": "",
        "output": "<p>It can be done with the standard 3D technics:</p>\n\n<ol>\n<li><p>Build mesh based on your point cloud. You need something that can be properly warped. For facial animation in CGI they use topology like this one so it can be properly animated(warped): <a href=\"http://www.digitaltutors.com/forum/showthread.php?9365-human-face-topology\" rel=\"nofollow noreferrer\"><img src=\"https://i9.photobucket.com/albums/a84/Monkey_Knife_Fight/EdgeLoop1.jpg\" alt=\"enter image description here\"></a>\nI will use regular mesh just to show core concept: \n<img src=\"https://i.sstatic.net/KnpJK.jpg\" alt=\"enter image description here\"></p></li>\n<li><p>Map your image to this mesh using  <a href=\"http://www.glprogramming.com/red/chapter09.html\" rel=\"nofollow noreferrer\">Texture Mapping</a>\n<img src=\"https://i.sstatic.net/kgS0F.jpg\" alt=\"enter image description here\"></p></li>\n<li><p>Apply warping(in my case it's just bending):\n<img src=\"https://i.sstatic.net/DyZoB.jpg\" alt=\"enter image description here\"> \n<img src=\"https://i.sstatic.net/MJlDM.jpg\" alt=\"enter image description here\"></p>\n\n<p><a href=\"http://polygonspixelsandpaint.tumblr.com/post/5335098335\" rel=\"nofollow noreferrer\">Puppet Warp tool in Photoshop CS5 works in similar way.</a></p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Choosing/Normalizing HoG parameters for object detection?",
        "input": "",
        "output": "<p>As you say, HOG basically uses a parameter that establishes the cell size in pixels. So if the image size changes, then the number of cells is different and then the descriptor is different in size.</p>\n\n<p><a href=\"http://www.researchgate.net/post/HOG_for_images_of_different_sizes2\" rel=\"nofollow\">The main approach</a> is to use HOG is to use windows with the same size in pixels (the same size during training and also during testing). So <code>extracted window</code> should be the same size of <code>trainingsample</code>.</p>\n\n<p>In <a href=\"http://www.researchgate.net/post/HOG_for_images_of_different_sizes2\" rel=\"nofollow\">that</a> reference, one user says:</p>\n\n<blockquote>\n  <p>HOG is not scale invariant. Getting the same length feature vector for\n  each image does not guarantee the scale invariance.</p>\n</blockquote>\n\n<p>So you should use the same window size...</p>\n",
        "system": ""
    },
    {
        "instruction": "Is there any existing (public) source code for large-scale kinect fusion?",
        "input": "",
        "output": "<p>You can have a experimental open source large-scale kinect fusion source code in PCL. they do volume stitch when camera pose crosses some threshold. check <a href=\"http://www.pointclouds.org/documentation/tutorials/using_kinfu_large_scale.php\" rel=\"nofollow\">this</a></p>\n\n<p>and there is actually new version of scalable kinect fusion in MSR, but they haven't yet put it into the SDK, so you can't use it right a way. </p>\n\n<p>They use a hierarchical data structure to store the unbouded reconstruction volume. you can go check <a href=\"http://research.microsoft.com/en-us/projects/scalablefusion/default.aspx\" rel=\"nofollow\">this</a> then download their paper, and implement it by yourself.</p>\n\n<p>============EDIT=============</p>\n\n<p>you can have another code from technology university of Munich, named <a href=\"https://github.com/tum-vision/fastfusion\" rel=\"nofollow\">'fastfusion'</a>. they use a multi-resolution octree to store the voxels, and extract mesh every second in other thread. It uses OpenNI.\nIt doesn't contain camera tracking, but you can use their dvo-slam for visual odometry.</p>\n\n<p>and a recently released project named <a href=\"http://www.robots.ox.ac.uk/~victor/infinitam/\" rel=\"nofollow\">InfiniTAM</a> use Hash table to store the voxels. it can runs on Kinect v2. </p>\n",
        "system": ""
    },
    {
        "instruction": "traincascade, classifiers and openCV",
        "input": "",
        "output": "<p>yes, you can stop it at, say, stage 15.</p>\n\n<p>for testing with the working 14 stages(15 might be broken), you re-send your cmdline with num_stages 14. this will generate a cascade.xml you can use.</p>\n\n<p>later just restart with num_cascades 30 to finish the training. (maybe you have to <em>delete</em> stage 15)</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV background subtraction: How to precompute background model?",
        "input": "",
        "output": "<p>If your moving objects are present right from the start, all updating background estimators will place them in the background initially. A solution to that is to initialize your MOG on all frames and then run MOG again with this initialization (as with your median estimate). Depending on the number of frames you might want to adjust the update parameter of MOG (learningRate) to make sure its fully initialized (if you have 100 frames it probably needs to be higher at least 0.01):</p>\n\n<pre><code>void BackgroundSubtractorMOG::operator()(InputArray image, OutputArray fgmask, double **learningRate**=0)\n</code></pre>\n\n<p>If your moving objects are not present right from the start, make sure that MOG is fully initialized when they appear by setting a high enough value for the update parameter learningRate.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV - GrabCut with custom foreground/background models",
        "input": "",
        "output": "<p>In opencv/sources/modules/imgproc/src/grabcut.cpp you can have a look how the models (GMMs) are encoded:</p>\n\n<pre><code>GMM::GMM( Mat&amp; _model )\n{\n    const int modelSize = 3/*mean*/ + 9/*covariance*/ + 1/*component weight*/;\n    if( _model.empty() )\n    {\n        _model.create( 1, modelSize*componentsCount, CV_64FC1 );\n        _model.setTo(Scalar(0));\n    }\n    else if( (_model.type() != CV_64FC1) || (_model.rows != 1) || (_model.cols != modelSize*componentsCount) )\n        CV_Error( CV_StsBadArg, \"_model must have CV_64FC1 type, rows == 1 and cols == 13*componentsCount\" );\n\n    model = _model;\n\n    coefs = model.ptr&lt;double&gt;(0);\n    mean = coefs + componentsCount;\n    cov = mean + 3*componentsCount;\n\n    for( int ci = 0; ci &lt; componentsCount; ci++ )\n        if( coefs[ci] &gt; 0 )\n             calcInverseCovAndDeterm( ci );\n}\n</code></pre>\n\n<p>So you need for every model a cv::Mat of 1 x 65 doubles (componentsCount equals 5). There are 3 means per component because its computing in RGB colorspace. Using GC_EVAL would indeed leave the models intact but I never tried it with pre-computed models. </p>\n",
        "system": ""
    },
    {
        "instruction": "Finding separate centroids of each white spots in a frame?",
        "input": "",
        "output": "<p>I tested the following code with <a href=\"https://i.sstatic.net/HFOUG.jpg\" rel=\"nofollow noreferrer\">the image you uploaded</a>.</p>\n\n<p>I got the following textual output:</p>\n\n<pre><code>cv2 version: 2.4.4\ncentroids: [(580, 547), (437, 546), (276, 545), (115, 545), (495, 425), (334, 424), (174, 424), (24, 423), (581, 304), (437, 303), (277, 303), (117, 302), (495, 182), (334, 181), (174, 181), (25, 181), (581, 60), (438, 59), (277, 59), (117, 59)]\n</code></pre>\n\n<p>and this image:</p>\n\n<p><img src=\"https://i.sstatic.net/LbaeN.png\" alt=\"enter image description here\"></p>\n\n<pre><code>#!/usr/bin/env python\n\nimport cv2\n\nimg = cv2.imread('HFOUG.jpg',cv2.CV_LOAD_IMAGE_GRAYSCALE)\n_,img = cv2.threshold(img,0,255,cv2.THRESH_OTSU)\nh, w = img.shape[:2]\n\ncontours0, hierarchy = cv2.findContours( img.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\nmoments  = [cv2.moments(cnt) for cnt in contours0]\n# Nota Bene: I rounded the centroids to integer.\ncentroids = [( int(round(m['m10']/m['m00'])),int(round(m['m01']/m['m00'])) ) for m in moments]\n\nprint 'cv2 version:', cv2.__version__\nprint 'centroids:', centroids\n\nfor c in centroids:\n    # I draw a black little empty circle in the centroid position\n    cv2.circle(img,c,5,(0,0,0))\n\ncv2.imshow('image', img)\n0xFF &amp; cv2.waitKey()\ncv2.destroyAllWindows()\n</code></pre>\n\n<p>See also this answer <a href=\"https://stackoverflow.com/a/9059648/15485\">https://stackoverflow.com/a/9059648/15485</a> to the question <a href=\"https://stackoverflow.com/questions/9056646/python-opencv-find-black-areas-in-a-binary-image/\">Python OpenCV - Find black areas in a binary image</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Merging two images showing brightness",
        "input": "",
        "output": "<p>it's me again :) It seems you are writing new photoshop.</p>\n\n<p>The result I've got:\n<img src=\"https://i.sstatic.net/OjejO.jpg\" alt=\"enter image description here\"></p>\n\n<p>The code:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;stdio.h&gt;\n#include &lt;functional&gt;\n#include &lt;algorithm&gt;\n#include &lt;numeric&gt;\n#include &lt;cstddef&gt;\n#include \"opencv2/opencv.hpp\"\n#include &lt;iostream&gt;\n#include &lt;fstream&gt;\n\nusing namespace std;\nusing namespace cv;\n int main( int argc, char** argv )\n{\n    namedWindow(\"Image\");\n\n    Mat Img1=imread(\"Img1.png\",-1);\n    Mat Img2=imread(\"Img2.png\");\n    cv::resize(Img1,Img1,Img2.size());\n    Img1.convertTo(Img1,CV_32FC4,1.0/255.0);\n    Img2.convertTo(Img2,CV_32FC3,1.0/255.0);\n\n    vector&lt;Mat&gt; ch; \n    split(Img1,ch);\n\n    Mat mask = ch[3].clone();              // here's the vignette\n\n    ch.resize(3);\n\n    Mat I1,I2,result;\n\n    cv::multiply(mask,ch[0],ch[0]);\n    cv::multiply(mask,ch[1],ch[1]);\n    cv::multiply(mask,ch[2],ch[2]);\n    merge(ch,I1);\n\n    vector&lt;Mat&gt; ch2(3);\n    split(Img2,ch2);\n    cv::multiply(1.0-mask,ch2[0],ch2[0]);\n    cv::multiply(1.0-mask,ch2[1],ch2[1]);\n    cv::multiply(1.0-mask,ch2[2],ch2[2]);\n    merge(ch2,I2);\n\n    result=I1+I2;\n\n    imshow(\"Image\",result);\n    waitKey(0);\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to make proper design/architecture of partially reusable algorithm?",
        "input": "",
        "output": "<p>I'd just use some tried and true design patterns.</p>\n\n<p>Use a <a href=\"http://en.wikipedia.org/wiki/Strategy_pattern\" rel=\"nofollow\">strategy pattern</a> to represent an algorithm that you may wish to swap out for alternatives.</p>\n\n<p>Use a <a href=\"http://en.wikipedia.org/wiki/Factory_%28object-oriented_programming%29\" rel=\"nofollow\">factory</a> to instantiate different algorithm (strategy) instances based on some input parameter or runtime context - I'm a fan of the <a href=\"http://en.wikipedia.org/wiki/Prototype_pattern\" rel=\"nofollow\">prototype factory</a> where you have \"inert\" instances of each object in some lookup table, and based on a key you pass in you can request a clone of the one needed. I like it mainly because it's easiest to extend - you can even add new configured prototype instances to such a factory at runtime.</p>\n\n<p>Note that the same \"strategy\" model does not have to serve for everything - it sounds like you might have some higher-level/fuzzy operations which then assemble or chain together low-level/detailed operations. The high level operations could be one type of abstract object while the detailed algorithms are the more concrete strategy instances.</p>\n\n<p>As far as the inputs to the various algorithms, if it varies a lot from algorithm to algorithm you could use an extensible object like a dictionary for parameters so that each algorithm can use just the parameters it needs and ignore the others for an operation. If the dictionary is modifiable during the operation this would also permit upstream algorithms to add parameters for downstream algorithms. Key-value pairs are pretty easy to dump to a log or view in a debugger. </p>\n\n<p>If each strategy instance has a unique semantic identifier you could easily debug the algorithms that get instantiated and chained together. (I use an audio DSP library that has a function to dump a description of the whole chain of configured audio processors, it's very handy).</p>\n\n<p>If you use a system with strategy patterns and extensible parameters you should also be able to segregate shared algorithms from application-specific algorithms, but still have the same basic framework for instantiating and running them.</p>\n\n<p>hth</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding keypoints in a 2d profile",
        "input": "",
        "output": "<p>I'm not sure if this helps, but I quickly converted both images to 1-D functions by finding the index of the maximum value in each line (blue and red).\nOn the same figure I show the derivatives (cyan and green).</p>\n\n<p><img src=\"https://i.sstatic.net/P99LV.png\" alt=\"enter image description here\"></p>\n\n<p>At least it seems your points could perhaps be found by using 1-D template matching, smoothing and/or looking at the derivative.</p>\n",
        "system": ""
    },
    {
        "instruction": "ASSERTION FAILED ERROR",
        "input": "",
        "output": "<p><code>Mat imread(const string&amp; filename, int flags=1)</code>, what is the call you're using in the line <code>Mat Previous_Gray=imread(\"image2.png\");</code> returns an 3 channel rgb image. So beacuse part of the assertion </p>\n\n<blockquote>\n  <p>prev0.channels() == next0.channels() == 1</p>\n</blockquote>\n\n<p>is false. Did you forget to convert PreviousGray <em>before</em> call calcOpticalFlowFarneback?</p>\n\n<p><a href=\"http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imread\" rel=\"nofollow\">http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imread</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Up to scale in 3D triangulation using epipolar geometry",
        "input": "",
        "output": "<p>You are likely to have a bug or a poorly estimated essential matrix. The unknown scale factor cannot be responsible for the reconstruction errors you see. Regardless of global scale, the result of projecting onto the image pair a 3d point estimated from good matches and with a valid essential matrix should be consistent.</p>\n<p>The meaning of &quot;up to scale&quot; in this context is that, even with intrinsically calibrated cameras, the standard method of estimating the essential matrix yields the same result if you replace your scene with one in which everything is larger or smaller by the same amount. You can remove this ambiguity either</p>\n<ul>\n<li>Before the fact, by calibrating the camera extrinsic parameters, i.e., the location and orientation of one camera with respect to the the other) using a method that fixes the scale. For example, using a calibration object of known shape and size.</li>\n<li>After the fact, at stereo reconstruction time. For example, by identifying in the scene an object of known physical size, and imposing that your computed 3d reconstruction matches that size.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "MSER Output as Binary Image",
        "input": "",
        "output": "<p>The result of the MSER is points, not the contours, so you can draw the all points on a black image to get the binary image.</p>\n\n<p>Refer the code from github <a href=\"https://github.com/Palethorn/Feature-extraction/blob/5ad0abfb25dc7cf80de1e9083cc955ebb78c8af0/blob_detectors/mser.cpp\" rel=\"nofollow\">https://github.com/Palethorn/Feature-extraction/blob/5ad0abfb25dc7cf80de1e9083cc955ebb78c8af0/blob_detectors/mser.cpp</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Build 3D surface from one 2D top-down image surface",
        "input": "",
        "output": "<p>Read in the image: </p>\n\n<pre><code>A = imread(filename)\n</code></pre>\n\n<p>Plot the surface plot using the magnitude of the value read in for each x and y from the file:</p>\n\n<pre><code>surf(A)\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Normalized distance between 3d/2d points",
        "input": "",
        "output": "<p>You can calculate a normalized Euclidean distance between vectors <strong>x</strong>, <strong>y</strong> (from same distribution) with:</p>\n\n<p><img src=\"https://i.sstatic.net/gZGaw.png\" alt=\"enter image description here\"></p>\n\n<p>where <strong>s</strong> is a standard deviation of <strong>x</strong> and <strong>y</strong>. In general you can look at <a href=\"http://en.wikipedia.org/wiki/Mahalanobis_distance\" rel=\"nofollow noreferrer\">Mahalanobis distance</a></p>\n",
        "system": ""
    },
    {
        "instruction": "composition of 3D rigid transformations",
        "input": "",
        "output": "<p>This is a simple matrix block-multiplication problem, but you have to remember that you are actually considering 4x4 matrices (associated to rigid transformations in the 3D homogeneous world) and not 3x4 matrices.</p>\n\n<p><strong>Your 3x4 RT matrices actually correspond to the first three rows of 4x4 matrices A, whose last rows are [0, 0, 0, 1]</strong>:</p>\n\n<p>RT23 -> A23=[R23, T23; 0, 0, 0, 1]</p>\n\n<p>RT12 -> A12=[R12, T12; 0, 0, 0, 1]</p>\n\n<p>Then, if you do the matrix block-multiplication on the 4x4 matrices (A13 = A23 * A12), you'll quickly find out that:</p>\n\n<p><strong>R13 = R23 * R12</strong></p>\n\n<p><strong>T13 = R23 * T12 + T23</strong></p>\n",
        "system": ""
    },
    {
        "instruction": "Is there a function for solving xA=b in opencv?",
        "input": "",
        "output": "<p><strong>Method using matrix inversion:</strong> </p>\n\n<p>If xA = b, so then x * A * inv(A) = b * inv(A) &lt;=> x = b * inv(A) &lt;=>, so in opencv you would have: </p>\n\n<pre><code>void mrdivide(const cv::Mat &amp;A, const cv::Mat &amp;b, cv::Mat &amp;x) {\n    x = b * A.inv();\n}\n</code></pre>\n\n<p>However, this is not recommended due to high cost of matrix inversion and from numerical methodology point of view, loss of accuracy. Furthermore, it can solve only system of linear equations which is not overdefined and system matrix A must be invertible. </p>\n\n<p><strong>Methods using matrix transposition:</strong> </p>\n\n<p>Since we have xA = b, so (xA)^T = b^T &lt;=> A^T * x^T = b^T, so we can use cv::solve(A.t(), b.t(), x), and x.t() is an result:</p>\n\n<pre><code>void mrdivide(const cv::Mat &amp;A, const cv::Mat &amp;b, cv::Mat &amp;x) {\n     cv::solve(A.t(), b.t(), x);\n     cv::transpose(x,x);\n}\n</code></pre>\n\n<p>This is general and recommended solution. It provides all functionality that solve() does. </p>\n",
        "system": ""
    },
    {
        "instruction": "Training and classifier computer vision",
        "input": "",
        "output": "<p>In machine learning, you have some sort of learning algorithm. A learning algorithm takes some data, called the training set, and produces a model of the world from that. So suppose you had a computer vision learning algorithm that attempted to classify images into two categories: it's a picture of a face, or it's not a picture of face.</p>\n\n<p>To keep things simple, just assume the learning algorithm is very stupid - you feed it pictures of faces marked \"face\" and pictures of things that don't have faces marked \"not a face.\" Our dumb algorithm then just calculates the average light intensity of pictures marked \"face\" and produces a model that says \"if the average intensity of a picture is closer to the average intensity of pictures I previously saw marked \"face\" than pictures I previously saw that were marked \"not a face\", then predict that the picture I'm being shown is a face.\"</p>\n\n<p>The pictures you had to show it to calculate the average light intensity of images of faces as well as pictures marked \"not a face\" is the training set.</p>\n\n<p>The training set is contrasted with the testing set. It's not very impressive for an algorithm to produce a model that tells you a picture is of a face if you've already shown it that picture before. So usually when you have data you \"hold out\" (or set aside) a small portion to evaluate how good the model actually is.</p>\n\n<p><strong>The basic process of a machine learning task breaks down as follows:</strong></p>\n\n<ol>\n<li>Obtain, prepare, and format data so that it can be used as input for a model.</li>\n<li>Through random choice, set aside some portion of the data to be designated as the \"training set\" and the other as the \"testing set.\" Usually like 10% or so is set aside for testing.</li>\n<li>Apply the learning algorithm to the training set.</li>\n<li>The output of a learning algorithm is a model which accepts input and produces some output given that input. In a computer vision context, the input is almost always going to be a picture, but the output could be a classification, perhaps a 3-D map, maybe it finds certain things in the picture; it depends on what you're trying to do.</li>\n<li>Determine the accuracy of the model by feeding it the data it's never seen before (i.e. the testing set) and compare the output of the model with what you know the output should be.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Active Shape Models&#39; fitting procedure doesn&#39;t converge with Statistical Model fitting function",
        "input": "",
        "output": "<p>The gradient image is wrong or better is not of any use because one needs to take the derivative along the profile normals instead of the horizontal and vertical direction.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to prepare testing data for object detection?",
        "input": "",
        "output": "<p>The answer depends of the method you are going to use. One of them you've proposed, and in that case I would put an fixed possible error rate for bad detecting case which, if the classifier result lays inside the error interval for one of the examples from data test file, is considered as a proper classified example. Of course this fixed error rate should be as small as not to \"overdetect\" in the data set.</p>\n\n<p>I would suggest trying <a href=\"http://research.cs.tamu.edu/prism/lectures/iss/iss_l13.pdf\" rel=\"nofollow\">cross-validation</a> as a technique to test classifier. From data set, it chooses some vectors (images) as a test set and the rest of them as a training set. Repeat few times and average the errors resulting in estimated classificator error. And you don't have to have separate data test set and you don't have to worry about the problem you stated.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculating the Area of the Hand using Depth Data",
        "input": "",
        "output": "<p>There are events from KinectInteractions that detect whenever the fist is in <a href=\"http://msdn.microsoft.com/en-us/library/dn188673.aspx\" rel=\"nofollow noreferrer\">gripped mode or released mode</a> as it's used for the KinectScrollViewer in the KinectRegion:</p>\n\n<p><a href=\"http://msdn.microsoft.com/en-us/library/microsoft.kinect.toolkit.controls.kinectregion.handpointergrip.aspx\" rel=\"nofollow noreferrer\">The HandPointGrip event</a>\n<a href=\"http://msdn.microsoft.com/en-us/library/microsoft.kinect.toolkit.controls.kinectregion.handpointergriprelease.aspx\" rel=\"nofollow noreferrer\">The HandPointGripReleased event</a></p>\n\n<p>Also this might be a duplicate of <a href=\"https://stackoverflow.com/questions/18729142/how-to-detect-open-closed-hand-using-microsoft-kinect-for-windows-sdk-ver-1-7-c\">this post</a> .</p>\n",
        "system": ""
    },
    {
        "instruction": "Using image with rectangle drawn over it in python",
        "input": "",
        "output": "<p>Heh, several years late to help the original poster, but I just hit this problem myself and didn't have control over the image source (so <code>cv2.imread</code> wasn't a great solution for me - I'd've had to have written the image back to disk first).  </p>\n\n<p>I was able to proceed with <code>image = image.copy()</code>.  </p>\n\n<p>Not sure what the root cause was - an equality check showed every value of the array was equivalent, and the array was the correct type.  <code>\u00af\\_(\u30c4)_/\u00af</code></p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: draw a rectangle around a region",
        "input": "",
        "output": "<p>Use cv2:</p>\n<pre><code>import cv2\n\ncv2.rectangle(img, (x1, y1), (x2, y2), color=(255,0,0), thickness=2)\n\n\nx1,y1 ------\n|          |\n|          |\n|          |\n--------x2,y2\n</code></pre>\n<p>[edit] to append the follow-up questions below:</p>\n<pre><code>cv2.imwrite(&quot;my.png&quot;,img)\n\ncv2.imshow(&quot;lalala&quot;, img)\nk = cv2.waitKey(0) # 0==wait forever\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Extract the points that generate the vectors x and y in the video",
        "input": "",
        "output": "<p>The motion vectors are  stored in <code>of</code>, which is a 2-D array of complex numbers. The real part contains the x-coordinates and the imaginary part contains the y-coordinates.</p>\n",
        "system": ""
    },
    {
        "instruction": "MATLAB kmeans not working for SURF/BRISK Points vectors",
        "input": "",
        "output": "<p>The problem here is that <code>BRISKPoints</code> is a MATLAB object, not a numeric matrix. You cannot do k-means on it directly.  What should go into k-means is the output of <code>extractFeatures</code>.  Note that <code>extractFeatures</code> can return either SURF or FREAK descriptors depending on the type of the input points or the value of the <code>'Method'</code> parameter.  You can use k-means to cluster SURF descriptors, which are simply numerical vectors, but not FREAK descriptors, which are strings of bits encapsulated in a <code>binaryFeatures</code> object. </p>\n\n<p>By the way, as of R2014b there is built-in support for the bag of words image classification in the Computer Vision System Toolbox.  Please see this <a href=\"http://www.mathworks.com/help/vision/examples/image-category-classification-using-bag-of-features.html\" rel=\"nofollow\">example</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Sliding a sliding window &quot;intelligently&quot;?",
        "input": "",
        "output": "<p>Have you considered looking at saliency detection algorithms?  Saliency detection algorithms give you an indication of <strong>where</strong> in the image a human would most likely focus on.  A good example would be a human in an open field.  The sky would have low saliency while the human a high one.  </p>\n\n<p>Maybe put your image through a saliency detection algorithm first, then threshold and find regions of where to search instead of the entire image.  </p>\n\n<p>A great algorithm for this is by Stas Goferman: <em>Context-Aware Saliency Detection</em> - <a href=\"http://webee.technion.ac.il/~ayellet/Ps/10-Saliency.pdf\" rel=\"nofollow\">http://webee.technion.ac.il/~ayellet/Ps/10-Saliency.pdf</a>.</p>\n\n<p>There is also code here to get you started: <a href=\"https://sites.google.com/a/jyunfan.co.cc/site/opensource-1/contextsaliency\" rel=\"nofollow\">https://sites.google.com/a/jyunfan.co.cc/site/opensource-1/contextsaliency</a></p>\n\n<p>Unfortunately it is in MATLAB, and from your tag you want to look at Python.  However, there are many similarities between <code>numpy / scipy</code> and MATLAB so hopefully that will help you if you want to transcribe any code.</p>\n\n<p>Check it out!</p>\n",
        "system": ""
    },
    {
        "instruction": "Implementing HoG features detection for sliding window classifier",
        "input": "",
        "output": "<p>Yes, it is as simple as that. Once you have your HOG model and your windows, you anly need to apply the window features to the models. And then select the best result (using a threshold or not, depending on your application).</p>\n\n<p><a href=\"http://thebrainiac1.blogspot.com.es/2012/07/v-behaviorurldefaultvmlo.html\" rel=\"nofollow\">Here</a> you have a sample code that performs the same steps. The key part is the following:</p>\n\n<pre><code>function detect(im,model,wSize)\n   %{\n   this function will take three parameters\n    1.  im      --&gt; Test Image\n    2.  model   --&gt; trained model\n    3.  wStize  --&gt; Size of the window, i.e. [24,32]\n   and draw rectangle on best estimated window\n   %}\n\ntopLeftRow = 1;\ntopLeftCol = 1;\n[bottomRightCol bottomRightRow d] = size(im);\n\nfcount = 1;\n\n% this for loop scan the entire image and extract features for each sliding window\nfor y = topLeftCol:bottomRightCol-wSize(2)   \n    for x = topLeftRow:bottomRightRow-wSize(1)\n        p1 = [x,y];\n        p2 = [x+(wSize(1)-1), y+(wSize(2)-1)];\n        po = [p1; p2];\n        img = imcut(po,im);     \n        featureVector{fcount} = HOG(double(img));\n        boxPoint{fcount} = [x,y];\n        fcount = fcount+1;\n        x = x+1;\n    end\nend\n\nlebel = ones(length(featureVector),1);\nP = cell2mat(featureVector);\n% each row of P' correspond to a window\n[~, predictions] = svmclassify(P',lebel,model); % classifying each window\n\n[a, indx]= max(predictions);\nbBox = cell2mat(boxPoint(indx));\nrectangle('Position',[bBox(1),bBox(2),24,32],'LineWidth',1, 'EdgeColor','r');\nend\n</code></pre>\n\n<p>For each window, the code extracts the HOG descriptor and stores them in <code>featureVector</code>. Then using <code>svmclassify</code> the code detects the windows where the object is present.</p>\n",
        "system": ""
    },
    {
        "instruction": "What should be the kernel size for deviation of 0.5",
        "input": "",
        "output": "<p>The Gaussian formula has no actual endpoint, it goes on to infinity. It's up to you to determine the point at which you can cut it off without affecting the results.</p>\n\n<p>For image processing you can almost certainly cut it off at the point where the formula reaches 1/256 or less. For a sigma of 0.5 this is 1.4833; since the kernel coordinates are whole numbers that means you can truncate to 1. This means the kernel has to go +/- 1 from the center, or 3x3.</p>\n",
        "system": ""
    },
    {
        "instruction": "matlab function &quot;m = size(X,dim)&quot; equivalent in opencv",
        "input": "",
        "output": "<p>Your title pretty much answers how to access the dimension.  You use the <code>size</code> command.  I'm not even sure why this question was even asked.  <a href=\"http://www.mathworks.com/help/matlab/ref/size.html\" rel=\"nofollow\">http://www.mathworks.com/help/matlab/ref/size.html</a>.  Doing a Google search, this was the first link that came up.</p>\n\n<h2>Referring to your code</h2>\n\n<p>You did not read in the image properly.  You are checking the size <strong>of the string</strong> that contains the filename.  You didn't read in the image itself.  Call <code>imread</code> first via:</p>\n\n<pre><code>im = imread('D:\\Proposals\\others\\test_some_title1.jpg');\n</code></pre>\n\n<p>Now do:</p>\n\n<pre><code>top = size(im, 2);\n</code></pre>\n\n<p>The reason why you get a <code>1 x 40</code> size is because your filename string is 40 characters long.</p>\n\n<p>Also, referring to what @nkjt said, you should not shadow over the <code>image</code> command with a variable called <code>image</code>.  <code>image</code> is used to take a matrix and display it on the screen as an image.  Bear in mind this is not the same as <code>imshow</code>.  Suggest you change the variable name to something like <code>im</code> like what I did.</p>\n",
        "system": ""
    },
    {
        "instruction": "using pylab or matplotlib to display an image from file",
        "input": "",
        "output": "<p>You have to use:</p>\n\n<pre><code>pl.imshow(image1, cmap=pl.gray())\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Display image patches in python",
        "input": "",
        "output": "<p>I've figured out the problem.\nIt looks like I didn't need to insist on creating an image from the arrays generated by the sliding window. Treating them like images the way they were was enough.</p>\n\n<pre><code>for w in windows:\n        pl.imshow(w) #no need to use Image.fromarray()\n        pl.show()\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Extracting long lines from images with short lines as noise?",
        "input": "",
        "output": "<p>For detecting straight lines, better use the well known method of <a href=\"http://en.wikipedia.org/wiki/Hough_transform\" rel=\"nofollow noreferrer\">Hough transform</a>.</p>\n\n<pre><code>img = imread('https://i.sstatic.net/V6bDn.png');\nbw = rgb2gray( imcrop( bw, [85 35 390 290] ) ) &gt; 128; % crop and threshold image\n[H,T,R] = hough(bw,'RhoResolution',0.5,'ThetaResolution',0.5);\nP  = houghpeaks(H,5,'threshold',ceil(0.3*max(H(:))),'NHoodSize',[31, 11]);\nlines = houghlines(bw,T,R,P,'FillGap',25,'MinLength',30);\n% display the result\nfigure;imshow(bw);hold on;\nfor k = 1:length(lines), \n    xy = [lines(k).point1; lines(k).point2];\n    plot(xy(:,1),xy(:,2),'LineWidth',2,'Color','green');\nend\n</code></pre>\n\n<p>You'll end up with something like</p>\n\n<p><img src=\"https://i.sstatic.net/exi3k.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "how to obtain the world coordinates of an image",
        "input": "",
        "output": "<p>I was thinking about problem, and came to the result:</p>\n\n<p><strong>You can't</strong> find the object size. The problem is by a single shot, when you have no idea how far the Object is from your camera you can't say something about the size of the object. The calibration just say how far is the image plane from the camera (focal length) and the open angles of the lense. When the focal length changes the calbriation changes too. </p>\n\n<p>But there are some possibiltys:</p>\n\n<p><a href=\"https://stackoverflow.com/questions/9940140/how-to-get-the-real-life-size-of-an-object-from-an-image-when-not-knowing-the-d\">How to get the real life size of an object from an image, when not knowing the distance between object and the camera?</a></p>\n\n<p>So how I understand you can approximate the size of the objects.</p>\n",
        "system": ""
    },
    {
        "instruction": "Computer Vision System Toolbox &amp; GUI",
        "input": "",
        "output": "<p>The only features of the Computer Vision System Toolbox itself that can be considered GUI components are <code>vision.VideoPlayer</code> and <code>vision.DeployableVideoPlayer</code> for playing videos. You can also use functions like <code>insertObjectAnnotation</code>, <code>insertShape</code>, <code>insertMarker</code>, <code>insertText</code> to draw text and graphics into the video frames.</p>\n\n<p>However, there are GUI components in the rest of MATLAB. For playing multiple video streams inside a custom GUI, take a look at this <a href=\"https://www.mathworks.com/help/vision/examples/video-display-in-a-custom-user-interface.html\" rel=\"nofollow\">example</a>   </p>\n",
        "system": ""
    },
    {
        "instruction": "Fast image segmentation algorithms for automatic object extraction",
        "input": "",
        "output": "<p>For speed, I'd quickly segment the image into surface and non-surface (stuff).  So this at least gets you from 24 bits (color?) to 8 bits or even one bit, if you are brave.</p>\n\n<p>From there you need to aggregate and filter the regions into blobs of appropriate size.  For that, I'd try a morphological (or linear) filter that is keyed to a disk that would just fit inside the smallest object of interest.  This would be an opening and a closing.  Perhaps starting with smaller radii for better results.</p>\n\n<p>From there, you should have an image of blobs that can be found and discriminated.  Each blob or region should designate the objects of interest.</p>\n\n<p>Note that if you can get to a 1-bit image, things can go VERY fast.  However, efficient tools that can make use of this data form (8 pixels per character) are often not forthcoming.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to draw curve on control points using opencv",
        "input": "",
        "output": "<p>This code sketch allows you to edit spline with mouse it uses the files from <a href=\"http://www.codeproject.com/Articles/30838/Overhauser-Catmull-Rom-Splines-for-Camera-Animatio\" rel=\"noreferrer\">this link</a> (attach to your project: overhauser.cpp overhauser.hpp and vec3.hpp):</p>\n\n<p>Left mouse button adds/moves a point, right removes.</p>\n\n<p><img src=\"https://i.sstatic.net/W632a.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;stdio.h&gt;\n#include &lt;functional&gt;\n#include &lt;algorithm&gt;\n#include &lt;numeric&gt;\n#include &lt;cstddef&gt;\n#include \"opencv2/opencv.hpp\"\n#include &lt;iostream&gt;\n#include &lt;fstream&gt;\n#include \"overhauser.hpp\"\n\nusing namespace std;\nusing namespace cv;\n\nMat result;\nMat Img;\n\nint current_color=0;\n\nvector&lt;cv::Point2f&gt; pts_red;\nvector&lt;cv::Point2f&gt; pts_green;\nvector&lt;cv::Point2f&gt; pts_blue;\n\nMat curvesImg;\nint selectedPt=-1;\n\nCRSpline* spline_red = 0;\nCRSpline* spline_green = 0;\nCRSpline* spline_blue = 0;\n\nunsigned char LUT_RED[256];\nunsigned char LUT_GREEN[256];\nunsigned char LUT_BLUE[256];\n\n// comparison function:\nbool mycomp (Point2f p1, Point2f p2)\n{\n    return p1.x&lt;p2.x;\n}\n\nfloat dist(Point2f p1,Point2f p2)\n{\n    return sqrt((p1.x-p2.x)*(p1.x-p2.x)+(p1.y-p2.y)*(p1.y-p2.y));\n}\n\nint findNEarestPt(Point2f pt, float maxDist)\n{   \n    vector&lt;Point2f&gt; current_pts_set;\n\n    current_color=0;\n\n    if(pt.x&gt;255 &amp;&amp; pt.x&lt;512)\n    {\n        current_color=1;\n    }\n\n    if(pt.x&gt;=512)\n    {\n        current_color=2;\n    }\n\n    float ptx=pt.x;\n\n    switch(current_color)\n    {\n    case 0:\n        current_pts_set=pts_red;\n        break;\n    case 1:\n        current_pts_set=pts_green;\n        pt.x-=255;\n        break;\n    case 2:\n        current_pts_set=pts_blue;\n        pt.x-=511;\n        break;\n    }\n\n    float minDist=FLT_MAX;\n    int ind=-1;\n    for(int i=0;i&lt;current_pts_set.size();++i)\n    {\n        float d=dist(pt,current_pts_set[i]);\n        if(minDist&gt;d)\n        {\n            ind=i;\n            minDist=d;\n        }\n    }\n    if(minDist&gt;maxDist)\n    {\n        ind=-1;\n    }\n\n    return ind;\n}\n\nfloat F(float t,float x, CRSpline* spline)\n{\n    vec3 rv = spline-&gt;GetInterpolatedSplinePoint(t);\n    return x-rv.x;\n}\n\nfloat solveForX(float x,CRSpline* slpine)\n{\n    float a=-1.0f,b=1.0,c,e=1e-2;\n    c=(a+b)/2;\n    while( (fabs(b-a)&gt;e) &amp;&amp; (F(c,x,slpine)!=0) )\n    {\n        if (F(a,x,slpine)*F(c,x,slpine)&lt;0)\n        {\n            b=c;\n        }\n        else\n        {\n            a=c;\n        }\n        c=(a+b)/2;\n    }\n    return c;\n}\n\n\nint ind=-1;\n\nvoid mouseHandler(int event, int x, int y, int flags, void* param)\n{\n    Point2f m;\n    m.x=x;\n    m.y=y;\n    curvesImg=Scalar(0,0,0);\n\n    switch (event)\n    {\n    case cv::EVENT_RBUTTONDOWN:\n        ind=findNEarestPt(m,5);\n        if (ind==-1)\n        {\n\n        }else\n        {\n            switch(current_color)\n            {\n            case 0:\n                pts_red.erase(pts_red.begin()+ind);\n                break;\n            case 1:\n                pts_green.erase(pts_green.begin()+ind);\n                break;\n            case 2:\n                pts_blue.erase(pts_blue.begin()+ind);\n                break;\n            }\n            ind=-1;\n        }\n        break;\n\n    case cv::EVENT_LBUTTONDOWN:\n        ind=findNEarestPt(m,5);\n        if (ind==-1)\n        {\n            switch(current_color)\n            {\n            case 0:\n                pts_red.push_back(m);\n                selectedPt=pts_red.size()-1;\n                break;\n            case 1:\n\n                pts_green.push_back(Point2f(m.x-255.0,m.y));\n                selectedPt=pts_green.size()-1;\n                break;\n            case 2:\n                pts_blue.push_back(Point2f(m.x-511,m.y));\n                selectedPt=pts_blue.size()-1;\n                break;\n            }\n        }else\n        {\n            selectedPt=ind;\n        }\n        break;\n\n    case cv::EVENT_MOUSEMOVE:\n        if(ind!=-1)\n        {\n            switch(current_color)\n            {\n            case 0:\n                pts_red[selectedPt].x=m.x;\n                pts_red[selectedPt].y=m.y;\n                break;\n            case 1:\n                pts_green[selectedPt].x=m.x-255;\n                pts_green[selectedPt].y=m.y;\n                break;\n            case 2:\n                pts_blue[selectedPt].x=m.x-511;\n                pts_blue[selectedPt].y=m.y;\n                break;\n            }\n        }\n        break;\n    case cv::EVENT_LBUTTONUP:\n        ind=-1;\n        break;\n    }\n\n    std::sort(pts_red.begin(),pts_red.end(),mycomp);\n    if(pts_red.size()&gt;0)\n    {\n        pts_red[pts_red.size()-1].x=255;\n        pts_red[0].x=0;\n    }\n\n    std::sort(pts_green.begin(),pts_green.end(),mycomp);\n    if(pts_green.size()&gt;0)\n    {\n        pts_green[pts_green.size()-1].x=255;\n        pts_green[0].x=0;\n    }\n\n    std::sort(pts_blue.begin(),pts_blue.end(),mycomp);\n    if(pts_blue.size()&gt;0)\n    {\n        pts_blue[pts_blue.size()-1].x=255;\n        pts_blue[0].x=0;\n    }\n\n    for(int i=0;i&lt;pts_red.size();++i)\n    {\n        circle(curvesImg,pts_red[i],5,Scalar(0,0,255),-1,CV_AA);\n    }\n\n    for(int i=0;i&lt;pts_green.size();++i)\n    {\n        circle(curvesImg,Point2f(pts_green[i].x+255,pts_green[i].y),5,Scalar(0,255,0),-1,CV_AA);\n    }\n\n    for(int i=0;i&lt;pts_blue.size();++i)\n    {\n        circle(curvesImg,Point2f(pts_blue[i].x+511,pts_blue[i].y),5,Scalar(255,0,0),-1,CV_AA);\n    }\n\n    if (spline_red) {delete spline_red;}\n    spline_red = new CRSpline();\n\n    if (spline_green) {delete spline_green;}\n    spline_green = new CRSpline();\n\n    if (spline_blue) {delete spline_blue;}\n    spline_blue = new CRSpline();\n\n    for (int i=0;i&lt;pts_red.size();++i)\n    {\n        vec3 v(pts_red[i].x,pts_red[i].y,0);\n        spline_red-&gt;AddSplinePoint(v);\n    }\n\n    for (int i=0;i&lt;pts_green.size();++i)\n    {\n        vec3 v(pts_green[i].x,pts_green[i].y,0);\n        spline_green-&gt;AddSplinePoint(v);\n    }\n\n    for (int i=0;i&lt;pts_blue.size();++i)\n    {\n        vec3 v(pts_blue[i].x,pts_blue[i].y,0);\n        spline_blue-&gt;AddSplinePoint(v);\n    }\n\n    vec3 rv_last(0,0,0);\n    if(pts_red.size()&gt;2)\n    {\n        for(int i=0;i&lt;256;++i)\n        {\n            float t=solveForX(i,spline_red);\n            vec3 rv = spline_red-&gt;GetInterpolatedSplinePoint(t);\n            if(rv.y&gt;255){rv.y=255;}\n            if(rv.y&lt;0){rv.y=0;}\n            unsigned char I=(unsigned char)(rv.y);\n            LUT_RED[i]=255-I;\n            if(i&gt;0)\n            {\n                line(curvesImg,Point(rv.x,rv.y),Point(rv_last.x,rv_last.y),Scalar(0,0,255),1);\n            }\n            rv_last=rv;\n        }\n    }\n    rv_last=vec3(0,0,0);\n    if(pts_green.size()&gt;2)\n    {\n        for(int i=0;i&lt;256;++i)\n        {\n            float t=solveForX(i,spline_green);\n            vec3 rv = spline_green-&gt;GetInterpolatedSplinePoint(t);\n            if(rv.y&gt;255){rv.y=255;}\n            if(rv.y&lt;0){rv.y=0;}\n            unsigned char I=(unsigned char)(rv.y);\n            LUT_GREEN[i]=255-I;\n            if(i&gt;0)\n            {\n                line(curvesImg,Point(rv.x+255,rv.y),Point(rv_last.x+255,rv_last.y),Scalar(0,255,0),1);\n            }\n            rv_last=rv;\n        }\n    }\n    rv_last=vec3(0,0,0);\n    if(pts_blue.size()&gt;2)\n    {\n        for(int i=0;i&lt;256;++i)\n        {\n            float t=solveForX(i,spline_blue);\n            vec3 rv = spline_blue-&gt;GetInterpolatedSplinePoint(t);\n            if(rv.y&gt;255){rv.y=255;}\n            if(rv.y&lt;0){rv.y=0;}\n            unsigned char I=(unsigned char)(rv.y);\n            LUT_BLUE[i]=255-I;\n            if(i&gt;0)\n            {\n                line(curvesImg,Point(rv.x+511,rv.y),Point(rv_last.x+511,rv_last.y),Scalar(255,0,0),1);\n            }\n            rv_last=rv;\n        }\n\n    }\n\n    int cur_col=0;\n\n    if(m.x&gt;255 &amp;&amp; m.x&lt;512)\n    {\n        cur_col=1;\n    }\n\n    if(m.x&gt;=512)\n    {\n        cur_col=2;\n    }\n\n    Scalar col;\n    switch(cur_col)\n    {\n    case 0:\n        col=Scalar(0,0,255);\n        break;\n    case 1:\n        col=Scalar(0,255,0);\n        break;\n    case 2:\n        col=Scalar(255,0,0);\n        break;\n    }\n    line(curvesImg,Point(0,m.y),Point(curvesImg.cols,m.y),col,1);\n    line(curvesImg,Point(m.x,0),Point(m.x,curvesImg.rows),col,1);\n\n    imshow(\"Correction curves\",curvesImg);  \n\n    vector&lt;Mat&gt; ch;\n    cv::split(Img,ch);\n    LUT(ch[0],Mat(256,1,CV_8UC1,LUT_BLUE),ch[0]);\n    LUT(ch[2],Mat(256,1,CV_8UC1,LUT_RED),ch[2]);\n    LUT(ch[1],Mat(256,1,CV_8UC1,LUT_GREEN),ch[1]);\n    cv::merge(ch,result);\n\n    imshow(\"Transformed\",result);   \n}\n// ---------------------------------\n// \n// ---------------------------------\n//==============================================================================\n\nint main( int argc, char** argv )\n{\n\n    for (int i=0;i&lt;256;++i)\n    {\n        LUT_RED[i]=i;\n        LUT_GREEN[i]=i;\n        LUT_BLUE[i]=i;\n    }\n\n    namedWindow(\"Image\");\n    namedWindow(\"Correction curves\");\n    namedWindow(\"Transformed\");\n\n    Img=imread(\"D:\\\\ImagesForTest\\\\lena.jpg\",1);\n\n    imshow(\"Image\",Img);\n\n    curvesImg=Mat::zeros(256,768,CV_8UC3);\n    setMouseCallback(\"Correction curves\", mouseHandler, NULL);\n    waitKey(0);\n\n    getchar();\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Creating targets for Photogrammetry applications",
        "input": "",
        "output": "<p>for sub-mm the targets are usually designed for each scenario. Which is still not obvious to me. </p>\n\n<p>However: If you want to achieve sub-mm accuracy then avoid SIFT and use circular white targets on black background. Something like this: ( <img src=\"https://i.sstatic.net/AIktD.jpg\" alt=\"coded gom white-black targets\"> ). Compared to other targets they are easier to detect and achieve with good algorithms/hardware/viewing angle/light conditions around 1/100px standard deviation for the center. Look for hough transform ellipse detectors. OpenCV seems to have one but I have no experience with this so far.</p>\n",
        "system": ""
    },
    {
        "instruction": "Training SVM with variable sized hog descriptors of training images (MATLAB)",
        "input": "",
        "output": "<p>As lejlot correctly mentioned, SVM cannot be trained with variable length vectors. </p>\n\n<p>You can just normalize image size to one, i.e. 256x256. There are 3 possibilities to do this:</p>\n\n<ol>\n<li>Crop the 256x256 patch around center.</li>\n<li>Resize image to 256x256 discarding original aspect ratio.</li>\n<li>Resize image to 256xM where M &lt; 256 - preserving original aspect ratio. Than add grey stripes at left and right (or top and bottom) to fill image to the 256x256.</li>\n</ol>\n\n<p>All variants are used by different authors and you have to check which one suits your task best.</p>\n",
        "system": ""
    },
    {
        "instruction": "Algorithm for finding weak (smallest diameter) connections in 2D and 3D objects",
        "input": "",
        "output": "<p>What I would do is shrink the data set iteratively (shrinking means deleting points that have neighbours (horizontal or vertical) that do not belong to the object itself). Imagine this like peeling an onion. Everytime the onion get's smaller.</p>\n\n<p>Now the important thing is that the weak connections will become thin (only one pixel wide) first. So you have to check in this iterative shrinking for sudden occurence of thin connections, i.e bridges between two parts. This is done in 2D by going around clockwise the pixel and checking how often you change from object to non-object. If you went more than once to non-object while going around, the pixel is a bridge.</p>\n\n<p>Alternatively and for 3D I would do the shrinking even further and look for separated regions, because while shrinking your object will divide at the weakest connection. How to detect division? By filling. Use any filling algorithm.</p>\n\n<p>How to do this in Matlab? I cannot make a full algorithm now, but you can follow the ideas above and I can recommend the free <a href=\"http://www.diplib.org/\" rel=\"nofollow\">Diplib</a> for Matlab. I know it has shrinking of binary areas inbuilt although also without it is not too difficult.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculating translation value and rotation angle of a rotated 2D image",
        "input": "",
        "output": "<p>This is essentially a <a href=\"http://en.wikipedia.org/wiki/Homography_%28computer_vision%29\" rel=\"noreferrer\">homography recovery</a> problem.  What you are doing is given co-ordinates in one image and the corresponding co-ordinates in the other image, you are trying to recover the combined translation and rotation matrix that was used to warp the points from the one image to the other.</p>\n\n<p>You can essentially combine the rotation and translation into a single matrix by multiplying the two matrices together.  Multiplying is simply compositing the two operations together.  You would this get:</p>\n\n<pre><code>H = [cos(theta) -sin(theta)  tx]\n    [sin(theta) cos(theta)   ty]\n    [    0           0        1]\n</code></pre>\n\n<p>The idea behind this is to find the parameters by minimizing the error through least squares between each pair of points.</p>\n\n<p>Basically, what you want to find is the following relationship:</p>\n\n<pre><code>xi_after = H*xi_before\n</code></pre>\n\n<p><code>H</code> is the combined rotation and translation matrix required to map the co-ordinates from the one image to the other.  <code>H</code> is also a 3 x 3 matrix, and knowing that the lower right entry (row 3, column 3) is 1, it makes things easier.  Also, assuming that your points are in the augmented co-ordinate system, we essentially want to find this relationship for each pair of co-ordinates from the first image <code>(x_i, y_i)</code> to the other <code>(x_i', y_i')</code>:</p>\n\n<pre><code>[p_i*x_i']   [h11 h12 h13]   [x_i]\n[p_i*y_i'] = [h21 h22 h23] * [y_i]\n[  p_i   ]   [h31 h32  1 ]   [ 1 ]\n</code></pre>\n\n<p>The scale of <code>p_i</code> is to account for homography scaling and vanishing points.  Let's perform a matrix-vector multiplication of this equation.  We can ignore the 3rd element as it isn't useful to us (for now):</p>\n\n<pre><code>p_i*x_i' = h11*x_i + h12*y_i + h13\np_i*y_i' = h21*x_i + h22*y_i + h23\n</code></pre>\n\n<p>Now let's take a look at the 3rd element.  We know that <code>p_i = h31*x_i + h32*y_i + 1</code>.  As such, substituting <code>p_i</code> into each of the equations, and rearranging to solve for <code>x_i'</code> and <code>y_i'</code>, we thus get:</p>\n\n<pre><code>x_i' = h11*x_i + h12*y_i + h13 - h31*x_i*x_i' - h32*y_i*x_i'\ny_i' = h21*x_i + h22*y_i + h23 - h31*x_i*y_i' - h32*y_i*y_i'\n</code></pre>\n\n<p>What you have here now are <strong>two</strong> equations for each unique pair of points.  What we can do now is build an over-determined system of equations.  Take each pair and build two equations out of them.  You will then put it into matrix form, i.e.:</p>\n\n<blockquote>\n  <p>Ah = b</p>\n</blockquote>\n\n<p><code>A</code> would be a matrix of coefficients that were built from each set of equations using the co-ordinates from the first image, <code>b</code> would be each pair of points for the second image and <code>h</code> would be the parameters you are solving for.  Ultimately, you are finally solving this linear system of equations reformulated in matrix form:</p>\n\n<p><img src=\"https://i.sstatic.net/j66jX.png\" alt=\"enter image description here\"></p>\n\n<p>You would solve for the vector <code>h</code> which can be performed through least squares.  In MATLAB, you can do this via:</p>\n\n<pre><code>h = A \\ b;\n</code></pre>\n\n<p>A sidenote for you: If the movement between images is truly just a rotation and translation, then h31 and h32 will both be zero after we solve for the parameters.  However, I always like to be thorough and so I will solve for h31 and h32 anyway.</p>\n\n<p><strong>NB:</strong> This method will only work if you have <strong>at least</strong> 4 <em>unique</em> pairs of points.  Because there are 8 parameters to solve for, and there are 2 equations per point, <code>A</code> must have at least a rank of 8 in order for the system to be consistent (if you want to throw in some linear algebra terminology in the loop).  You will not be able to solve this problem if you have less than 4 points.</p>\n\n<p>If you want some MATLAB code, let's assume that your points are stored in <code>sourcePoints</code> and <code>targetPoints</code>.  <code>sourcePoints</code> are from the first image and <code>targetPoints</code> are for the second image.  Obviously, there should be the same number of points between both images.  It is assumed that both <code>sourcePoints</code> and <code>targetPoints</code> are stored as <code>M x 2</code> matrices.  The first columns contain your <code>x</code> co-ordinates while the second columns contain your <code>y</code> co-ordinates.</p>\n\n<pre><code>numPoints = size(sourcePoints, 1);\n\n%// Cast data to double to be sure\nsourcePoints = double(sourcePoints);\ntargetPoints = double(targetPoints);\n\n%//Extract relevant data\nxSource = sourcePoints(:,1);\nySource = sourcePoints(:,2);\nxTarget = targetPoints(:,1);\nyTarget = targetPoints(:,2);\n\n%//Create helper vectors\nvec0 = zeros(numPoints, 1);\nvec1 = ones(numPoints, 1);\n\nxSourcexTarget = -xSource.*xTarget;\nySourcexTarget = -ySource.*xTarget;\nxSourceyTarget = -xSource.*yTarget;\nySourceyTarget = -ySource.*yTarget;\n\n%//Build matrix\nA = [xSource ySource vec1 vec0 vec0 vec0 xSourcexTarget ySourcexTarget; ...\n    vec0 vec0 vec0 xSource ySource vec1 xSourceyTarget ySourceyTarget];\n\n%//Build RHS vector\nb = [xTarget; yTarget];\n\n%//Solve homography by least squares\nh = A \\ b;\n\n%// Reshape to a 3 x 3 matrix (optional)\n%// Must transpose as reshape is performed\n%// in column major format\nh(9) = 1; %// Add in that h33 is 1 before we reshape\nhmatrix = reshape(h, 3, 3)';\n</code></pre>\n\n<p>Once you are finished, you have a combined rotation and translation matrix.  If you want the <code>x</code> and <code>y</code> translations, simply pick off column 3, rows 1 and 2 in <code>hmatrix</code>.  However, we can also work with the vector of <code>h</code> itself, and so h13 would be element 3, and h23 would be element number 6.  If you want the angle of rotation, simply take the appropriate inverse trigonometric function to rows 1, 2 and columns 1, 2.  For the <code>h</code> vector, this would be elements 1, 2, 4 and 5.  There will be a bit of inconsistency depending on which elements you choose as this was solved by least squares.  One way to get a good overall angle would perhaps be to find the angles of all 4 elements then do some sort of average.  Either way, this is a good starting point.</p>\n\n<h2>References</h2>\n\n<p>I learned about homography a while ago through Leow Wee Kheng's Computer Vision course.  What I have told you is based on his slides:  <a href=\"http://www.comp.nus.edu.sg/~cs4243/lecture/camera.pdf\" rel=\"noreferrer\">http://www.comp.nus.edu.sg/~cs4243/lecture/camera.pdf</a>.  Take a look at slides 30-32 if you want to know where I pulled this material from.  However, the MATLAB code I wrote myself :)</p>\n",
        "system": ""
    },
    {
        "instruction": "Structure from Motion, Reconstruct the 3D Point Cloud given 2D Image points correspondence",
        "input": "",
        "output": "<p>It is the right procedure to reconstructe an object. I worked on this topic the last year at a project at our University. The experience I made is that it isn't easy to reconstruct a object by hand moving camera.</p>\n\n<p><strong>Matching</strong></p>\n\n<p>First you have to think about the matching of intereset points. SURF and SIFT are good matching methods for this points. When the object is moving less then 15\u00b0 you can think about to use USURF which is a bit faster then the normal SURF (for more details watch at the SURF paper). In our project we decided to us Optical Flow in OpenCV it looks a bit slower but was more robust about outliers. Your object is only rotating so you can think to use this, too.</p>\n\n<p><strong>Evaluation of Matrix</strong></p>\n\n<p>Next is evaluating your results of the new camera matrix. Do you have a possibility to find out how much the object was rotated (like some step motor or something)? So you can compare your computed results with the steps of the motor. If it is higher then threshold you know the computation was bad. But be carfull the precision of some step motors is not so good, but some experiments could bring more informations about that.</p>\n\n<p><strong>Evaluation of Cloud</strong></p>\n\n<p>There are some nice ways to evaluate the computed cloud. The easiest way is to compute the reprojection error of the cloud. For that you just reverse your reconstruction and look how far the computed images points away from the original corresponding points.\nA other Test is to check if all points are infronte of the camera. By computing it can happend that the points lie infront and behind the camera. I understand it can happend when both camera are to close each other, and the triangulation terminates as well.</p>\n\n<p><strong>First Image Pair</strong></p>\n\n<p>Iam not sure if this step is necessary with a static camera. But first of all we had to calculate a Fundamental matrix. We made the experience to use the Image pair that has the most matches to extract them and use the RANSAC version give the best results. But maybe you can try to place the object so that it has the most Intereset points in the front for the first shot.</p>\n\n<p><strong>Following Image Pairs</strong></p>\n\n<p>What worked really well is to extract the new camera positions from the exisiting point cloud which was computed from old image pairs before. For that you have remember the 2D 3D correspondenc of images before. It is called Perspective\u00adn\u00adPoint\u00a0Camera\u00a0Pose\u00a0Estimation\u00a0(PnP).</p>\n\n<p>At the end we had some good and bad results. It was depending on the scanning object. Here are some papers which helped me:</p>\n\n<p><a href=\"http://phototour.cs.washington.edu/ModelingTheWorld_ijcv07.pdf\" rel=\"nofollow\">Modeling The World</a></p>\n\n<p><a href=\"http://cvg.ethz.ch/mobile/LiveMetric3DReconstructionICCV2013.pdf\" rel=\"nofollow\">Live Metric 3D-Reconstruction</a></p>\n",
        "system": ""
    },
    {
        "instruction": "iPhone 4(S) vs iPad2 computer vision performance problems",
        "input": "",
        "output": "<p>Similar processors, different camera's. The iPad's camera has less MP's than the iPhone's one. On the iPhone, make sure you downsize your image to an acceptable size before processing.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to determinate object position with openCV in the world coordinate",
        "input": "",
        "output": "<p>You can try calibrating with a chessboard, using the functions: findChessboardCorners() and calibrateCamera(). With these functions you can make a coordinate system and find the position of the object, from the image, in the real world. </p>\n",
        "system": ""
    },
    {
        "instruction": "SimpleCV car detection example",
        "input": "",
        "output": "<p>colorDistance appears to return a rotated, flipped image. If you do a quick transform, you can avoid such upsets. e.g.</p>\n\n<pre><code>x,y,w,h   = 470,200,200,200\ncImg      = Image('parking-car.png')\nncImg     = Image('parking-no-car.png')\ncar       = cImg.crop(x,y,w,h)\nncar      = nImg.crop(x,y,w,h)\nycar      = car.colorDistance(Color.YELLOW).rotateRight().flipHorizontal()\nnycar     = ncar.colorDistance(Color.YELLOW).rotateRight().flipHorizontal()\nonly_car  = car - ycar\nnonly_car = ncar - nycar \ncarmc     = only_car.meanColor()\nncarmc    = nonly_car.meanColor()\n\nprint \"yellow car present, mean color:\", carmc\nprint \"no yellow car present, mean color\", ncarmc\n</code></pre>\n\n<p>As to the meancolor being different, I would assume that either the image has been adjusted slightly or the value of Color.YELLOW has changed...</p>\n\n<p>As an aside, if you are comparing two images which have had the colorDistance method called on them (or subtracting one crop from another), then they have both been transformed the same way, so you only need to do the rotateRight().flipHorizontal() on the final image before it is shown (if at all).</p>\n",
        "system": ""
    },
    {
        "instruction": "Computing rotation and translation matrix from 3d points and their 2d correspondences",
        "input": "",
        "output": "<p>If you know or guessed the camera matrix <code>A</code> (and optionally the distortion coefficients), the simplest approach is to use the function <code>cv::solvePnP</code> (<a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?#solvepnp\" rel=\"nofollow\">doc link</a>) or its robust version <code>cv::solvePnPRansac</code> (<a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?#solvepnpransac\" rel=\"nofollow\">doc link</a>).</p>\n\n<p>If you do not know the camera matrix, I don't think you can estimate the rotation matrix <code>R</code> and translation vector <code>t</code>. However, you can estimate <code>A*R</code> and <code>A*t</code> using the Direct Linear Transform (DLT) algorithm, which is explained in Hartley's &amp; Zisserman's book in \u00a77.1 p178. If you denote <code>P = A*[R | t]</code>, then you can estimate P as follows:</p>\n\n<pre><code>cv::Mat_&lt;double&gt; pts_world(npoints,4), pts_image(npoints,3);\n// [...] fill pts_world &amp; pts_image\ncv::Mat_&lt;double&gt; C = cv::Mat_&lt;double&gt;::zeros(3*npoints,12);\nfor(int r=0; r&lt;npoints; ++r)\n{\n    cv::Mat_&lt;double&gt; pt_world_t = pts_world.row(r);\n    double x = pts_image.at&lt;double&gt;(r,0);\n    double y = pts_image.at&lt;double&gt;(r,1);\n    double w = pts_image.at&lt;double&gt;(r,2);\n    C.row(3*r+0).colRange(4,8) = -w*pt_world_t;\n    C.row(3*r+0).colRange(8,12) = y*pt_world_t;\n    C.row(3*r+1).colRange(0,4) = w*pt_world_t;\n    C.row(3*r+1).colRange(8,12) = -x*pt_world_t;\n    C.row(3*r+2).colRange(0,4) = -y*pt_world_t;\n    C.row(3*r+2).colRange(4,8) = x*pt_world_t;\n}\ncv::Mat_&lt;double&gt; P;\ncv::SVD::solveZ(C,P); // P is a 12x1 column vector\nP = P.reshape(1,3); // Reshape P to be a standard 3x4 projection matrix\n</code></pre>\n\n<p>After that, a good idea would be to perform an iterative optimization (e.g. using Levenberg-Marquardt algorithm), in order to minimize the reprojection error.</p>\n",
        "system": ""
    },
    {
        "instruction": "Combine detectors in Bag of visual words",
        "input": "",
        "output": "<p>In bag of words, the number of key points or the descriptor size is irrelevant, once you generate the code book, you get a histogram whose dimensions are dependent on your codebook size. Again, the histogram is normalized, so it does not depend on the number of features detected per image. Suppose you have SIFT and SURF features, all you need to do is generate 2 codebooks and concatenate them to get a feature vector.</p>\n\n<p>A brief overview of the method is mentioned here:\n<a href=\"http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Image rectification for half sphere",
        "input": "",
        "output": "<p>With a single image, it is not possible to get depth (at least accurately), even if your camera is calibrated. If you have a calibrated pair of cameras, after rectification, the disparity would provide you with depth information. I feel that the depth computation would be extremely hard in this case, given the fact that intensity is almost the same at many parts of the bowl, so stereo algorithms may not work well. Assuming you know the camera parameters and somehow manage to get the depth, all you need to compute is the depth of 3 points on the table, which would lie on a plane. Since you would know the exact 3D position for each pixel on the bowl, all you need to do is compute the intersection of the ray which lies on the bowl with the plane, and you will get a projection. It may be a many to one mapping as it is a projection. A more sophisticated approach (to get a one to one map) is, since all points on the bowl lie on a manifold, i.e the inherent degree of freedom is 2, you could compute a 3d -> 2d transformation (isometric embedding, commonly used for the generation of maps, the globe is 3d while a map is 2d) of all points in the 3D space which lie on the manifold (surface of the bowl) using multidimensional scaling.</p>\n",
        "system": ""
    },
    {
        "instruction": "Integrating opencv 3.0.0 in ubuntu for python and c++ and building lib using cmake",
        "input": "",
        "output": "<p>Installation in Linux\nThese steps have been tested for Ubuntu 10.04 but should work with other distros as well.</p>\n\n<p>Required Packages</p>\n\n<pre><code>GCC 4.4.x or later\nCMake 2.8.7 or higher\nGit\nGTK+2.x or higher, including headers (libgtk2.0-dev)\npkg-config\nPython 2.6 or later and Numpy 1.5 or later with developer packages (python-dev, python-numpy)\nffmpeg or libav development packages: libavcodec-dev, libavformat-dev, libswscale-dev\n[optional] libtbb2 libtbb-dev\n[optional] libdc1394 2.x\n[optional] libjpeg-dev, libpng-dev, libtiff-dev, libjasper-dev, libdc1394-22-dev\n</code></pre>\n\n<p>The packages can be installed using a terminal and the following commands or by using Synaptic Manager:</p>\n\n<pre><code>[compiler] sudo apt-get install build-essential\n[required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\n[optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev\n</code></pre>\n\n<p>Getting OpenCV Source Code</p>\n\n<p>You can use the latest stable OpenCV version or you can grab the latest snapshot from our Git repository.</p>\n\n<pre><code>Getting the Latest Stable OpenCV Version\nGo to our downloads page.\nDownload the source archive and unpack it.\nGetting the Cutting-edge OpenCV from the Git Repository\nLaunch Git client and clone OpenCV repository. If you need modules from OpenCV contrib repository then clone it too.\n</code></pre>\n\n<p>For example</p>\n\n<pre><code>cd ~/&lt;my_working_directory&gt;\ngit clone https://github.com/Itseez/opencv.git\ngit clone https://github.com/Itseez/opencv_contrib.git\nBuilding OpenCV from Source Using CMake\n</code></pre>\n\n<p>Create a temporary directory, which we denote as , where you want to put the generated Makefiles, project files as well the object files and output binaries and enter there.</p>\n\n<p>For example</p>\n\n<pre><code>cd ~/opencv\nmkdir build\ncd build\nConfiguring. Run cmake [&lt;some optional parameters&gt;] &lt;path to the OpenCV source directory&gt;\n</code></pre>\n\n<p>For example</p>\n\n<pre><code>cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local ..\nor cmake-gui\n\n\n\n\n\nset full path to OpenCV source code, e.g. /home/user/opencv\nset full path to &lt;cmake_build_dir&gt;, e.g. /home/user/opencv/build\nset optional parameters\nrun: \u201cConfigure\u201d\nrun: \u201cGenerate\u201d\n</code></pre>\n\n<p>Description of some parameters</p>\n\n<pre><code>build type: CMAKE_BUILD_TYPE=Release\\Debug\nto build with modules from opencv_contrib set OPENCV_EXTRA_MODULES_PATH to &lt;path to opencv_contrib/modules/&gt;\nset BUILD_DOCS for building documents\nset BUILD_EXAMPLES to build all examples\n</code></pre>\n\n<p>[optional] Building python. Set the following python parameters:</p>\n\n<pre><code>PYTHON2(3)_EXECUTABLE = &lt;path to python&gt;\nPYTHON_INCLUDE_DIR = /usr/include/python&lt;version&gt;\nPYTHON_INCLUDE_DIR2 = /usr/include/x86_64-linux-gnu/python&lt;version&gt;\nPYTHON_LIBRARY = /usr/lib/x86_64-linux-gnu/libpython&lt;version&gt;.so\nPYTHON2(3)_NUMPY_INCLUDE_DIRS = /usr/lib/python&lt;version&gt;/dist-packages/numpy/core/include/\n[optional] Building java.\n</code></pre>\n\n<p>Unset parameter: BUILD_SHARED_LIBS</p>\n\n<p>It is useful also to unset BUILD_EXAMPLES, BUILD_TESTS, BUILD_PERF_TESTS - as they all will be statically linked with OpenCV and can take a lot of memory.\nBuild. From build directory execute make, recomend to do it in several threads</p>\n\n<p>For example</p>\n\n<pre><code>make -j7 # runs 7 jobs in parallel\n[optional] Building documents. Enter &lt;cmake_build_dir/doc/&gt; and run make with target \u201chtml_docs\u201d\n</code></pre>\n\n<p>For example</p>\n\n<pre><code>cd ~/opencv/build/doc/\nmake -j7 html_docs\n</code></pre>\n\n<p>To install libraries, from build directory execute</p>\n\n<pre><code>sudo make install\n[optional] Running tests\n</code></pre>\n\n<p>Get the required test data from OpenCV extra repository.\nFor example</p>\n\n<pre><code>git clone https://github.com/Itseez/opencv_extra.git\nset OPENCV_TEST_DATA_PATH environment variable to &lt;path to opencv_extra/testdata&gt;.\nexecute tests from build directory.\n</code></pre>\n\n<p>For example</p>\n\n<p><code>&lt;cmake_build_dir&gt;/bin/opencv_test_core</code></p>\n\n<p>Credits: OpenCV Installation page.</p>\n",
        "system": ""
    },
    {
        "instruction": "cv2.threshold() error (-210)",
        "input": "",
        "output": "<p>The OpenCV <a href=\"http://docs.opencv.org/modules/core/doc/utility_and_system_functions_and_macros.html#error\" rel=\"nofollow noreferrer\">error codes</a> can be looked up in <a href=\"https://github.com/opencv/opencv/blob/8f4e5c2fb8d473c69a3b14983583332738d33372/modules/core/include/opencv2/core/types_c.h#L184\" rel=\"nofollow noreferrer\"><code>types_c.h</code></a>.</p>\n\n<p>Error code -210 is defined as:</p>\n\n<pre><code>CV_StsUnsupportedFormat= -210, /**&lt; the data format/type is not supported by the function*/\n</code></pre>\n\n<p>So, you'll need to coerce your image into <code>uint8</code> data type before passing it to <code>cv2.threshold</code>. This can be done with numpy using the <code>astype</code> method:</p>\n\n<pre><code>afterFourier = afterFourier.astype(np.uint8)\n</code></pre>\n\n<p>This will truncate all of the float values in <code>afterFourier</code> to 8 bit values, so you may want to do some scaling/rounding to the array before you do this, depending on your application.</p>\n",
        "system": ""
    },
    {
        "instruction": "Why image is not merging",
        "input": "",
        "output": "<p>As @berak commented, you are not using <a href=\"http://docs.opencv.org/modules/core/doc/basic_structures.html#size\" rel=\"nofollow\"><code>cv::Size</code></a> correctly, whose first parameter is <code>width</code>, and second <code>height</code> (not the other way around).</p>\n\n<p>Change</p>\n\n<pre><code>resize(im2, image, Size(input.rows, input.cols));\n</code></pre>\n\n<p>to</p>\n\n<pre><code>resize(im2, image, Size(input.cols, input.rows));\n</code></pre>\n\n<hr>\n\n<p><strong>Update:</strong> You further need to address <code>image</code> that is passed in to function <code>blending_overlay</code>. It has only one channel, but you're trying to access other channels from it:</p>\n\n<pre><code>float target = (float)img1.at&lt;uchar&gt;(i, 3*j+c)/255.0 ; // img1 = image here\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Image change the shape when displaying",
        "input": "",
        "output": "<p>your vignette is only in the alpha [4th] channel, and also it looks inverted (opacity values here). </p>\n\n<p>(your 1st picture seems to show a proper alpha composite with a white image(or background), that's probably from photoshop or the like. )</p>\n\n<pre><code>Mat img=imread(\"vig.png\",-1); // load 'as is', don't convert to bgr !!\nMat ch[4]; \nsplit(img,ch);\n\nMat im2 = ch[3];              // here's the vignette\n\n// im2 = 255 - im2;           // eventually cure the inversion\n\nimshow(\"image\",im2);\nwaitKey();\nimwrite(\"img.jpg\",im2);\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/o6Uzt.jpg\" alt=\"enter image description here\"></p>\n\n<p>again, note, that opencv won't do any alpha-compositing, you'll have to roll your own formulas for that.</p>\n",
        "system": ""
    },
    {
        "instruction": "Error in resize the image",
        "input": "",
        "output": "<p>What format is <code>image12</code>, why has it no file ending? According to the <a href=\"http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#imread\" rel=\"nofollow\">documentation</a>, it has to be one of the following:</p>\n\n<blockquote>\n  <ul>\n  <li>Windows bitmaps - *.bmp, *.dib (always supported)</li>\n  <li>JPEG files - *.jpeg, *.jpg, *.jpe (see the Notes section)</li>\n  <li>JPEG 2000 files - *.jp2 (see the Notes section)</li>\n  <li>Portable Network Graphics - *.png (see the Notes section)</li>\n  <li>Portable image format - *.pbm, *.pgm, *.ppm (always supported)</li>\n  <li>Sun rasters - *.sr, *.ras (always supported)</li>\n  <li>TIFF files - *.tiff, *.tif (see the Notes section)</li>\n  </ul>\n</blockquote>\n\n<p>Furthermore the documentation states, that when an image cannot be read, <code>imread</code> returns an empty matrix. This could be the case in your problem.</p>\n",
        "system": ""
    },
    {
        "instruction": "Why does hough not find line?",
        "input": "",
        "output": "<p>The default threshold value is too high so the line is not found. I also reduced the nhood size since you want to find horizontal and vertical lines and not angles, so they will all be very close to each other. Also note at the top I set the edges to zero, in the image you posted there is a thin border of 204's around the outside, this just elmiminates the border. Here is my script. </p>\n\n<pre><code>clc;clearvars;close all;\nim=imread('B5oOc.png');\nim=rgb2gray(im);\nim(:,1:2)=0;\nim(1,:)=0;\nim(end,:)=0;\nim(:,end)=0;\nBW=edge(im,'canny');\n\n[H, T, R] = hough(BW);\nP = houghpeaks(H, 20,'NHoodSize',[1 1],'threshold',ceil(0.3*max(H(:))));\nlines = houghlines(BW, T, R, P, 'FillGap', 1, 'MinLength', 3);\n\nimshow(imadjust(mat2gray(H)),'XData',T,'YData',R,...\n      'InitialMagnification','fit');\ntitle('Hough Transform of Image');\nxlabel('\\theta'), ylabel('\\rho');\naxis on, axis normal, hold on;\ncolormap(hot);\n\nx = T(P(:,2));\ny = R(P(:,1));\nplot(x,y,'s','color','blue');\n\nfigure;\nimagesc(im);hold on;colormap gray;\naxis image; \nmax_len = 0;\nfor k = 1:length(lines)\n   xy = [lines(k).point1; lines(k).point2];\n   plot(xy(:,1),xy(:,2),'LineWidth',2,'Color','green');\n\n   % Plot beginnings and ends of lines\n   plot(xy(1,1),xy(1,2),'x','LineWidth',2,'Color','yellow');\n   plot(xy(2,1),xy(2,2),'x','LineWidth',2,'Color','red');\n\n   % Determine the endpoints of the longest line segment\n   len = norm(lines(k).point1 - lines(k).point2);\n   if ( len &gt; max_len)\n      max_len = len;\n      xy_long = xy;\n   end\nend\n\n% highlight the longest line segment\nplot(xy_long(:,1),xy_long(:,2),'LineWidth',2,'Color','red');\n</code></pre>\n\n<p>The output is this:\n<img src=\"https://i.sstatic.net/prkYX.png\" alt=\"Hough transform result\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Finding center of a cross hair in image",
        "input": "",
        "output": "<p>If your images are similar and consistent (no noise, affine transformation) then the following might work.</p>\n\n<ol>\n<li><p>Cut out manually a sample of that patch to be highlighted.</p></li>\n<li><p>Use a <a href=\"http://en.wikipedia.org/wiki/Template_matching\" rel=\"nofollow\">window sliding(template)</a> image matching technique to get other similar patches.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Brightness is not adjusting in the image",
        "input": "",
        "output": "<p>It seems that you need gamma correction. </p>\n\n<p>Take a look here:</p>\n\n<p><a href=\"http://subokita.com/2013/06/18/simple-and-fast-gamma-correction-on-opencv/\" rel=\"nofollow\">http://subokita.com/2013/06/18/simple-and-fast-gamma-correction-on-opencv/</a></p>\n\n<p>and the source of link above (with source/result pictures):</p>\n\n<p><a href=\"http://imagingsolution.net/program/opencv/gamma-correction/\" rel=\"nofollow\">http://imagingsolution.net/program/opencv/gamma-correction/</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Compute sift at custom key points",
        "input": "",
        "output": "<p>Yes. See the <a href=\"http://www.vlfeat.org/overview/sift.html#tut.sift.custom\" rel=\"nofollow\">custom frames</a> section of VLFeat tutorial:</p>\n\n<blockquote>\n  <p>[...] vl_sift [...] can bypass the detector and compute the descriptor on custom frames using the Frames option.</p>\n  \n  <p>For instance, we can compute the descriptor of a SIFT frame centered at position <code>(100,100)</code>, of scale <code>10</code> and orientation <code>-pi/8</code> by:</p>\n</blockquote>\n\n<pre><code>fc = [100;100;10;-pi/8] ;\n[f,d] = vl_sift(I,'frames',fc) ;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Finding pits in an image",
        "input": "",
        "output": "<p>Your case reminds me of a paper (<a href=\"http://www.sciencedirect.com/science/article/pii/S0262885609001371\" rel=\"nofollow noreferrer\">Human Detection Using a Mobile Platform and Novel Features Derived From a Visual Saliency Mechanism</a>) that calculates Saliency on an image based on on-center ganglion cell notion, i.e. a method that detects bright pixels surrounded by dark areas (or the opposite called off-center cells).</p>\n\n<p><img src=\"https://i.sstatic.net/kWa1q.jpg\" alt=\"Ganglion cells\"></p>\n\n<p>To approximate these cells you can use rectangular areas. By the use of integral images, you can speed up the procedure. Check the paper for details.</p>\n\n<p>One more idea would've been convolution of a composite filter. Find a template that is very close to every pit and correlate the template with the image (or use multiple filters for scale/form variation).</p>\n",
        "system": ""
    },
    {
        "instruction": "How to save part of image in arrays or matrices in Matlab",
        "input": "",
        "output": "<p>I'm sure this could be optimised a bit more.</p>\n\n<pre><code>new_im = zeros(size(im));\n\n% Loop through each column\nfor c=1:1:size(im,2)\n    column = im(:,c);\n\n    % The first non-zero value in each column will be the top of the window\n    upper_candidates = find(column&gt;0); \n    top = upper_candidates(1);\n\n    % The first zero value after that will be your lower boundary\n    lower_candidates = find(column(top:end) == 0);\n    bottom = lower_candidates(1);\n\n    % Insert each column into the new image\n    new_im = im(top:bottom,c);  \nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "SolvePnP - How to use it?",
        "input": "",
        "output": "<p>Solve PnP is a function, which given the 3D model of an object (let's say, a chessboard) and a view of this object in the real world, will give you an approximate position and orientation of the camera relative to the object.</p>\n\n<p>The 3D model and view of the object are sets of corresponding 3D and 2D points. The function will work better when you know the object model (positions of key points of the object) and positions of those key points on the image from camera.</p>\n\n<p>I'm not an expert in 3D image reconstruction, but it would seem that using the information from each following image containing observed scene and its key points, and finding those on images from views you should be able to iteratively improve your model and also improve the approximation of the camera's position.</p>\n\n<p>As you have a disparity map, which shows the distance of key points of the scene viewed from two different points, it could be indeed better to use triangulation, if you know the exact points of view. Or their good approximations (and then you would need to improve those approximations with subsequent new views).</p>\n",
        "system": ""
    },
    {
        "instruction": "Object Detection: Training Requried or No Training Required?",
        "input": "",
        "output": "<p>To answer the question you asked in the title, if you want to be able to determine what the object in the picture is you need a supervised algorithm (a.k.a. trained). Otherwise you would be able to determine, in some cases, the edges or the presence of an object, but not what kind of an object it is. In order to tell what the object is you need a labelled training set.</p>\n\n<p>Regarding the contents of the question, the number of possible angles in a picture of an object is infinite. If you just have four pictures in your training set, the test example could be taken in an angle that falls halfway between training example A and training example B, making it hard to recognize for your algorithm. The larger the training set the higher the probability of recognizing the object. Be careful: you never reach the absolute certainty that your algorithm will recognize the object. It just becomes more likely.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can I find data to test Facial Tracking",
        "input": "",
        "output": "<p>In case someone else is wondering where they can find a huge database for faces from different angles take a look at </p>\n\n<p><a href=\"http://www.milbo.org/muct/index.html\" rel=\"nofollow\">http://www.milbo.org/muct/index.html</a></p>\n\n<p>and </p>\n\n<p><a href=\"http://web.mit.edu/emeyers/www/face_databases.html\" rel=\"nofollow\">http://web.mit.edu/emeyers/www/face_databases.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Bad argument error while drawing border",
        "input": "",
        "output": "<p>Quoting the relevant documentation for <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html?#borderinterpolate\" rel=\"nofollow\"><code>borderInterpolate()</code></a>, which is referenced in the documentation for <code>copyMakeBorder()</code> (emphasis mine):</p>\n\n<blockquote>\n  <p>Border type, one of the BORDER_* , <strong>except for BORDER_TRANSPARENT</strong> and BORDER_ISOLATED</p>\n</blockquote>\n\n<p>This explains the error you are getting -- <code>BORDER_TRANSPARENT</code> is not supported by <code>copyMakeBorder()</code></p>\n\n<p>Even though the <code>BORDER_TRANSPARENT</code> flag is not supported, there is still a way to create transparent borders. You can use the <code>BORDER_CONSTANT</code> flag when calling <code>copyMakeBorder</code> on a BGRA image, as long as your constant value has a zero 4th channel value. A simple example follows:</p>\n\n<pre><code>const auto im = cv::imread(\"some_image.jpg\");\ncv::cvtColor(im, im, CV_BGR2BGRA); // Image must have alpha channel!\n\n// clone() below is important -- there is a bug if using ROI without cloning first.\nconst auto roi = im(cv::Rect(200,200,200,200)).clone();\ncv::Mat bordered;\ncv::copyMakeBorder(roi, bordered, 20, 20, 20, 20, cv::BORDER_CONSTANT, cv::Scalar::all(0));\n</code></pre>\n\n<p>As an aside, I don't know what your intent was in your assignment to <code>borderInterpolate</code>, but due to C++'s comma operator, the line you wrote:</p>\n\n<pre><code>int borderInterpolate = (50, 100, BORDER_TRANSPARENT);\n</code></pre>\n\n<p>is exactly equivalent to:</p>\n\n<pre><code>int borderInterpolate = BORDER_TRANSPARENT; // 50 and 100 are evaluated and ignored\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Affine transformation model using RANSAC",
        "input": "",
        "output": "<p>If you have the Computer Vision System Toolbox you can use the <code>estimateGeometricTransform</code> function.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to configure Probabilistic Occupancy Map people detector",
        "input": "",
        "output": "<p>In the associated publication, the authors mention they use the camera calibration to generate the rectangles for a human silhouette in every position in the grid. It seems the code that accomplishes this is not included in the source files, in that case you will have to write it yourself.</p>\n\n<p>In the calibration data for their datasets, you can see that they make use of two homographies per camera, the head plane homography and the ground plane homography. You can use this to quickly obtain the required rectangles.</p>\n\n<p>The head plane homography is a 3x3 matrix that describes a mapping from one plane to another. In this case it describes the mapping from 2D room coordinates (at head level) to 2D image coordinates. You can determine this homography for your own camera with the function findHomography in opencv. All you need to do is measure the coordinates of four points on the ground in the room, and stand an upright pole on those markings. The pole should be as long as the average person you want to track. You can now write a small program  that allows you to click on the top of the pole in each camera view. You now have four world points (the coordinates measured in the room) and four image points per camera (the points you clicked). With findHomography you can determine the homography. Do the same for the markings on the ground without the pole, and you have the two homographies per camera.</p>\n\n<p>You can now use the homographies to project the 8 corner points of a rectangle standing on any position in the room onto their image coordinates for each camera. Take the bounding box of all 8 points and you have the rectangle for that room location and that camera.</p>\n\n<p>The authors of the method mentioned using a human silhouette, this indicates that their approach may be more accurate than using a cuboid. However, there is no such thing as the silhouette of a moving person, so the solution with the cuboid is likely to be perfectly workable.</p>\n",
        "system": ""
    },
    {
        "instruction": "Why do I must use Sobel Operator?",
        "input": "",
        "output": "<p>Handwriting is composed by strokes, usually the strokes are filled with a solid color: with these assumption a stroke is well described by its edges while its color or the background color is not so useful in describing it. The edges are basically described by boolean values: a pixel is white when it is part of an edge, black otherwise.</p>\n\n<p>The grey level of the output image of the Sobel filter is proportional to the strength of the edges and so it can be used to describe the edges and then to describe the strokes, or, to \"capture\" the essence of the handwritten letters and digits.</p>\n\n<p>If you feed the network with the raw image of an handwritten letter or digit there is a risk that the network learns some not so useful features like the full grey levels that do not describe very well the letter or the digit.</p>\n\n<p>Some authors claim that the Sobel filter gives the edges in a brightness invariant way even if I think it is not strictly true.</p>\n\n<p>Just out of curiosity, what is the paper where the author recommends the reader to use a Sobel filter? </p>\n",
        "system": ""
    },
    {
        "instruction": "Independent colour transformation on the original image",
        "input": "",
        "output": "<p>Accessing RGB Values of an Image can be done either by the Method showed in <a href=\"https://stackoverflow.com/questions/8932893/accessing-certain-pixel-rgb-value-in-opencv?lq=1\">Accessing certain pixel RGB value in openCV</a></p>\n\n<pre><code>valRed   = image.at&lt;cv::Vec3b&gt;(row,col)[0]; //R\nvalGreen = image.at&lt;cv::Vec3b&gt;(row,col)[1]; //G\nvalBlue  = image.at&lt;cv::Vec3b&gt;(row,col)[2]; //B\n</code></pre>\n\n<p>Compute your new values and write them to another image using: </p>\n\n<pre><code>image.at&lt;cv::Vec3b&gt;(row,col)[0] = newval[0];  //R\nimage.at&lt;cv::Vec3b&gt;(row,col)[1] = newval[1];  //G\nimage.at&lt;cv::Vec3b&gt;(row,col)[2] = newval[2];  //B\n</code></pre>\n\n<hr>\n\n<p>or, if you really want to use split (and create 3 new images, one per channel), you can use this:</p>\n\n<pre><code>split(img , colors);\n</code></pre>\n\n<p>Read:</p>\n\n<pre><code>valRed   = colors[0].at&lt;uchar&gt;(row,col)\nvalGreen = colors[1].at&lt;uchar&gt;(row,col)\nvalBlue  = colors[2].at&lt;uchar&gt;(row,col)\n</code></pre>\n\n<p>Write:</p>\n\n<pre><code>colors[0].at&lt;uchar&gt;(row,col) = newValRed;    //R\ncolors[1].at&lt;uchar&gt;(row,col) = newValGreen;  //G\ncolors[2].at&lt;uchar&gt;(row,col) = newValBlue;   //B\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Affine Model using Ransac",
        "input": "",
        "output": "<p>The first place to look for literature about the subject could be the book <em>Multiple View Geometry in Computer Vision</em>, by Hartley and Zisserman.</p>\n\n<p>From 3 correspondences you can find an affine homography <em>A</em> of size 3x3, that maps each 2D image point <em>p</em> to each point <em>q</em> with the following equation (assuming <em>p</em> and <em>q</em> are 2D points in homogeneous form):</p>\n\n<pre><code>A*p = q\n</code></pre>\n\n<p>To find <em>A</em> that matches three sets of point pairs: <em>{p1, p2, p3} -> { q1, q2, q3 }</em> you just have to stack the points in two matrices <em>P</em> and <em>Q</em>, both of size 3x3.</p>\n\n<p>Matrix <em>'P'</em> is such that cells <em>P(1,i)</em> and <em>P(2,i)</em> should contain the first and second coordinate of the <em>i</em>-th point <em>p</em>. Matrix <em>Q</em> is such that cells <em>Q(1,i)</em> and <em>Q(2,i)</em> should contain the first and second coordinate of the <em>i</em>-th point <em>q</em>.</p>\n\n<p>Cells <em>P(3,i)</em> and <em>Q(3,i)</em>, for <em>i=1..3</em> will contain 1 (for the homogeneous coordinates).</p>\n\n<p>Once formed these matrices, you can find <em>A</em> by solving:</p>\n\n<pre><code>A*P = Q\n</code></pre>\n\n<p>I.e., by simply:</p>\n\n<pre><code>A = Q*P.inv()\n</code></pre>\n\n<p>Note however, that what you are looking for is a projective planar homography, and you require 4 points minimum to calculate it, and a different method. You must solve the equation:</p>\n\n<pre><code>H * p \u03b1 q\n</code></pre>\n\n<p>where <em>H</em> is the homography you are looking for, and \u03b1 means proportionality. To find <em>H</em> from four correspondences you must use the <a href=\"http://en.wikipedia.org/wiki/Direct_linear_transformation\" rel=\"nofollow\">Direct Linear Transformation</a> method. Nevertheless, in OpenCV you can use <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=findhomography#findhomography\" rel=\"nofollow\">findHomography</a> to find <em>H</em>.</p>\n\n<p>Hope it helps, regards.</p>\n",
        "system": ""
    },
    {
        "instruction": "Hair region boundary detection using image processing",
        "input": "",
        "output": "<ol>\n<li>use face haarcascade to detect face.</li>\n<li>use eye cascade to detect one or both eyes .</li>\n<li>expand the region of face from top .</li>\n<li>estimate for had using eye point and face position.\nthis is the simplest way to detect forehead ...</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "converting rgb image to double and keeping the values in matlab",
        "input": "",
        "output": "<p>Your original image is already using a <code>jet</code> colormap. The problem is, when you convert it to grayscale, you lose some crucial information. See the image below.</p>\n\n<p><img src=\"https://i.sstatic.net/ca3Pd.png\" alt=\"enter image description here\"></p>\n\n<p>In the original image you have a heatmap. Blue areas generally indicate \"low value\", whereas red areas indicate \"high values\". But when converted to grayscale, both areas indicate low value, as they aproach dark pixels (see the arrows).</p>\n\n<p>A possible solution is this:</p>\n\n<blockquote>\n  <p>You take <strong>every</strong> pixel of your image, find the nearest (closest)\n  color value in the <code>jet</code> colormap and use its index as a gray value.</p>\n</blockquote>\n\n<p>I will show you first the final code and the results. The explanation goes below:</p>\n\n<pre><code>I = im2double(imread('myimage.png'));\n\nmap = jet(256);\nIrgb = reshape(I, size(I, 1) * size(I, 2), 3);\nIgray = zeros(size(I, 1), size(I, 2), 'uint8');\nfor ii = 1:size(Irgb, 1)\n    [~, idx] = min(sum((bsxfun(@minus, Irgb(ii, :), map)) .^ 2, 2));\n    Igray(ii) = idx - 1;\nend\nclear Irgb;\n\nsubplot(2,1,1), imagesc(I), axis equal tight xy\nsubplot(2,1,2), imagesc(Igray), axis equal tight xy\n</code></pre>\n\n<p>Result:</p>\n\n<p><img src=\"https://i.sstatic.net/3VCet.png\" alt=\"enter image description here\"></p>\n\n<pre><code>&gt;&gt; whos I Igray\n  Name         Size                Bytes  Class     Attributes\n\n  I          110x339x3            894960  double              \n  Igray      110x339               37290  uint8  \n</code></pre>\n\n<p><strong>Explanation:</strong></p>\n\n<p>First, you get the <code>jet</code> colormap, like this:</p>\n\n<pre><code>map = jet(256);\n</code></pre>\n\n<p>It will return a <code>256x3</code> colormap with the possible colors on the jet palette, where each row is a RGB pixel. <code>map(1,:)</code> would be kind of a dark blue, and <code>map(256,:)</code> would be kind of a dark red, as expected.</p>\n\n<p>Then, you do this:</p>\n\n<pre><code>Irgb = reshape(I, size(I, 1) * size(I, 2), 3);\n</code></pre>\n\n<p>... to turn your <code>110x339x3</code> image into a <code>37290x3</code> matrix, where each row is a RGB pixel.</p>\n\n<p>Now, for each pixel, you take the Euclidean distance of that pixel to the <code>map</code> pixels. You take the index of the nearest one and use it as a gray value. The minus one (<code>-1</code>) is because the index is in the range 1..256, but a gray value is in the range 0..255.</p>\n\n<p><strong>Note:</strong> the Euclidean distance takes a square root at the end, but since we are just trying to find the closest value, there is no need to do so.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Here is a 10x faster version of the code:</p>\n\n<pre><code>I = im2double(imread('myimage.png'));\nmap = jet(256);\n[C, ~, IC] = unique(reshape(I, size(I, 1) * size(I, 2), 3), 'rows');\nequiv = zeros(size(C, 1), 1, 'uint8');\nfor ii = 1:numel(equiv)\n    [~, idx] = min(sum((bsxfun(@minus, C(ii, :), map)) .^ 2, 2));\n    equiv(ii) = idx - 1;\nend\nIrgb = reshape(equiv(IC), size(I, 1), size(I, 2));\nIrgb = Irgb(end:-1:1,:);\nclear equiv C IC;\n</code></pre>\n\n<p>It runs faster because it exploits the fact that the colors on your image are restricted to the colors in the <code>jet</code> palette. Then, it counts the <code>unique</code> colors and only match them to the palette values. With fewer pixels to match, the algorithm runs much faster. Here are the times:</p>\n\n<ul>\n<li><p><strong>Before:</strong></p>\n\n<p>Elapsed time is 0.619049 seconds.</p></li>\n<li><p><strong>After:</strong></p>\n\n<p>Elapsed time is 0.061778 seconds.</p></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Feature extraction from each Bounding Box",
        "input": "",
        "output": "<p>Just save the bounding box in a separate image. If you are using opencv , see <a href=\"http://answers.opencv.org/question/10364/set-roi-in-cvmat/\" rel=\"nofollow\">how to set roi in an</a> image. Now extract feature on the image containing only the portion in the bounding box.</p>\n",
        "system": ""
    },
    {
        "instruction": "Sliding Window Object detection",
        "input": "",
        "output": "<p>I assume that you are referring to pyramidal approach which is frequently used in optical flow.\nThe patch is first detected at upper pyramid scale, than the obtained coordinates are rescaled and the patch is searched again in lower pyramid scale.</p>\n\n<p>Haar object detection does not need image rescaling as the features can be easily rescaled using integral image.</p>\n",
        "system": ""
    },
    {
        "instruction": "Should stereo projection be internally consistent?",
        "input": "",
        "output": "<p>I was doing something slightly wrong.  When reprojecting from 3D to 2D, I missed that <code>stereoRectify</code> returns <code>R1</code>, the output rectification rotation matrix.  When calling <code>projectPoints</code>, I needed to pass the <em>inverse</em> of that matrix as the second parameter (<code>rvec</code>).</p>\n",
        "system": ""
    },
    {
        "instruction": "Can&#39;t get foreground from MOG2",
        "input": "",
        "output": "<p>Could you provide more information? What do you mean by you \"can't seem to get the foreground image\"? How do you realize that you \"don't have it\"? Is it empty? Is it just black when you try to display it? </p>\n\n<p>I'm using C++, but also had an issue with the foreground image when I tried to display it along other images in one window. I first had to convert the image from gray- to rgb-colorspace (in C++ there is the method <code>cvtColor(InputArray src, OutputArray dst, int code, int dstCn=0)</code> for this). Maybe trying this could help you, but maybe not since I don't know what your precise problem is ;-)</p>\n",
        "system": ""
    },
    {
        "instruction": "VLFeat SVM storage",
        "input": "",
        "output": "<p>I am not using this approach however it should be sufficient to save the model and bias terms to a file for later testing</p>\n",
        "system": ""
    },
    {
        "instruction": "What does normalizing the gradient magnitude signify?",
        "input": "",
        "output": "<p>Practically it does not make much sense because gradient is sparse in an image, however if the image is of some texture which has a certain gradient prevalent throughout the image (sub-image), then such a normalization would help to get the relative gradient with respect to the gradient in the surroundings.</p>\n",
        "system": ""
    },
    {
        "instruction": "Hough transform with a different theta interval with matlab",
        "input": "",
        "output": "<p>Due to the definition of the hough-Transformation, the vale for <code>r(roh,theta)=r(-roh,theta+180)</code>. You can flip the data you get for -90:0 horizontally and you will get the data for 90:180.</p>\n\n<p>The following code uses the example from the documentation and completes the data to full 360 degree:</p>\n\n<pre><code>%example code\nRGB = imread('gantrycrane.png');\n\n% Convert to intensity.\nI  = rgb2gray(RGB);\n\n% Extract edges.\nBW = edge(I,'canny');\n[H,T,R] = hough(BW,'RhoResolution',0.5,'Theta',-90:0.5:89.5);\n\n% Display the original image.\nsubplot(3,1,1);\nimshow(RGB);\ntitle('Gantrycrane Image');\n\n% Display the Hough matrix.\nsubplot(4,1,2);\nimshow(imadjust(mat2gray(H)),'XData',T,'YData',R,...\n      'InitialMagnification','fit');\ntitle('Hough Transform of Gantrycrane Image');\nxlabel('\\theta'), ylabel('\\rho');\naxis on, axis normal, hold on;\ncolormap(hot);\n\n%Modifications begin\nsubplot(3,1,3);\n%append another 180 degree to the axis\nT2=[T T+180];\n%append flipped data\nH2=[H,H(end:-1:1,:)];\n%plot the same way.\nimshow(imadjust(mat2gray(H2)),'XData',T2,'YData',R,...\n      'InitialMagnification','fit');\ntitle('Hough Transform of Gantrycrane Image');\nxlabel('\\theta'), ylabel('\\rho');\naxis on, axis normal, hold on;\ncolormap(hot);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "hough transform - javascript - node.js",
        "input": "",
        "output": "<p>It seems you try to implement the algorithm of <a href=\"http://www.ecse.rpi.edu/~qji/Papers/ellipse_det_icpr02.pdf\" rel=\"nofollow\">Yonghong Xie; Qiang Ji (2002). A new efficient ellipse detection method 2. p. 957</a>.</p>\n\n<h3>Ellipse removal suffers from several bugs</h3>\n\n<p>In your code, you perform the removal of found ellipse (step 12 of the original paper's algorithm) by resetting coordinates to <code>{-1, -1}</code>.</p>\n\n<p>You need to add:</p>\n\n<pre><code>`if (arr_edges[x1y1].x === -1) break;`\n</code></pre>\n\n<p>at the end of the x2y2 block. Otherwise, the loop will consider -1, -1 as a white point.</p>\n\n<p>More importantly, your algorithm consists in erasing every point which distance to the center is smaller than <code>b</code>. <code>b</code> supposedly is the minor axis half-length (per the original algorithm). But in your code, variable <code>b</code> actually is <strong>the latest</strong> (and not most frequent) half-length, and you erase points with a distance lower than b (instead of greater, since it's the minor axis). In other words, you clear all points inside a circle with a distance lower than latest computed axis.</p>\n\n<p>Your sample image can actually be processed with a clearing of all points inside a circle with a distance lower than selected major axis with:</p>\n\n<pre><code>max_minor      = arr_accum[max_votes.index].d;\n</code></pre>\n\n<p>Indeed, you don't have overlapping ellipses and they are spread enough. Please consider a better algorithm for overlapping or closer ellipses.</p>\n\n<h3>The algorithm mixes major and minor axes</h3>\n\n<p>Step 6 of the paper reads:</p>\n\n<blockquote>\n  <p>For each third pixel (x, y), if the distance between (x, y) and (x0,\n  y0) is greater than the required least distance for a pair of pixels\n  to be considered then carry out the following steps from (7) to (9).</p>\n</blockquote>\n\n<p>This clearly is an approximation. If you do so, you will end up considering points further than the minor axis half length, and eventually on the major axis (with axes swapped). You should make sure the distance between the considered point and the tested ellipse center is smaller than currently considered major axis half-length (condition should be <code>d &lt;= a</code>). This will help with the ellipse erasing part of the algorithm.</p>\n\n<p>Also, if you also compare with the least distance for a pair of pixels, as per the original paper, 40 is too large for the smaller ellipse in your picture. The comment in your code is wrong, it should be at maximum half the smallest ellipse minor axis <em>half-length</em>.</p>\n\n<h3>LEAST_REQUIRED_ELLIPSES is too small</h3>\n\n<p>This parameter is also misnamed. It is the minimum number of votes an ellipse should get to be considered valid. Each vote corresponds to a pixel. So a value of 6 means that only 6+2 pixels make an ellipse. Since pixels coordinates are integers and you have more than 1 ellipse in your picture, the algorithm might detect ellipses that are not, and eventually clear edges (especially when combined with the buggy ellipse erasing algorithm). Based on tests, a value of 100 will find four of the five ellipses of your picture, while 80 will find them all. Smaller values will not find the proper centers of the ellipses.</p>\n\n<h3>Sample image is not black &amp; white</h3>\n\n<p>Despite the comment, sample image is not exactly black and white. You should convert it or apply some threshold (e.g. RGB values greater than 10 instead of simply different form 0).</p>\n\n<p>Diff of minimum changes to make it work is available here:\n<a href=\"https://gist.github.com/pguyot/26149fec29ffa47f0cfb/revisions\" rel=\"nofollow\">https://gist.github.com/pguyot/26149fec29ffa47f0cfb/revisions</a></p>\n\n<p>Finally, please note that <code>parseInt(x.toFixed(0))</code> could be rewritten <code>Math.floor(x)</code>, and you probably want to not truncate all floats like this, but rather round them, and proceed where needed: the algorithm to erase the ellipse from the picture would benefit from non truncated values for the center coordinates. This code definitely could be improved further, for example it currently computes the distance between points <code>x1y1</code> and <code>x2y2</code> twice.</p>\n",
        "system": ""
    },
    {
        "instruction": "Curvedness output",
        "input": "",
        "output": "<p>Make sure your input image is of type float and not int. See this answer for how to convert an int Mat to a float one.\n<a href=\"https://stackoverflow.com/questions/6302171/convert-uchar-mat-to-float-mat-in-opencv\">Convert uchar Mat to float Mat in OpenCV?</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Optimal Parameters for haarclassifiers",
        "input": "",
        "output": "<p>For face detection, you cannot set the third parameter (i.e. <code>scaleFactor</code> - parameter specifying how much the image size is reduced at each image scale) to be <code>1</code>, which you will lose the ability to search the faces in the multi-scale way as it will not be able to be re-sized. Try to change it to <code>1.05</code> (i.e. default value) or others.</p>\n\n<p>To get better face detections, check out <a href=\"https://stackoverflow.com/a/20805153/2589776\">this thread</a> for the recommended values of <a href=\"http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html?highlight=detectmultiscale#cascadeclassifier-detectmultiscale\" rel=\"nofollow noreferrer\"><code>CascadeClassifier::detectMultiScale()</code></a> parameters, .</p>\n\n<hr>\n\n<p>For pedestrian detection, in order to improve its quality, you should try to modify <a href=\"http://docs.opencv.org/modules/gpu/doc/object_detection.html#gpu-hogdescriptor-detectmultiscale\" rel=\"nofollow noreferrer\"><code>HOGDescriptor::detectMultiScale()</code></a>'s third (i.e. <code>hit_threshold</code>) and the last parameters (i.e. <code>group_threshold</code>).</p>\n\n<p>Particularly, you should both make them higher in order to get better pedestrian detection result:</p>\n\n<ul>\n<li><code>hit_threshold</code>: Threshold for the distance between features and SVM classifying plane.</li>\n<li><code>group_threshold</code>: Coefficient to regulate the similarity threshold. When detected, some objects can be covered by many rectangles. The threshold is used in a group of rectangles to retain it.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Tracking in video sequence with user-defined target to track",
        "input": "",
        "output": "<p>Try using a correlation tracker.  The modern ones are pretty good, for example: <a href=\"https://www.youtube.com/watch?v=-8-KCoOFfqs\" rel=\"nofollow\">https://www.youtube.com/watch?v=-8-KCoOFfqs</a>.  You can also get the code for this in <a href=\"http://dlib.net/video_tracking_ex.cpp.html\" rel=\"nofollow\">dlib</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Image Transformation Filter",
        "input": "",
        "output": "<p>Take a look at OpenCV's <a href=\"http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html#remap\" rel=\"nofollow\">remap</a> function. You are looking to do a geometric transform. You'll have to design the X,Y maps such that the pixels map in the way you have shown in your example. </p>\n",
        "system": ""
    },
    {
        "instruction": "Extract black objects from color background",
        "input": "",
        "output": "<p>I would convert the image to the HSV colour space and then use the Value channel. This basically separates colour and brightness information. </p>\n\n<p>This is the 50% cyan image</p>\n\n<p><img src=\"https://i.sstatic.net/Tae8E.png\" alt=\"enter image description here\"></p>\n\n<p>Then you can just do a simple threshold to isolate the dots.</p>\n\n<p><img src=\"https://i.sstatic.net/ZLycY.png\" alt=\"enter image description here\"></p>\n\n<p>I just did this very quickly and im sure you could get better results. Maybe find contours in the image and then remove any contours with a small area, to filter any remaining noise.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to get the depth value of a ball from a disparity map",
        "input": "",
        "output": "<p>You need to provide a lot more details to get a useful answer. </p>\n\n<p>For example, are you looking for a fully automated solution, or is it acceptable to have a human operator provide some input (\"hints\")? If the latter, the problem becomes a lot easier - a common method is to get mouse input (click) on one or a few pixels in an input image, look up the corresponding depth(s) through the disparity map, then \"grow\" a fitted sphere from there by adding neighboring pixels - you'll want to start with a lose fit error threshold, and tighten it as the number of added samples increases.</p>\n",
        "system": ""
    },
    {
        "instruction": "Computer Vision technique to match person shape and crop it out of picture",
        "input": "",
        "output": "<p>HoG will help you to detect a human shape in the image as noted by @kkuilla. Once you have the location as a bounding box in the picture you should use a segmentation method to extract foreground from background (inside the box).</p>\n\n<p>This could be done with generic methods:</p>\n\n<p><a href=\"https://www.google.com.br/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCMQFjAA&amp;url=http%3A%2F%2Fmath.arizona.edu%2F~rcrandall%2FECE532_ProjectPaper.pdf&amp;ei=Fog6VdLlG-3HsQSIv4HIAg&amp;usg=AFQjCNEbEV5Y8VTF57-BCTg2X6YLzN1d8g&amp;sig2=X0fKSR1Y6lvFVHObTYXEjA&amp;bvm=bv.91665533,d.cWc\" rel=\"nofollow noreferrer\">Chan-Vese segmentation</a></p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Otsu%27s_method\" rel=\"nofollow noreferrer\">Otsu method</a></p>\n\n<p>Or segmentation method more specific to people, which a google scholar\nsearch can help to find them:</p>\n\n<p><a href=\"https://scholar.google.com.br/scholar?as_ylo=2011&amp;q=segmenting+people+from+images&amp;hl=en&amp;as_sdt=0,5\" rel=\"nofollow noreferrer\">https://scholar.google.com.br/scholar?as_ylo=2011&amp;q=segmenting+people+from+images&amp;hl=en&amp;as_sdt=0,5</a></p>\n\n<p>I am afraid that there will not be any solution already implemented in OpenCV for segmenting people, but take a look at this anwser:\n<a href=\"https://stackoverflow.com/questions/8489091/finding-people-silhouette-in-opencv-c\">Finding people silhouette in OpenCV C++</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Frames missing while reading video frame by frame in matlab",
        "input": "",
        "output": "<p>I noticed a error you made indexing the images. <code>BB</code> has a variable size, thus you can't use it to linearise the indices. Instead of <code>num2str(i+k*(size(BB,1)))</code> I would use a counter which is incremented each iteration.</p>\n",
        "system": ""
    },
    {
        "instruction": "Trying to make color balance of photoshop using opencv",
        "input": "",
        "output": "<p>1, Try it - all you have to do is change the sign, and recompile!</p>\n\n<p>2, Learn about other <a href=\"https://en.wikipedia.org/wiki/HSL_and_HSV\" rel=\"nofollow\">color representations</a> - you almost never want to do this sort of operation directly in RGB.</p>\n\n<p>Color is surprisingly complex, a good place to start is the <a href=\"http://www.poynton.com/ColorFAQ.html\" rel=\"nofollow\">color faq</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Could not load file or assembly &#39;Magick.NET-x86.DLL&#39; or one of its dependencies",
        "input": "",
        "output": "<p>Magick.NET needs the <a href=\"http://www.microsoft.com/en-in/download/details.aspx?id=30679\" rel=\"noreferrer\">VC 2012 Runtime</a> installed, and Magick.NET V7+ (.NET 4.0) needs 2012 and the <a href=\"http://www.microsoft.com/en-us/download/details.aspx?id=48145\" rel=\"noreferrer\">VC 2015 Runtime</a> installed. Download and install on client machine. If it pops up a \"repair or uninstall\" dialog then its already installed and you might be <a href=\"https://stackoverflow.com/a/22260912/1294758\">missing another dependency</a>. If not, install VCR and restart your app.</p>\n",
        "system": ""
    },
    {
        "instruction": "Eyeglasses Detection in facial image",
        "input": "",
        "output": "<p>The paper is behind a paywall and I would have skipped it, but I think the problem is interesting.</p>\n\n<ol>\n<li><p>I have only skimmed it, but it looks like they use histograms of the six measurements to show that all six results in a bimodal distribution, where no-glasses/glasses are well separated. The find that combining M2 and M5 gives the best result.</p></li>\n<li><p>As I read it, the parameters are fixed and you should optimise them. So you should probably apply some preprocessing so you get images that are more equal. They test on a data set where this has been done and find that it decreases performance, but combining measurements can improve it.</p></li>\n</ol>\n\n<p>Anyways, I think you should consider researching some more before committing to this method. A quick search for \"automatic eyeglass detection\" gives several promising hits.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting face very fast from video using vision.CascadeObjectDetector in matlab",
        "input": "",
        "output": "<p>There are a few of things you can try:</p>\n\n<ul>\n<li><p>Definitely move <code>FaceDetect = vision.CascadeObjectDetector;</code> outside of the loop. You only need to create the face detector object once. Re-creating it for every frame is definitely your performance bottleneck.</p></li>\n<li><p><code>vision.VideoFileReader</code> returns a frame of class <code>'single'</code> by default. If you change the output data type to <code>'uint8'</code>, that should speed up the face detector.  Use <code>obj=vision.VideoFileReader('basu.avi', 'VideoOutputDataType', 'uint8');</code></p></li>\n<li><p><code>vision.VideoFileReader</code> can also do the conversion to grayscale for you. Use <code>obj=vision.VideoFileReader('basu.avi', 'VideoOutputDataType', 'uint8', 'ImageColorSpace', 'Intensity');</code>  This may be faster than calling <code>rgb2gray</code>.</p></li>\n<li><p>Try limiting the size of the faces being detected using <code>'MinSize'</code> and <code>'MaxSize'</code> options of <code>vision.CascadeObjectDetector</code> and/or try downsampling the frame before detecting faces.</p></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Questions about the Structure From Motion Pipeline",
        "input": "",
        "output": "<p>The pipeline you propose is generally correct. Except 3.1.</p>\n\n<p>2.2) Correct. RANSAC picks points at random to estimate the fundamental matrix and is robust enough to outliers (as long as you have enough valid matches of course). Homography outliers are NOT necessarily bad matches and so homography should not be used to filter matches.</p>\n\n<p>3.1) Incorrect: Homography inliers are matches that are perfectly aligned in both views, for example points that exhibit proportional or similar movement between the 2 views.\nWhat this means is, the higher the number of homography inliers in a view pair, the LESS the ViewPair is a good candidate as a seed for Baseline Triangulation. The camera matrices of such 2 views from a Fundamental matrix estimated with RANSAC will most likely come out inaccurate and the reconstuction will never pick up.\nWhat you want to do instead, is start with the ViewPair that has the LOWEST percentage of homography inliers, and still a high number of matches.\nUnfortunately the Image Pairs that have the highest number of matches also usually have the highest number of homography inliers. This is due to the fact that usually those pairs contain very little camera movement...</p>\n\n<p>3.4) What I do is try the triangulation using all 4 possible Camera matrix ambiguations. R1|t1, R1|t2, R2|t1, R2|t2</p>\n\n<p>8) Yes</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect blob in very noisy image",
        "input": "",
        "output": "<p>The background looks quite even so you should be able to isolate the main object using thresholding, allowing you to use array masking to identify regions within the main object. I would have a go with some tools from scikit image to see where that gets you <a href=\"http://scikit-image.org/docs/dev/auto_examples/\" rel=\"nofollow\">http://scikit-image.org/docs/dev/auto_examples/</a></p>\n\n<p>I would try gaussian/median filtering followed by thresholding/filling gaps. Or you could try random walker segmentation, or pherhaps texture classification might be more useful. When you have a list of smaller objects within the main object you can then filter these with respect to shape, size, roundness etc\n<a href=\"http://scikit-image.org/docs/dev/auto_examples/plot_label.html#example-plot-label-py\" rel=\"nofollow\">http://scikit-image.org/docs/dev/auto_examples/plot_label.html#example-plot-label-py</a> </p>\n",
        "system": ""
    },
    {
        "instruction": "Detect and fix text skew by rotating image",
        "input": "",
        "output": "<p>Based on your above comment, here is the code based on the tutorial <a href=\"http://felix.abecassis.me/2011/10/opencv-rotation-deskewing/\" rel=\"nofollow noreferrer\">here</a>, working fine for the above image,</p>\n\n<p><strong>Source</strong></p>\n\n<p><img src=\"https://i.sstatic.net/hhIsE.png\" alt=\"enter image description here\"> </p>\n\n<p><strong>Rotated</strong></p>\n\n<p><img src=\"https://i.sstatic.net/BZccx.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code> Mat src=imread(\"text.png\",0);\n Mat thr,dst;\n threshold(src,thr,200,255,THRESH_BINARY_INV);\n imshow(\"thr\",thr);\n\n  std::vector&lt;cv::Point&gt; points;\n  cv::Mat_&lt;uchar&gt;::iterator it = thr.begin&lt;uchar&gt;();\n  cv::Mat_&lt;uchar&gt;::iterator end = thr.end&lt;uchar&gt;();\n  for (; it != end; ++it)\n    if (*it)\n      points.push_back(it.pos());\n\n  cv::RotatedRect box = cv::minAreaRect(cv::Mat(points));\n  cv::Mat rot_mat = cv::getRotationMatrix2D(box.center, box.angle, 1);\n\n  //cv::Mat rotated(src.size(),src.type(),Scalar(255,255,255));\n  Mat rotated;\n  cv::warpAffine(src, rotated, rot_mat, src.size(), cv::INTER_CUBIC);\n imshow(\"rotated\",rotated);\n</code></pre>\n\n<blockquote>\n  <p></p>\n</blockquote>\n\n<p><strong>Edit:</strong></p>\n\n<p>Also see the answer <a href=\"https://stackoverflow.com/questions/23147752/why-does-this-rotate-method-give-the-image-dead-space-opencv/23149915#23149915\">here</a> , might be helpful.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCv: Filling in characters",
        "input": "",
        "output": "<p>If the letter is always with specific colour you can use colour based segmentation,</p>\n\n<ul>\n<li><p><a href=\"http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor\" rel=\"nofollow noreferrer\">Convert source to hsv</a> colour space.</p></li>\n<li><p>Perform <a href=\"http://docs.opencv.org/modules/core/doc/operations_on_arrays.html#inrange\" rel=\"nofollow noreferrer\">inRange()</a> between lower and upper threshold for the particular colour.</p></li>\n</ul>\n\n<p>For the above image you culd use some thing like</p>\n\n<pre><code> Mat src=imread(\"l.jpg\",1);\n Mat hsv,thr;\n cvtColor(src,hsv,CV_BGR2HSV);\n inRange(hsv,Scalar(76,84,86),Scalar(135,255,255),thr);\n imshow(\"thr\",thr);\n</code></pre>\n\n<p>See the result,</p>\n\n<p><img src=\"https://i.sstatic.net/6Axd4.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "How to do Mul blending in opencv",
        "input": "",
        "output": "<p>You are not accessing the image channels correctly. Moreover, you do not need to store the result in a <code>float</code> image, <code>uchar</code> is OK. Also, your loop on RGB channels should end when <code>rgb&lt;img1.channels()</code>.</p>\n\n<p>Try this code:</p>\n\n<pre><code>cv::Mat img1 = cv::imread(\"E:\\\\img.jpg\");\ncv::Mat img2 = cv::Mat (img1.size(),img1.type());\nstd::vector&lt;cv::Mat&gt; colors_1;\ncv::split(img2, colors_1);\ncolors_1[0] = 113;\ncolors_1[1] = 221;\ncolors_1[2] = 216;\ncv::merge(colors_1,img2);\ncv::Mat result(img1.size(), CV_8UC3);\nfor(int i = 0; i &lt; img1.rows; ++i){\n    for(int j = 0; j &lt; img1.cols; ++j){\n        for (int c=0 ; c&lt;img1.channels();c++){\n            uchar target = img1.at&lt;uchar&gt;(i, 3*j+c);\n            uchar blend = img2.at&lt;uchar&gt;(i, 3*j+c);\n            result.at&lt;uchar&gt;(i, 3*j+c) = cv::saturate_cast&lt;uchar&gt;(target*blend/255.);\n        }\n    }\n}  \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "MATLAB : How to crop an object from a binary image by identifying some features?",
        "input": "",
        "output": "<p>You can find the pixel coordinates from the mask <code>BWmm</code> and then use the coordinates with <code>imcrop</code>.</p>\n\n<p>To find the extermal points of <code>BWmm</code> use</p>\n\n<pre><code>projX = any( BWmm, 1 ); % projection of mask along x direction\nprojY = any( BWmm, 2 ); % projection of mask along y direction\nfx = find( projX, 1, 'first' ); % first column with non-zero val in mask\ntx = find( projX, 1, 'last' );  % last column with non-zero val in mask\nfy = find( projY, 1, 'first' ); % first row with non-zero val in mask\nty = find( projY, 1, 'last' );  % last row with non-zero val in mask\ncropRect = [fx, fy, tx-fx+1, ty-fy+1];\ncImg = imcrop( I, cropRect );\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "apply the edge detection in an image segmentation",
        "input": "",
        "output": "<p>points parameter is the pointer to the snakes point array </p>\n\n<p><a href=\"https://fossies.org/dox/opencv-2.4.8/snakes_8cpp_source.html\" rel=\"nofollow\">https://fossies.org/dox/opencv-2.4.8/snakes_8cpp_source.html</a></p>\n\n<p>Here is an example of how to use snakes</p>\n\n<p><a href=\"http://download.andol.info/cvsnakeimage.cpp\" rel=\"nofollow\">http://download.andol.info/cvsnakeimage.cpp</a></p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV&gt;&gt; Structure from motion, triangulation",
        "input": "",
        "output": "<ol>\n<li><p>Triangulated points are in the same coordinate system as the cameras which are used for triangulation...</p></li>\n<li><p>In a practical sense, No. </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Rectification with too much black areas on images C++",
        "input": "",
        "output": "<p>Try to call <code>stereoRectify</code> like this:</p>\n\n<pre><code>stereoRectify(cameraMatrix[0], distCoeffs[0],\n              cameraMatrix[1], distCoeffs[1],\n              imageSize, R, T, R1, R2, P1, P2, Q,\n              0, -1, imageSize, &amp;roi1, &amp;roi2 );\n</code></pre>\n\n<p>i.e. use <code>0</code>instead of the flag <code>CALIB_ZERO_DISPARITY</code>.</p>\n\n<p>Also, in order to improve the RMS obtained by <code>stereoCalibrate</code>, try with flag <code>CV_CALIB_USE_INTRINSIC_GUESS</code> (see <a href=\"https://stackoverflow.com/questions/23059076/stereocalibrate-changes-focal-lengths-even-when-it-was-not-supposed-to/23082274#23082274\">this related answer</a>):</p>\n\n<pre><code>rms = stereoCalibrate(object_points, image_points[0], image_points[1],\n                cameraMatrix[0], distCoeffs[0],\n                cameraMatrix[1], distCoeffs[1],\n                imageSize, R, T, E, F,\n                TermCriteria(CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 100, 1e-5),\n                CV_CALIB_USE_INTRINSIC_GUESS+\n                    CV_CALIB_FIX_ASPECT_RATIO+CV_CALIB_FIX_INTRINSIC);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Locate touching pixels of two objects",
        "input": "",
        "output": "<p>Use morphological operations.<br>\nLet <code>M</code> be your matrix with zeros (no object) ones and twos indicating the locations of different objects.</p>\n\n<pre><code>M1 = M == 1; % create a logical mask of the first object\nM2 = M == 2; % logical mask of second object\ndM1 = imdilate( M1, [0 1 0; 1 1 1; 0 1 0] ); % \"expand\" the mask to the neighboring pixels\n[touchesY touchesX] =...\n   find( dM1 &amp; M2 ); % locations where the expansion of first object overlap with second one\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "sliding window technique for multiple people detection",
        "input": "",
        "output": "<p>The Computer Vision System Toolbox includes <code>vision.PeopleDetector</code> object which uses a sliding window HoG-SVM algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV 2.4.8 Visual Studio 2012",
        "input": "",
        "output": "<p>I SOLVED IT !!! Finally! After you write the directory in your PATH, you have to restart the computer, otherwise VS doesn't know where the dlls are. That's why it works when you open the project manually, and not within the IDE</p>\n",
        "system": ""
    },
    {
        "instruction": "Trying to translate formula for blending mode",
        "input": "",
        "output": "<p>First, the author of the link you provided assumed that a pixel color as a value between 0 and 1.</p>\n\n<p>Imagine you want to blend 2 images <code>img1</code> and <code>img2</code>. The formula says that if a pixel  in <code>img1</code> as a value <code>Target &gt; 0.5</code> then the resulting value is <code>(1 - (1-2*(Target-0.5)) * (1-Blend))</code> for the blended image where <code>Blend</code> is the value of the pixel of <code>img2</code>.</p>\n\n<p>On the other hand, if <code>Target &lt;= 0.5</code> the resulting color value will be <code>((2*Target) * Blend)</code>.</p>\n\n<p>You need to do this for each pixel.</p>\n\n<p><a href=\"http://www.barbato.us/2010/12/01/blimageblending-emulating-photoshops-blending-modes-opencv/#\" rel=\"nofollow\">This link</a> provides an overlay blending function with OpenCV.</p>\n\n<p>Here is an example with a grayscale image. For a RGB image, you need to do this for each channel. Of course <code>img1</code> and <code>img2</code> must have the same size. Maybe there is a quicker way to do it with OpenCV.</p>\n\n<pre><code>Mat img1;\nMat img2;\nimg1 = imread(\"img1.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\nimg2 = imread(\"img2.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\nMat result(img1.size(), CV_32F);\n\nfor(int i = 0; i &lt; img1.size().height; ++i){\n    for(int j = 0; j &lt; img1.size().width; ++j){\n        float target = float(img1.at&lt;uchar&gt;(i, j)) / 255;\n        float blend = float(img2.at&lt;uchar&gt;(i, j)) / 255;\n        if(target &gt; 0.5){\n            result.at&lt;float&gt;(i, j) = (1 - (1-2*(target-0.5)) * (1-blend));\n        }\n        else{\n            result.at&lt;float&gt;(i, j) = ((2*target) * blend);\n        }\n    }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Interpreting color function and adjusting pixels values",
        "input": "",
        "output": "<p>I think in this case, Shadows, Midtones and Highlights defines the range of trackbars values.</p>\n\n<ul>\n<li>Shadows - precise adjusting (small range);</li>\n<li>Midtones - medium adjusting (medium range);</li>\n<li>Highlights - heavy adjusting (wide range).</li>\n</ul>\n\n<p>It allows fast and precise color correction.</p>\n\n<p>The code snippet:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;stdio.h&gt;\n#include &lt;functional&gt;\n#include &lt;algorithm&gt;\n#include &lt;numeric&gt;\n#include &lt;cstddef&gt;\n#include \"opencv2/opencv.hpp\"\nusing namespace std;\nusing namespace cv;\n\nint val_Cyan_Red=0;\nint val_Magenta_Green=0;\nint val_Yellow_Blue=0;\nMat result;\nMat Img;\n\nvoid on_trackbar( int, void* )\n{\nfloat SH=0.1; // The scale of trackbar ( depends on ajusting mode Shadows/Midtones/Highlights )\n\nfloat cr_val=(float)val_Cyan_Red/255.0;\nfloat mg_val=(float)val_Magenta_Green/255.0;\nfloat yb_val=(float)val_Yellow_Blue/255.0;\n// Cyan_Red\nfloat R1=0;\nfloat G1=1;\nfloat B1=1;\n\nfloat R2=1;\nfloat G2=0;\nfloat B2=0;\n\nfloat DR=(1-cr_val)*R1+(cr_val)*R2-0.5;\nfloat DG=(1-cr_val)*G1+(cr_val)*G2-0.5;\nfloat DB=(1-cr_val)*B1+(cr_val)*B2-0.5;\n\nresult=Img+(Scalar(DB,DG,DR)*SH);\n\n// Magenta_Green\n R1=1;\n G1=0;\n B1=1;\n\n R2=0;\n G2=1;\n B2=0;\n\n DR=(1-mg_val)*R1+(mg_val)*R2-0.5;\n DG=(1-mg_val)*G1+(mg_val)*G2-0.5;\n DB=(1-mg_val)*B1+(mg_val)*B2-0.5;\n\nresult+=(Scalar(DB,DG,DR)*SH);\n\n// Yellow_Blue\n\n R1=1;\n G1=1;\n B1=0;\n\n R2=0;\n G2=0;\n B2=1;\n\n DR=(1-yb_val)*R1+(yb_val)*R2-0.5;\n DG=(1-yb_val)*G1+(yb_val)*G2-0.5;\n DB=(1-yb_val)*B1+(yb_val)*B2-0.5;\n\nresult+=(Scalar(DB,DG,DR)*SH);\n\nimshow(\"Result\",result);\nwaitKey(10);\n}\n\n// ---------------------------------\n// \n// ---------------------------------\nint main( int argc, char** argv )\n{\n    namedWindow(\"Image\",cv::WINDOW_NORMAL);\n    namedWindow(\"Result\");\n\n    Img=imread(\"D:\\\\ImagesForTest\\\\cat2.jpg\",1);\n    Img.convertTo(Img,CV_32FC1,1.0/255.0);  \n\n   createTrackbar(\"CyanRed\", \"Image\", &amp;val_Cyan_Red, 255, on_trackbar);\n   createTrackbar(\"MagentaGreen\", \"Image\", &amp;val_Magenta_Green, 255, on_trackbar);\n   createTrackbar(\"YellowBlue\", \"Image\", &amp;val_Yellow_Blue, 255, on_trackbar);\n\n    imshow(\"Image\",Img);\n    waitKey(0);\n}\n</code></pre>\n\n<p>Yhe result for approximately the values above (zero offset is 128):\n<img src=\"https://i.sstatic.net/C0cwq.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Multiview 3D reconstruction",
        "input": "",
        "output": "<p>If you have the Computer Vision System Toolbox, this <a href=\"http://www.mathworks.com/help/vision/ug/sparse-3-d-reconstruction-from-multiple-views.html#zmw57dd0e9002\" rel=\"nofollow\">example</a> may be helpful.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to find the position and score of Sliding Window over image in MATLAB",
        "input": "",
        "output": "<p>Generally people create an image pyramid and have a sliding window detector. In that, they translate the detector by one pixel, left to right and top to bottom and compute detection score over the whole image followed by non-maximal suppression. Size of the sliding window depends on the aspect ratio of your detector, the minimum size of the object you want to detect, and then you can run it at multiple scales to detect bigger objects.</p>\n",
        "system": ""
    },
    {
        "instruction": "Convert any image to colorable while creating closed areas with OpenCV and iOS",
        "input": "",
        "output": "<p>You can follow this steps,</p>\n\n<ol>\n<li><p>Find contour.</p></li>\n<li><p>For each contour check whether its closed or not as explained <a href=\"https://stackoverflow.com/questions/22240746/recognize-open-and-closed-shapes-opencv/22242203#22242203\">here</a>, and then draw the contour with appropriate color.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Image classification using cascaded boosting in scikit-learn - why would classification terminate early?",
        "input": "",
        "output": "<p>Problem solved. My training array, <code>labels</code>, had no negatives, it consisted only of labels for one class. Of course the training would terminate immediately!</p>\n",
        "system": ""
    },
    {
        "instruction": "How correlation help in matching two images?",
        "input": "",
        "output": "<p>Probably you need Normalized Cross Correlation, the maximum will be in 4th row. </p>\n",
        "system": ""
    },
    {
        "instruction": "Score recognition from a scoreBox by Opencv &amp; tesseract-ocr",
        "input": "",
        "output": "<p>I wouldn't recommend to rewrite an OCR, but it really looks like the trained package you are using may not be adequate for this application. It might be necessary to create new <a href=\"https://code.google.com/p/tesseract-ocr/wiki/TrainingTesseract3\" rel=\"nofollow\">trained data</a>, as you suggested, with the appropriate font.</p>\n",
        "system": ""
    },
    {
        "instruction": "What does eigen value of structure tensor matrix denote?",
        "input": "",
        "output": "<p>Note that this matrix always has nonnegative eigenvalues. Basically this rule says that one should favor rapid change in all directions, that is corners are better features than edges or flat surfaces.</p>\n\n<p>The biggest eigenvalue corresponds to the eigenvector pointing towards the direction of the most significant change in the image at the point <strong>u</strong>.</p>\n\n<ul>\n<li>If the two eigenvalues are small the image at point <strong>u</strong> does not change much.</li>\n<li>If one of the eigenvectors is large and the other is small this point might lie on an edge in the image but it will be difficult to figure out where exactly on that edge.</li>\n<li>If both are large, the point is like a corner.</li>\n</ul>\n\n<p>There is a nice presentation with examples in the panoramic stitching <a href=\"http://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf\" rel=\"noreferrer\">slide deck</a> from a course taught by Rajesh Rao at the University of Washington.</p>\n\n<p><img src=\"https://i.sstatic.net/ctoOD.png\" alt=\"enter image description here\"></p>\n\n<p>Here <strong>E(u,v)</strong> denotes the Eucledian distance between the two areas in the vicinities of pixels shifted by the vector <em>(u,v)</em> from each other. This distance tells how easy it is to distinguish the two pixels from one another.</p>\n\n<p><strong>Edit</strong> The matrix of image derivatives is denoted <strong>H</strong> in this illustration probably because of its relation to <a href=\"http://en.wikipedia.org/wiki/Corner_detection#The_Harris_.26_Stephens_.2F_Plessey_.2F_Shi.E2.80.93Tomasi_corner_detection_algorithm\" rel=\"noreferrer\">Harris corner detection algorithm</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Accuracy depending on the image size",
        "input": "",
        "output": "<p>The answer is \"it depends\".  It depends on the specific problem you are trying to solve.  If you are training a classifier to determine whether or not an image contains a face, you can get away with reducing the size of the image quite a bit. 32x32 is a common size used by face detectors.  On the other hand, if you are trying to determine whose face it is, you will most likely need a higher-resolution image. </p>\n\n<p>Think about it this way: reducing the size of the image removes high-frequency information. The more of it you remove, the less specific your representation becomes. I would expect that decreasing image size would decrease false negatives and increase false positives, but again, that depends on what kind of categories are you trying to classify.  For any particular problem there is probably a \"sweet spot\", an image size that yields the maximum accuracy.</p>\n",
        "system": ""
    },
    {
        "instruction": "Real time traffic sign recognition in android",
        "input": "",
        "output": "<p>I found this work with some research: <a href=\"https://www.academia.edu/4950526/Traffic_Sign_Recognition_system_on_Android_devices\" rel=\"nofollow\">https://www.academia.edu/4950526/Traffic_Sign_Recognition_system_on_Android_devices</a> </p>\n",
        "system": ""
    },
    {
        "instruction": "What is helperGrowEdges function in MATLAB",
        "input": "",
        "output": "<p>As hinted out by Naveh, It works those functions are present only in MATLAB 2014 version and not in older versions. \nOlder versions do not have that example as well when I try to open edit <code>TextDetectionExample</code></p>\n",
        "system": ""
    },
    {
        "instruction": "Image rectification for convave shape",
        "input": "",
        "output": "<p>If you want to \"flatten\" the hemisphere, a perpendicular view from the top and well centered is indeed better. You can develop the geometric model of this setup and this can give you the deformation equations in polar coordinates. Then write an unwarping function.</p>\n\n<p>Alternatively, it is not impossible that the camera distortion parameters of the standard models will supply enough flexibility to match this deformation with sufficient accuracy. You can try it by \"painting\" a grid inside the hemisphere. The way to map the grid to the sphere is not unique, anyway (as there is no unique way to flatten Earth maps).</p>\n\n<p>UPDATE: old answer before the OP better described the problem</p>\n\n<p>That is an unusual thing you are asking. This could be called shading removal, in some way the opposite of shape-from-shading. What you see is the original color with variable luminance, depending on how it reflects the light.</p>\n\n<p>To correct that, you can divide the color image by a pure luminance image. The cheap way to get it is to compute the luminance (weighted sum of RGB components) from the given image. The hard way is to synthetize a scene with a gray sphere and a gray plane using an appropriate shading model (diffuse) and knowing the light sources.</p>\n\n<p>I doubt you will ever obtain a perfect correction.</p>\n",
        "system": ""
    },
    {
        "instruction": "multiview images points matching for common points",
        "input": "",
        "output": "<p>This <a href=\"http://www.mathworks.com/help/vision/ug/sparse-3-d-reconstruction-from-multiple-views.html\" rel=\"nofollow\">example</a> includes a helper function that does the multi-view matching using the <a href=\"http://www.mathworks.com/help/stats/kdtreesearcher-class.html\" rel=\"nofollow\">KDTreeSearcher</a> object from the Statistics Toolbox.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV&gt;&gt; camera relative pose estimation",
        "input": "",
        "output": "<p>First of all, calibrate your camera instead of using predefined values. It would always make a big impact.\nThe relative pose computed by 8-pt or 5-pt are subject to a lot of noise and by no means imply the final word. Having said that, it would be a good idea to reconstruct the points and then bundle adjust the entire scene. Refine your extrinsics and you should come up with a better pose.</p>\n",
        "system": ""
    },
    {
        "instruction": "is it possible to apply car detection on IMAGE using haar cascade method in OpenCV?",
        "input": "",
        "output": "<p>Just compare the center of the detection rectangle from the previous frame to that in the current frame. If the center has not changed too much, it is likely the same object. In that case, don't count the \"new\" rectangle in the current frame.</p>\n",
        "system": ""
    },
    {
        "instruction": "Bag-of-Words (BOW) in VLFeat",
        "input": "",
        "output": "<p>given that you already have the \"dictionary\" from vl_kmeans:  </p>\n\n<pre><code>[centers] = vl_kmeans(data, numClusters); \n</code></pre>\n\n<p>In order to build histogram of image I, you need to get the 128-D descriptors of that image using vl_sift:  </p>\n\n<pre><code>[~,D] = vl_sift(I)  \n</code></pre>\n\n<p>Each column of D is the descriptor of one interest point (or frame) in image I. Now you need to build the histogram of I based on <code>D</code> and the dictionary <code>centers</code>. The simplest way is using a for loop:    </p>\n\n<pre><code>H = zeros(1,numClusters);\nfor i=1:size(D,2)\n  [~, k] = min(vl_alldist(D(:,i), centers)) ;\n  H(k) = H(k) + 1;\nend\n</code></pre>\n\n<p>Now it is up to you to normalise the histogram H or not, before passing it to SVM. Note that there is possibly a faster way to build the histogram that does not need a loop; but I think my code (in Matlab) is clear enough to explain the algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "How do you fit AAM generated using Tim Cootes toolbox to a new image?",
        "input": "",
        "output": "<p>Actually, I think <strong>you can't</strong>. You have to provide a set of initial landmarks from which the search will start. That's because the program implements the ASM search only and does not include any face or eye detector to find a starting point for the algorithm. </p>\n\n<p>Try Stephen Milborrow ASM/AAM software, <a href=\"http://www.milbo.users.sonic.net/stasm/index.html\" rel=\"nofollow\">STASM</a>. It has similar functionalities and, as I recall, includes face detection with two different algorithms for initialization. I learned a lot about ASM and AAM with it. </p>\n\n<p>Hope it helps!</p>\n",
        "system": ""
    },
    {
        "instruction": "Decomposition of Euclidean homography matrix",
        "input": "",
        "output": "<p>I've the missing step and concept and solved this problem. The chapter is correct. But some concepts are not very comprehensively explained. </p>\n\n<p>The input to Ma's book's flow is Euclidean homography whereas I fed in a Projective homography which results in a wrong answer. </p>\n",
        "system": ""
    },
    {
        "instruction": "Optimization of integral image",
        "input": "",
        "output": "<p>Start to check compiler settings, is it set to maximum performance? </p>\n\n<p>Than, depending from architecture, calculation of integral image have several bottleneck. </p>\n\n<ol>\n<li><p>Computations itself, some low cost CPU can't perform integer math with good performance. No solution. </p></li>\n<li><p>Data flow is not optimal. The solution is to provide optimal data flows ( number of sequential read and write streams). For example you can process 2 rows simultaneously. </p></li>\n<li><p>Data dependency of algorithm. On modern CPU it can be biggest problem. The solution is to change processing algorithm. For example calculate odd/even pixels without dependency (more calculations , less dependency).</p></li>\n<li><p>Processing can be done using GPU. </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Motion vectors calculation",
        "input": "",
        "output": "<p>When the object <code>hOpticalFlow</code> is constructed in the third line of the code, the <code>OutputValue</code> property is set to <code>'Horizontal and vertical components in complex form'</code> which has the effect that when you apply the <code>step</code> command to <code>hOpticalFlow</code> and the image (frame), you will not get just the magnitudes of the flowVectors, but complex numbers that represent these planar flow vectors. It is just a compact way for the command to return the information. Once you have the complex numbers in <code>ofVectors</code>, which is the output of the <code>step</code> command, the command</p>\n\n<pre><code>ua = real(ofVectors);\n</code></pre>\n\n<p>stores the horizontal component of each vector in <code>ua</code>. After the command</p>\n\n<pre><code>ia = ofVectors - ua;\n</code></pre>\n\n<p>is executed, <code>ia</code> contains the imaginary (i.e., vertical components of the flow vectors) because the real parts in <code>ua</code> are subtracted from the complex numbers in <code>ofVectors</code>. However, you need to get rid of the imaginary units in <code>ia</code>, so you divide by <code>0+1i</code>. This is what the command</p>\n\n<pre><code>va = ia/complex(0,1);\n</code></pre>\n\n<p>does.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to capture hair wisp structure from an image?",
        "input": "",
        "output": "<p>Your Sobels are the same while they supposed to have different code for x and y. 0, 1 and 1, 0.on top of that you loose resolution and sign by specifying cv8U as depth inSobel and only then converting to float. Also please provide input resolution and your outcome image. </p>\n",
        "system": ""
    },
    {
        "instruction": "Convolution after applying a gaussian filter kernel",
        "input": "",
        "output": "<p>There are 2 reasons:</p>\n\n<ol>\n<li><p><strong>Precision:</strong>\nIn the last line of your <code>imageFilter</code> method you are converting to the <code>uint8</code> data type (which is an 8-bit integer) and so you are basically sort of rounding your result:</p>\n\n<p><code>finalImg = uint8(finalImg);</code></p>\n\n<p>The native method <code>conv2</code> however is presented with a matrix <code>gaussKernel</code> of data type <code>double</code>, this does convolution in doubles and so in grater precision. </p></li>\n<li><p><strong>Boundary conditions</strong>: <code>conv2</code> has a third parameter (see <a href=\"http://www.mathworks.com/help/matlab/ref/conv2.html\" rel=\"nofollow\">docs</a>) that has to do with what the shape of the result. In your method you're calculating the result of the convolution with zero padding of the <strong>same</strong> size as the original image while <code>conv2</code> by default calculates the full convolution.</p></li>\n</ol>\n\n<p>So a simple modification will yield identical results:</p>\n\n<pre><code>gaussKernel = fspecial('gaussian', 3, 2);\ntestImage1 = imageFilter(im, gaussKernel);\ntestImage2 = uint8(conv2(im, gaussKernel, 'same'));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Getting real depth from disparity map",
        "input": "",
        "output": "<p>You need to perform a <strong><a href=\"https://docs.opencv.org/4.1.1/dc/dbb/tutorial_py_calibration.html\" rel=\"noreferrer\">camera calibration</a></strong> step first, to get a proper parameters, know as <em>camera matrix</em>.</p>\n\n<p>One you have these values, you can perform a proper computation of the <strong><a href=\"https://docs.opencv.org/4.1.1/dd/d53/tutorial_py_depthmap.html\" rel=\"noreferrer\">disparity map</a></strong> with the corrected values (the one got from the camera calibration step, know as <em>remapping</em> or <em>undistortion</em>) and then, to obtain the real depth (in mm or meters), you can finally do:</p>\n\n<pre><code>depth = baseline * focal / disparity\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to determine the number of weak classifiers in ada boost",
        "input": "",
        "output": "<p>To get the parameters of your model you should do cross-validation. </p>\n\n<p>If the size of your database allows it (you have sufficient samples) you could split your learning set to get a validation set. Cross validation works that way : you learn only to ~75% of your learning set and then test the score onto the remaining ~25%. And then you do that for different values of your parameters and pick the value that lead to the higest score. </p>\n\n<p>If the size of you database doesn't allow it you can do k-fold cross-validation (I won't explain it here but you can look it up on wikipedia). </p>\n\n<p>Scikit-learn implements a tool called gridsearch that will \"automatically\" do that if you provide it with the right things. </p>\n\n<p><a href=\"http://scikit-learn.org/stable/modules/grid_search.html\" rel=\"nofollow\">http://scikit-learn.org/stable/modules/grid_search.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Bad disparity map using StereoBM in OpenCV",
        "input": "",
        "output": "<p>It turned out that the problem was the visualization and not the data itself. Somewhere I read that <code>cv2.reprojectImageTo3D</code> required a disparity map as floating point values, which is why I was requesting <code>cv2.CV_32F</code> from <code>block_matcher.compute</code>.</p>\n\n<p>Reading the OpenCV documentation more carefully has led me to think that I was thinking this in error, and I'd actually like to work with integers than floats for the sake of speed, but the documentation for <code>cv2.imshow</code> wasn't clear on what it does with 16 bit signed integers (as compared to 16 bit unsigned), so for the visualization I'm leaving the values as floats.</p>\n\n<p>The <a href=\"http://docs.opencv.org/modules/highgui/doc/user_interface.html#imshow\" rel=\"noreferrer\">documentation of <code>cv2.imshow</code></a> reveals that 32 bit floating point values are assumed to be between 0 and 1, so they're multiplied by 255. 255 is the saturation point at which a pixel is displayed as white. In my case, this assumption produced a binary map. I manually scaled it to the range of 0-255 and then divided it by 255 in order to cancel out the fact that OpenCV does the same as well. I know, it's a horrible operation, but I'm only doing it in order to tune my <code>StereoBM</code> offline so performance is uncritical. The solution looks like this:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Other code as above\ndisp = block_matcher.compute(rectified_l, rectified_r, disptype=cv2.CV_32F)\nnorm_coeff = 255 / disp.max()\ncv2.imshow(\"disparity\", disp * norm_coeff / 255)\n</code></pre>\n\n<p>Then the disparity map looks okay.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to find point position relation between two images using homography?",
        "input": "",
        "output": "<p>Assume that you have a Point A with homogeneous coordinate :</p>\n\n<p><code>\n    A = [x, y, 1]\n</code>\nMaybe you also have an homography H like this:\n<code>\n    H = [h00 h01 h02;\n         h10 h11 h12;\n         h20 h21 h22]\n</code>\nThen do the product H * A' where A' is the transpose of A</p>\n\n<p>You will get something like this\n<code>\n   Bs = [X, Y, s]\n</code></p>\n\n<p>You have to normalize the result by s in order to have\n<code>\n    B = [Xb, Yb, 1]\n</code></p>\n",
        "system": ""
    },
    {
        "instruction": "Gaussian Pyramid Kernel &amp; Gaussian blur",
        "input": "",
        "output": "<p>If you're simply going to perform a blur, consider decomposing your 2D kernel into two 1D kernels for the sake of speed.</p>\n\n<p><a href=\"http://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm\" rel=\"nofollow\">http://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm</a></p>\n\n<p>The \"Pyramid\" context may be a bit misleading here; if you only want to blur an image and perhaps display the blurred image, or process it further, then image pyramids aren't necessarily relevant.</p>\n",
        "system": ""
    },
    {
        "instruction": "Kalman Filter and sudden measurements jumps",
        "input": "",
        "output": "<p>Before the answer, I want to be sure I got the problem you have right.</p>\n\n<p>You have measurements, some of them are good (Low measurement noise) yet others are outliers.<br>\nThe problem you're having is tuning the measurement noise covariance matrix.</p>\n\n<p>Practically, you tune for the good measurements.<br>\nOutliers measurements are rejected by using the Error Covariance.\nIf the innovation falls outside an ellipse you define using the Error Covariance Matrix the measurement is rejected.<br>\nWhenever a measurement is rejected you just apply the prediction step again and wait for another measurement.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to use points / scene corners of homography as input for ROI in another function",
        "input": "",
        "output": "<p>it looks as if the transform maps the object points to points that are outside the scene region. you would have to check and verify the transformed points.</p>\n",
        "system": ""
    },
    {
        "instruction": "Effect of variance (sigma) at Gaussian smoothing",
        "input": "",
        "output": "<p>I think it should be done in the following steps, first from the signal processing point of view:</p>\n\n<ol>\n<li>Gaussian Filter is a low pass filter. Low pass filters as their names imply pass low frequencies - keeping low frequencies. So when we look at the image in the frequency domain the highest frequencies happen in the edges(places that there is a high change in intensity and each intensity value corresponds to a specific visible frequency).</li>\n<li>The role of sigma in the Gaussian filter is to control the variation\naround its mean value. So as the Sigma becomes larger the more variance allowed around mean and as the Sigma becomes smaller the less variance allowed around mean.</li>\n<li>Filtering in the spatial domain is done through convolution. it simply\nmeans that we apply a kernel on every pixel in the image. The law exists for kernels. Their sum has to be zero.</li>\n</ol>\n\n<p>Now putting all together! When we apply a Gaussian filter to an image, we are doing a low pass filtering. But as you know this happen in the discrete domain(image pixels). So we have to quantize our Gaussian filter in order to make a Gaussian kernel. In the quantization step, as the Gaussian filter(GF) has a small sigma it has the steepest pick. So the more weights will be focused in the center and the less around it. </p>\n\n<p>In the sense of natural image statistics! The scientists in this field of studies showed that our vision system is a kind of Gaussian filter in the responses to the images. see for example take a look at a broad scene! don't pay attention to a specific point! so you see a broad scene with lots things in it. but the details are not clear! Now see a specific point in that seen. you see more details that previously you didn't. This is the Sigma appear here. when you increase the sigma you are looking to the broad scene without paying attention to the details exits. and when you decrease the value you will get more details.</p>\n\n<p>I think Wikipedia can help more than me, <a href=\"http://en.wikipedia.org/wiki/Low-pass_filter\" rel=\"noreferrer\">Low Pass Filters</a>, <a href=\"http://en.wikipedia.org/wiki/Gaussian_blur\" rel=\"noreferrer\">Guassian Blur</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Matlab 3D reconstruction",
        "input": "",
        "output": "<p>What you are looking for is called \"dense reconstruction\". The best way to do this is with calibrated cameras.  Then you can rectify the images, compute disparity for every pixel (in theory), and then get 3D world coordinates for every pixel. Please check out this <a href=\"http://www.mathworks.com/help/vision/ug/stereo-vision.html\" rel=\"nofollow\">Stereo Calibration and Scene Reconstruction</a> example.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to find straight line segments nearest to a straight line segment?",
        "input": "",
        "output": "<p>The geometry of segment/segment distance is not so simple.</p>\n\n<p>Imagine two line segments in general position, and dilate one of them. I mean, draw two parallel segments at a distance <code>d</code> and two half circles of radius <code>d</code> centered on the endpoints. This is the locus of constant distance <code>d</code> from the segment. You need to find the smallest <code>d</code> such that the other segment is hit.</p>\n\n<p>You can decompose the plane in three areas: the band between the two perpendiculars through the endpoints, and the two outer half planes. You can clip the other segment in these three areas and process the clipped segments separately.</p>\n\n<p>Inside the band, either the segments intersect and the distance is zero. Or they don't and the distance is the shortest distance between the line of support of the first segment and the endpoints of the other.</p>\n\n<p>Inside the half planes, the distance is the shortest of the distances between the considered endpoint and both endpoints of the other segment, and the distance between the endpoint and the other segment, provided then endpoint projects inside the other segment.</p>\n\n<p>ALTERNATIVE:</p>\n\n<p>Maybe it is easier to use the parametric equations of the two segments and minimize the (squared) distance, like:</p>\n\n<p><code>Min(p, q) ((Xa (1-p) + Xb p) - (Xc (1-q) + Xd q))^2 + ((Ya (1-p) + Yb p) - (Yc (1-q) + Yd q))^2</code> under constraints <code>0 &lt;= p, q &lt;=1</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to generate a sparse version of a dense point cloud",
        "input": "",
        "output": "<p>As an alternative to random sub-sampling mentioned in the first answer, you could try this:</p>\n\n<ol>\n<li>Compute the bounding box the point cloud (axis-aligned or oriented bounding box),</li>\n<li>Choose a cell size (the bounding box now contains W x H x D cells of this size),</li>\n<li>Hash all the points of the point cloud to their respective cell grid and keep only N point(s) maximum per cell (N >= 1), or simpler, just drop or keep every Nth point.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "My function&#39;s output differs from conv2() output",
        "input": "",
        "output": "<p>You are computing the correlation.  The 2D convolution is a 180 degree rotation (flip both axes) of the kernel.</p>\n\n<pre><code>h = rot90(h,2);\n</code></pre>\n\n<p>You won't notice the difference with a symmetric filter, like a Gaussian or an averaging kernel, but a derivative will be reversed in both directions.</p>\n",
        "system": ""
    },
    {
        "instruction": "Using the output of harris corner detector to detect corners (A new issue arises)",
        "input": "",
        "output": "<p>The equation for <code>C</code> seems wrong.  Try:</p>\n\n<pre><code>C = (Ix.*Iy).^2;  % was .^4\n</code></pre>\n\n<p>Also, you generally smooth <code>Ix</code> and <code>Iy</code> with a Gaussian. MATLAB filters <code>A</code>, <code>B</code> and <code>C</code>, with the 2D kernel <code>k = w(:)*w(:)'</code> where <code>w=fspecial('gaussian',[1 5],1.5);</code>.</p>\n\n<p>And you probably want to use <code>conv2(...,'same')</code>, or similarly with <code>filter2</code>.</p>\n\n<p>Then if you have the Image Processing Toolbox, you can use <code>BW = imregionalmax(cornerness,8);</code> to get a 2D mask of the maxima (corners). If you don't have the toolbox, follow <a href=\"https://stackoverflow.com/a/22218892/2778484\">this answer for finding local maxima in a 2D matrix</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to find new image corner points after a perspective transform?",
        "input": "",
        "output": "<p>To get the points after perspective transform, you can use the <code>perspectiveTransform</code> function from openCV with the same <code>transform</code> matrix you used in <code>warpPerspective</code> \nfor example</p>\n\n<pre><code>std::vector&lt;Point2f&gt; camera_corners,world_corners;\ncamera_corners.push_back(Point2f(0, 0));\ncamera_corners.push_back(Point2f(100, 100));\n....\nperspectiveTransform(camera_corners, world_corners, transform);\n</code></pre>\n\n<p>Now <code>world_corners</code> contains the warped points</p>\n",
        "system": ""
    },
    {
        "instruction": "Trying to translate formula for blending mode",
        "input": "",
        "output": "<p>Here is Overlay blending mode for Photoshop implementation , above formula work as follows for Grayscale image</p>\n\n<pre><code>Mat img1;\nMat img2;\nimg1 = imread(\"img1.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\nimg2 = imread(\"img2.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\nMat result(img1.size(), CV_32F);\n\nfor(int i = 0; i &lt; img1.size().height; ++i){\n    for(int j = 0; j &lt; img1.size().width; ++j){\n        float target = float(img1.at&lt;uchar&gt;(i, j)) / 255;\n        float blend = float(img2.at&lt;uchar&gt;(i, j)) / 255;\n        if(target &gt; 0.5){\n            result.at&lt;float&gt;(i, j) = (1 - (1-2*(target-0.5)) * (1-blend));\n        }\n        else{\n            result.at&lt;float&gt;(i, j) = ((2*target) * blend);\n        }\n    }\n}\n</code></pre>\n\n<p>and for color image you only need to use loop for color channels</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting almost straight lines",
        "input": "",
        "output": "<p>Apply canny edge detector to the image and do a labeling and you'll detect most of the rectangles.</p>\n\n<p>rgb : the image</p>\n\n<p>edges = edge(rgb2gray(rgb), 'canny');</p>\n\n<p>labels = label2rgb(bwlabel(edges, 8));</p>\n\n<p>figure, imshow(edges)</p>\n\n<p>figure, imshow(labels)</p>\n",
        "system": ""
    },
    {
        "instruction": "Given Three Points Compute Affine Transformation",
        "input": "",
        "output": "<p>Usually, an affine transormation of 2D points is experssed as</p>\n\n<pre><code>x' = A*x\n</code></pre>\n\n<p>Where <code>x</code> is a three-vector <code>[x; y; 1]</code> of original 2D location and <code>x'</code> is the transformed point. The affine matrix <code>A</code> is </p>\n\n<pre><code>A = [a11 a12 a13;\n     a21 a22 a23;\n       0   0   1]\n</code></pre>\n\n<p>This form is useful when <code>x</code> and <code>A</code> are <em>known</em> and you wish to recover <code>x'</code>.  </p>\n\n<p>However, you can express this relation in a different way.\nLet</p>\n\n<pre><code>X = [xi yi 1  0  0  0;\n      0  0 0 xi yi  1 ]\n</code></pre>\n\n<p>and <code>a</code> is a column vector</p>\n\n<pre><code>a = [a11; a12; a13; a21; a22; a23]\n</code></pre>\n\n<p>Then</p>\n\n<pre><code>X*a = [xi'; yi']\n</code></pre>\n\n<p>Holds for all pairs of corresponding points <code>x_i, x_i'</code>. </p>\n\n<p>This alternative form is very useful when you know the correspondence between pairs of points and you wish to recover the paramters of <code>A</code>.<br>\nStacking all your points in a large matrix <code>X</code> (two rows for each point) you'll have 2*n-by-6 matrix <code>X</code> multiplyied by 6-vector of unknowns <code>a</code> equals a 2*n-by-1 column vector of the stacked corresponding points (denoted by <code>x_prime</code>):</p>\n\n<pre><code>X*a = x_prime\n</code></pre>\n\n<p>Solving for <code>a</code>:</p>\n\n<pre><code>a = X \\ x_prime\n</code></pre>\n\n<p>Recovers the parameters of <code>a</code> in a least-squares sense.</p>\n\n<p>Good luck and stop skipping class!</p>\n",
        "system": ""
    },
    {
        "instruction": "Algorithms for Tracking moving objects with a moving camera",
        "input": "",
        "output": "<p>Type: 'zdenek kalal predator' to google.com and watch the videos, read the papers that came up. I think it will give you a lot of insight.</p>\n",
        "system": ""
    },
    {
        "instruction": "Count number of white pixels along a line in OpenCV",
        "input": "",
        "output": "<p>I ended up solving it by using OpenCV's line iterator as follows and I'm currently trying to rewrite my line params function to be better.</p>\n\n<pre><code>def lineWhiteness(line, image):\n    (pt1, pt2) = lineParams(line, len(image))\n    count = 0\n    li = cv.InitLineIterator(cv.fromarray(image), pt1, pt2)\n    for (r, g, b) in li:\n        if (r or g or b):\n            count += 1\n    return count\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "What is second derivative of Guassian?",
        "input": "",
        "output": "<p>To find edge magnitude (like the very basic edge detection methods) you can simply apply a gaussian using \"imfilter\"\nlike in </p>\n\n<pre><code>filter = fspecial('gaussian',h,sigma); %create a gaussian filter\nfiltered = imfilter(img, filter);  %gaussian convolved with image\nfigure;imshow(filtered,[]);        %you can see a blurred version of your image\n[fx,fy] = gradient(filtered);      %take that blurred image's derivative\n</code></pre>\n\n<p>Then find the magnitude of the derivative like in </p>\n\n<pre><code>edgemap = fx.*fx + fy.*fy;     \n</code></pre>\n\n<p>You can normalize this edgemap or apply thresholding if you wish.</p>\n",
        "system": ""
    },
    {
        "instruction": "Safe way to push back Mat objects to a vector",
        "input": "",
        "output": "<p>Changing the cv::Mat type from CV_32FC1 to CV_8UC1 does not change anything - you still did a header copy when adding to the vector. And this is expected and desired. By default, opencv avoids copying entire matrix data. If you would like to store elements in vector and they are different from each other <strong>by values</strong> you have to use clone(). It is like a proof. Let's say you are not using clone() and still want to distinguish matrices in vector by their values. It seems to me like a contradiction. \nOpencv Mat works like a smart pointer. It stores the data until the reference count for the variable goes 0. So safely, you can do following as you yourself noticed in the question:</p>\n\n<pre><code>b=K-M;\na.push_back(b.clone());\nb=K+M;\na.push_back(b);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Region of Interest in nighttime vehicle detection",
        "input": "",
        "output": "<p>I would turn the problem around and say that we are looking for headlights\nABOVE a certain line rather than saying that the headlights are below a certain line i.e. the horizon,</p>\n\n<p>Your images have a very high reflection onto the tarmac and we can use that to our advantage. We know that the maximum amount of light in the image is somewhere around the reflection and headlights. We therefore look for the row with the maximum light and use that as our floor. Then look for headlights above this floor.</p>\n\n<p>The idea here is that we look at the profile of the intensities on a row-by-row basis and finding the row with the maximum value. </p>\n\n<p>This will only work with dark images (i.e. night) and where the reflection of the headlights onto the tarmac is large.</p>\n\n<p>It will NOT work with images taking in daylight.</p>\n\n<p>I have written this in Python and OpenCV but I'm sure you can translate it to a language of your choice.</p>\n\n<pre><code>import matplotlib.pylab as pl\nimport cv2\n\n# Load the image\nim = cv2.imread('headlights_at_night2.jpg')\n\n# Convert to grey.\ngrey_image = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/0B9dF.png\" alt=\"grey_image\"></p>\n\n<p>Smooth the image heavily to mask out any local peaks or valleys\nWe are trying to smooth the headlights and the reflection so that there will be a nice peak. Ideally, the headlights and the reflection would merge into one area</p>\n\n<pre><code>grey_image = cv2.blur(grey_image, (15,15))\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/sZt6D.png\" alt=\"grey_blurred\"></p>\n\n<p>Sum the intensities row-by-row</p>\n\n<pre><code>intensity_profile = []\nfor r in range(0, grey_image.shape[0]):\n    intensity_profile.append(pl.sum(grey_image[r,:]))\n</code></pre>\n\n<p>Smooth the profile and convert it to a numpy array for easy handling of the data</p>\n\n<pre><code>window = 10\nweights = pl.repeat(1.0, window)/window\nprofile = pl.convolve(pl.asarray(intensity_profile), weights, 'same')\n</code></pre>\n\n<p>Find the maximum value of the profile. That represents the y coordinate of the headlights and the reflection area. The heat map on the left show you the distribution. The right graph shows you the total intensity value per row.</p>\n\n<p>We can clearly see that the sum of the intensities has a peak.The y-coordinate is 371 and indicated by a red dot in the heat map and a red dashed line in the graph. </p>\n\n<pre><code>max_value = profile.max()\nmax_value_location = pl.where(profile==max_value)[0]\nhorizon =  max_value_location\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/RujKd.png\" alt=\"\"></p>\n\n<p>The blue curve in the right-most figure represents the variable <code>profile</code></p>\n\n<p>The row where we find the maximum value is our floor. We then know that the headlights are above that line. We also know that most of the upper part of the image will be that of the sky and therefore dark. </p>\n\n<p>I display the result below.\nI know that the line in both images are on almost the same coordinates but I think that is just a coincidence.</p>\n\n<p><img src=\"https://i.sstatic.net/ayhRa.png\" alt=\"final1\">\n<img src=\"https://i.sstatic.net/zWh44.png\" alt=\"final2\"></p>\n",
        "system": ""
    },
    {
        "instruction": "undistortPoints() cannot handle lens distortions",
        "input": "",
        "output": "<p><strong>How to make your code work?</strong></p>\n\n<p>I am able to reproduce the described behavior using the code you provided, however, either one of the two following options solve the problem:</p>\n\n<ul>\n<li><p>Replace <code>const Point3f tvec(10, 20, 30);</code> by  <code>const Point3f tvec(10, 20, N);</code>  where <code>N</code> is <em>much lower</em> than 0 (e.g. -300) or <em>much larger</em> than 100 (e.g. 300).</p></li>\n<li><p>Replace your call to <code>solvePnP</code> by a call to <code>solvePnPRansac</code>.</p></li>\n</ul>\n\n<p><strong>Why does each of these changes fix the undesired behavior?</strong></p>\n\n<p>First, consider what your original code requests from the <code>solvePnP</code> function. You are using a rotation of rather small magnitude, hence for simplicity of the explanation, I will assume that the rotation is identity. Then, the camera is positionned at world coordinates X=10, Y=20 and Z=30 and you generate object points randomly with world coordinates (X,Y,Z) uniformly drawn in [0,100]<sup>3</sup>. <strong>Hence, the camera is in the middle of the possible range for the object points</strong>, as illustrated on the following picture:</p>\n\n<p><img src=\"https://i.sstatic.net/hlZUA.png\" alt=\"enter image description here\"></p>\n\n<p>This means that object points may be generated very close to the focal plane (i.e. the plane going through the optical center and perpendicularly with respect to the optical axis). The projection in the camera image for such object points is undefined. <strong>However, in practice the non-linear optimization algorithm for <code>undistortPoints</code> is unstable even for object points close to the focal plane</strong>. This unstability causes the iterative algorithm for <code>undistortPoints</code> to diverge, except when the coefficients are all zero since in that case the initial values remain strictly constant during the estimation.</p>\n\n<p>Hence, the two possible solutions to avoid this behavior are the following:</p>\n\n<ul>\n<li><p>Avoid generating object points near the focal plane of the camera, i.e. change the translation vector <em>or</em> the range of the coordinates of the object points.</p></li>\n<li><p>Eliminate the object points too close to the focal plane of the camera, whose undistorted estimation diverged (outliers), before the PnP estimation for example using <code>solvePnPRansac</code>.</p></li>\n</ul>\n\n<hr>\n\n<p><strong>Details about why <code>undistortPoints</code> fails:</strong></p>\n\n<p>NB: As we know the 3D world points, I used the following call to obtain the true undistorted coordinates, independently from the result of <code>undistortPoints</code>:</p>\n\n<pre><code>cv::projectPoints(obj_pts, rvec, tvec, cv::Mat_&lt;double&gt;::eye(3,3), cv::Mat_&lt;double&gt;::zeros(5,1), true_norm_pts);\n</code></pre>\n\n<p>The following function is a simplified version of what <code>undistortPoints</code> is doing:</p>\n\n<pre><code>void simple_undistort_point(const cv::Mat &amp;img_pt,\n                            const cv::Mat_&lt;double&gt; &amp;K,\n                            const cv::Mat_&lt;double&gt; &amp;D,\n                            cv::Mat &amp;norm_pt)\n{\n    // Define temporary variables\n    double k[8]={D.at&lt;double&gt;(0),\n                 D.at&lt;double&gt;(1),\n                 D.at&lt;double&gt;(2),\n                 D.at&lt;double&gt;(3),\n                 D.at&lt;double&gt;(4)},\n           fx, fy, ifx, ify, cx, cy;\n    fx = K.at&lt;double&gt;(0,0);\n    fy = K.at&lt;double&gt;(1,1);\n    ifx = 1./fx;\n    ify = 1./fy;\n    cx = K.at&lt;double&gt;(0,2);\n    cy = K.at&lt;double&gt;(1,2);\n    // Cancel distortion iteratively\n    const int iters = 5;\n    double x, y, x0, y0;\n    x0=x=(img_pt.at&lt;double&gt;(0)-cx)*ifx;\n    y0=y=(img_pt.at&lt;double&gt;(1)-cy)*ify;\n    for(int j = 0; j &lt; iters; ++j)\n    {\n        double r2 = x*x + y*y;\n        double icdist = 1/(1 + ((k[4]*r2 + k[1])*r2 + k[0])*r2);\n        double deltaX = 2*k[2]*x*y + k[3]*(r2 + 2*x*x);\n        double deltaY = k[2]*(r2 + 2*y*y) + 2*k[3]*x*y;\n        x = (x0 - deltaX)*icdist;\n        y = (y0 - deltaY)*icdist;\n    }\n    // Store result\n    norm_pt.create(1,2,CV_64F);\n    norm_pt.at&lt;double&gt;(0) = x;\n    norm_pt.at&lt;double&gt;(1) = y;\n}\n</code></pre>\n\n<p>If you add code to check how <code>x</code> and <code>y</code> change with each iteration, you'll see that the iterative optimization diverges due to <code>r2</code> being very large at the beginning. Here is a log example:</p>\n\n<pre><code>#0:   [2.6383300, 1.7651500]    r2=10.0766000, icdist=0.0299408, deltaX=0, deltaY=0\n#1:   [0.0789937, 0.0528501]    r2=0.00903313, icdist=0.9892610, deltaX=0, deltaY=0\n#2:   [2.6100000, 1.7462000]    r2=9.86128000, icdist=0.0309765, deltaX=0, deltaY=0\n#3:   [0.0817263, 0.0546783]    r2=0.00966890, icdist=0.9885120, deltaX=0, deltaY=0\n#4:   [2.6080200, 1.7448800]    r2=9.84637000, icdist=0.0310503, deltaX=0, deltaY=0\nend:  [0.0819209, 0.0548085]\ntrue: [0.9327440, 0.6240440]\n</code></pre>\n\n<p>When <code>r2</code> is large, <code>r2*r2*r2</code> is huge hence <code>icdist</code> is very small, hence the next iteration starts with a very small <code>r2</code>. When <code>r2</code> is very small, <code>icdist</code> is close to 1, hence <code>x</code> and <code>y</code> are respectively set to <code>x0</code> and <code>y0</code> and we are back with a large <code>r2</code>, etc.</p>\n\n<p>So why is <code>r2</code> so large in the first place? Because the points may be generated close to the focal plane, in which case they are far from the optical axis (hence a very large <code>r2</code>). See the following log example:</p>\n\n<pre><code>img_pt#0=[991.4992804037340, 629.5460091483255], r2=10.07660, norm(cv_undist-true)=1.0236800\nimg_pt#1=[5802.666489402056, 4402.387472311543], r2=554.4490, norm(cv_undist-true)=2.1568300\nimg_pt#2=[5040.551339386630, 5943.173381042060], r2=639.7070, norm(cv_undist-true)=2.1998700\nimg_pt#3=[741.9742544382640, 572.9513930063181], r2=5.749100, norm(cv_undist-true)=0.8158670\nimg_pt#4=[406.9101658356062, 403.0152736214052], r2=1.495890, norm(cv_undist-true)=0.1792810\nimg_pt#5=[516.2079583447821, 1038.026553216831], r2=10.88760, norm(cv_undist-true)=1.0494500\nimg_pt#6=[1876.220394606081, 8129.280202695572], r2=747.5450, norm(cv_undist-true)=2.2472900\nimg_pt#7=[236.9935231831764, 329.3418854620716], r2=0.599625, norm(cv_undist-true)=0.0147487\nimg_pt#8=[1037.586015858139, 1346.494838992490], r2=25.05890, norm(cv_undist-true)=1.2998400\nimg_pt#9=[499.9808133105154, 715.6213031242644], r2=5.210870, norm(cv_undist-true)=0.7747020\n</code></pre>\n\n<p>You can see that for most points, <code>r2</code> is very large, except for a few (#3, #4 &amp; #7) which are also those associated with the best undistortion accuracy.</p>\n\n<p>This problem is due to the particular undistortion algorithm implemented in OpenCV, which has been chosen for its efficiency. Other non-linear optimization algorithm (e.g. Levenberg-Marquardt) would be more accurate but also much slower, and would definitely be an overkill in most applications.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculate rotation/translation matrix to match measurement points to nominal points",
        "input": "",
        "output": "<p>This is indeed a job for a rigid registration algorithm. </p>\n\n<p>In order to handle your tolerances you have a couple of options:</p>\n\n<ul>\n<li>Simple option: Run rigid registration, check afterwards if result is within tolerances</li>\n<li>Bit harder option: Offset your points in the CAD, where you have imbalanced tolerances\nthe rest the same as the previous option.</li>\n<li>Hardest option: What you probably want to do is and have the offset as in the second option, and also add a weight function based on measured position and set tolerance. This weight function should effect the energy function in such a way, that the individual function vectors are larger when you have a small tolerance and smaller when you have a larger tolerence.</li>\n</ul>\n\n<p>So now about implementation, for options 1 and 2 your fastest way to result would probably be:</p>\n\n<ul>\n<li>Use PCL C++ version in a visual studio 2010 environment. There's lots of information about installation of PCL and VS2010 and get it running. Also PCL has a nice ICP registration tutorial that should get you going.</li>\n<li>Use VTK for python, it has an ICP algorithm:<br>\n<a href=\"https://stackoverflow.com/questions/15383666/installing-vtk-for-python\">Installing VTK for Python</a><br>\n<a href=\"http://www.vtk.org/Wiki/VTK/Examples/Python/IterativeClosestPoints\" rel=\"nofollow noreferrer\">http://www.vtk.org/Wiki/VTK/Examples/Python/IterativeClosestPoints</a></li>\n</ul>\n\n<p>If your really want option 3 you can do:</p>\n\n<ul>\n<li>Make the weight function in PCL library source code and compile it</li>\n<li>Make the complete ICP algorithm yourself in .net:\n\n<ul>\n<li><a href=\"http://www.math.tau.ac.il/~dcor/Graphics/adv-slides/ICP.ppt\" rel=\"nofollow noreferrer\">http://www.math.tau.ac.il/~dcor/Graphics/adv-slides/ICP.ppt</a></li>\n<li>Use math.numerics sparse matrix/vector algebra and solvers to create your own optimizer</li>\n<li>Realize the Lev-Marq or Gauss-Newton optimizer from:<br>\nimm methods for non-linear least squares problems, K. Madsen, 2004</li>\n<li>Generate your own function vector and jacobian matrix (with weight function)</li>\n<li>Have quite some patience to get is all to work together :)</li>\n<li>Post the result for the others on StackOverflow that are waiting for ICP in C# .net</li>\n</ul></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Measuring similarity of hand-drawn curve to template, penalizing scale/rotation",
        "input": "",
        "output": "<p>A good choice is <em>Chamfer matching</em>. It is quite robust, but not invariant to transformations, as you require. The input is a point set so it doesn't matter if it's a closed curve or not, and the direction of the stroke doesn't matter either.</p>\n\n<p>Although it's a very standard method, I didn't find a Wikipedia article; <a href=\"http://www.gavrila.net/Research/Chamfer_System/chamfer_system.html\" rel=\"nofollow\">this</a> is the best source I found, altough it's actually discussing an extended method.</p>\n\n<p>It should be in OpenCV but again I didn't find any documentation, only <a href=\"http://fossies.org/dox/opencv-2.4.8/chamfer_8cpp_source.html\" rel=\"nofollow\">this sample</a>. But in any case if you have an implementation of a <a href=\"http://opencv-code.com/tutorials/count-and-segment-overlapping-objects-with-watershed-and-distance-transform/#more-476\" rel=\"nofollow\">distance transform</a>, it's trivial to write yourself.</p>\n\n<p><a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/classes/cs294_people_places_things/2004sp/cs294/borgefors88.pdf\" rel=\"nofollow\">Here</a> is the original publication, for reference.</p>\n",
        "system": ""
    },
    {
        "instruction": "How Sobel Mask is derived?",
        "input": "",
        "output": "<p>The Sobel Kernel is a convolution of the derivation kernel [-1 0 1] with a smoothing kernel [1 2 1]'. The former is straightforward, the later is rather arbitrary - you can see it as some sort of discrete implementation of a 1D Gaussian of a certain sigma if you want.</p>\n",
        "system": ""
    },
    {
        "instruction": "How is subpixel accuracy obtained with phase shifting in structured Light?",
        "input": "",
        "output": "<p>To understand the resolution limits in structured light systems, I'll give the following scenarios:</p>\n\n<ul>\n<li>If the projector displays pseudo-random dots/codewords so they can be used to find the correspondence using the triangulation then we are limited by the projector\u2019s resolution in both the u and v directions.</li>\n<li>If the projector displays binary patterns, then it could achieve higher spatial resolution. This is because such a technique uses vertical or horizontal stripes for codification, its spatial resolution is only limited by the projector\u2019s resolution in either u or v direction, but not both.</li>\n<li>Higher spatial resolution could be achieved by using continuous patterns in both directions, and the patterns are typically sinusoidal in nature (no longer limited by projector's specification) (instead of finding corresponding point using intensity of the structured patterns, it uses phase as a constraint to solve for (x;y;z) coordinates\npixel by pixel if the system is calibrated).</li>\n</ul>\n\n<p>The phase-shifting based method (digital fringe projection using sinusoidal patterns) allows precise sub-pixel correspondence between the projector and the camera without any interpolation. Therefore, theoretically, it could achieve highly accurate 3D shape measurement if calibration is properly performed. </p>\n\n<p>For more information, check the following references:</p>\n\n<p>S. Zhang, \u201cRecent progresses on real-time 3-D shape measurement using digital fringe projection techniques,\u201d Opt. Laser Eng. 48(2), 149\u2013158 (2010).</p>\n\n<p>W. Lohry, V. Chen, and S. Zhang, \"Absolute three-dimensional shape measurement using coded fringe patterns without phase unwrapping or projector calibration,\" Opt. Express  22, 1287-1301 (2014).</p>\n",
        "system": ""
    },
    {
        "instruction": "How to compute a 3d miniature model from a large set of 3d geometric models",
        "input": "",
        "output": "<p>An option is to generate side views of the building at a suitable resolution, using the rendering engine and map them as textures to a parallelipipoid.</p>\n\n<p>The next level of refinement is to obtain a bump or elevation map that you can use for embossing. Not the easiest to do.</p>\n\n<p>If the modeler allows it, you can slice the volume using a 2D grid of \"voxels\" (actually prisms). You can do that by repeatedly cutting the model in two with a plane. And in every prism, find the vertex closest to the observer. This will give you a 2D map of elevations, with the desired resolution.</p>\n\n<p>Alternatively, intersect parallel \"rays\" (linear objects) with the solid and keep the first endpoint.</p>\n\n<p>It can also be that your modeler includes a true voxel model, or that rendering can be zone with a Z-buffer that you can access.</p>\n",
        "system": ""
    },
    {
        "instruction": "Single Color Enhancement - MATLAB",
        "input": "",
        "output": "<ol>\n<li>I think saturation is not the correct approach, at least not initially. As you said, it enhances all colors, and not red specifically. But more importantly, you will probably need to differentiate between red patches corresponding to traffic lights and to other objects (you have some excellent examples in your image - road signs, taillights, pedestrian lights), and saturation might lead to loss of information which could be helpful for this filtering.</li>\n<li><p>To enhance the red channel only, you should work in RGB rather than HSV. You can do something like this:</p>\n\n<pre><code>r_channel = img_rgb(:,:,1);\nr_channel = uint8(double(r_channel * 2));\n</code></pre>\n\n<p>You should note, however, that this can lead as well to a loss of information in your most important channel, as explained in the previous item.</p></li>\n<li><p>This is caused by the saturation values exceeding 1 when you compute <code>s_channel = s_channel * 10;</code>. Try adding <code>s_channel = s_channel / max(s_channel(:));</code> after that line. The output image from hsv2rgb should be in the range [0,1]. From there bring it back to [0 255] by rescaling. Be sure to bring the image back to its original datatype as well (most likely uint8): <code>img_rgb_enhanced = uint8(img_rgb_enhanced * 255);</code>.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Converting image to numpy array in python",
        "input": "",
        "output": "<p>Writing without thinking may be dangerous but in your case it is just wrong. The page you've mentioned shows this:</p>\n\n<pre><code>&gt;&gt;&gt; lena = misc.imread('lena.png')\n&gt;&gt;&gt; type(lena)\n&lt;type 'numpy.ndarray'&gt;\n&gt;&gt;&gt; lena.shape, lena.dtype\n((512, 512), dtype('uint8'))\n</code></pre>\n\n<p><code>&gt;&gt;&gt;</code> is Python's interactive console prompt string. It is there were commands go.</p>\n\n<p><code>&lt;type 'numpy.ndarray'&gt;</code> and <code>((512, 512), dtype('uint8'))</code> are <strong>results</strong> of commands. So your corresponding code should be only</p>\n\n<pre><code>tree = misc.imread('C:\\\\Users\\\\app\\\\Pictures\\\\treephoto1.jpg')\ntype(tree)\ntree.shape, tree.dtype\n</code></pre>\n\n<p>Notice that</p>\n\n<pre><code>type(tree)\ntree.shape, tree.dtype\n</code></pre>\n\n<p>do nothing and just show you information about your image data.</p>\n\n<p><strong>Update</strong></p>\n\n<p>Basically (not always) your image is layered. RGB is three separate layers. So if your filtering isn't aware of it you'll have to separate layers yourself.</p>\n\n<p>Hope that helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "Feasible computer vision techniques for detecting and classifying animal faces",
        "input": "",
        "output": "<p>Note that predators  need binocular vision and thus their eyes are located in front (humans included). Prey have eyes on the sides for larger field of view and early detection of predators. I would concentrate on predators and try to find eyes for some reasonably homogeneous species. Handling a wide variety of animals from whales to insects sounds a bit over-optimistic. </p>\n\n<p>Speaking of human face detection - it is not only face detection but also discarding non-faces about which in your case you did not say a word and may be not even gave a thought. Since you want to separate the face form background, You cannot concentrate on positive examples only unless they are very different from background which is not the case when animal are very heterogeneous. Thus segmentation of an animal face in general case would be a hard task.</p>\n",
        "system": ""
    },
    {
        "instruction": "I&#39;m trying to do real time object detection and tracking in MATLAB. But it&#39;s giving me error",
        "input": "",
        "output": "<p>Your problem is that you are trying to use <code>vision.VideoFileReader</code>, while trying to read frames from a camera.  <code>vision.VideoFileReader</code> is only for reading video files.  If you are getting frames from the camera you do not need it at all.  You should add your <code>videoinput</code> object to the <code>obj</code> struct, and you should try using <code>getsnapshot</code> inside <code>readFrame()</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "MATLAB image() is rotated after hold on",
        "input": "",
        "output": "<p>The rows of an image are stored from top to bottom, you may use</p>\n\n<pre><code>hold on,image([1 size(img,1)],[size(img,2) 1],img)\n</code></pre>\n\n<p>to reverse the image along y-axis.</p>\n\n<p>To reverse the loaded matrix at the beginning, use (row reverse)</p>\n\n<pre><code>img1 = flipdim(img,1);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Hu moments and SVM does not work",
        "input": "",
        "output": "<p>I hope the following suggestions can help you\u2026..</p>\n\n<ol>\n<li><p>The simplest task is to use a clustering algorithm and try to cluster the data into two classes. If an algorithm like \u2018k-means\u2019 can do the job why make things complex by using SVM and Neural Nets. I suggest you use this technique because your feature vector dimension is of a very small size (7 Hu Moments) as well as your number of samples.</p></li>\n<li><p>Perform feature Normalization (specified in point 4) to make sure the values fall in a limited range.</p></li>\n<li><p>Check out \u201cis your data really separable?\u201d As your data is small, take a few samples from positive images and a few samples from negative images and plot the feature vectors. If you can visually see the difference surely any learning algorithm can do the job for you. As I said earlier simple tricks can do better than complex math. </p></li>\n<li><p>Only if you then decide to use SVM you should know the following:</p>\n\n<p>\u2022    As I can see from your code you are using a Linear SVM, may be your data is non-separable by a linear kernel. Try using some polynomial kernel or other kernels. There is one option bool CvSVM::train_auto in openCV just have a look.</p>\n\n<p>\u2022    Try to check whether the feature vector values you are getting are proper values or not (make sure that they are not some garbage values).</p>\n\n<p>\u2022    Also you can perform feature normalization \u201cZERO MEAN and UNIT VARIENCE\u201d before you use it for training.</p>\n\n<p>\u2022    Most importantly increase the number of images for training, both positively and negatively labeled.</p>\n\n<p>\u2022    Last but not least SVM is not magic, at the end of the day it is just drawing a line between two sets of points. So don\u2019t expect it to classify anything you give it as input.</p></li>\n<li><p>If nothing works \u201cJust improve your feature extraction technique\u201d</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How to proceed the masked image",
        "input": "",
        "output": "<p>try this one:</p>\n\n<pre><code>int main()\n{\ncv::Mat img = imread (\"E:\\\\lena.jpg\");\ncv::Mat mask = cv::Mat::zeros(img.rows, img.cols, CV_8UC1);\ncv::ellipse(mask,cv::Point(mask.cols/2,mask.rows/2),cv::Size(mask.cols/2,mask.rows/2),0,0,360,cv::Scalar(255), CV_FILLED, 8,0);\n\ncv::imshow(\"mask\", mask);\n\ncv::Mat result = cv::Mat(img.rows, img.cols, CV_8UC1, img.type());\nresult.setTo(cv::Scalar(0,0,0));\n\nimg.copyTo(result, mask);\ncv::imshow(\"result\", result);\ncv::waitKey(-1);\nreturn 0;\n}\n</code></pre>\n\n<p>input: </p>\n\n<p><img src=\"https://i.sstatic.net/k1g0d.jpg\" alt=\"enter image description here\"></p>\n\n<p>computed mask:</p>\n\n<p><img src=\"https://i.sstatic.net/9QPCt.png\" alt=\"enter image description here\"></p>\n\n<p>result:</p>\n\n<p><img src=\"https://i.sstatic.net/NSQEj.png\" alt=\"enter image description here\"></p>\n\n<p>idea: create a black image and copy only the masked area to it. \nThis is the line <code>img.copyTo(result, mask);</code>. openCV masks are 1-channel CV_8U images of same size as the image.</p>\n\n<p>if you want to manipulate pixel only in the mask region in general, you can do it like this:</p>\n\n<pre><code>loop over y and x coordinate of the image\n    if(mask.at&lt;unsigned char&gt;(cv::Point(x,y)) != 0)\n        manipulate the pixel in img\n    else\n        do not manipulate the pixel\n</code></pre>\n\n<p>Here is another example where you can see different things:\n1. how to create a background mask if you have a foreground mask\n2. how to loop over foreground/background only and perform some tasks on those pixel\n3. how to copy from images, using a mask</p>\n\n<pre><code>int main()\n{\n// load image:\ncv::Mat img = cv::imread (\"lena.jpg\");\n\n// create the foreground mask in form of an ellipse:\ncv::Mat foregroundMask = cv::Mat::zeros(img.rows, img.cols, CV_8UC1);\ncv::ellipse(foregroundMask,cv::Point(foregroundMask.cols/2,foregroundMask.rows/2),cv::Size(foregroundMask.cols/2,foregroundMask.rows/2),0,0,360,cv::Scalar(255), CV_FILLED, 8,0);\n\ncv::Mat foreground = img.clone();\n\n// create the background mask which is just everything that is not foreground\ncv::imshow(\"mask\", foregroundMask);\ncv::Mat backgroundMask = 255-foregroundMask;\n\n// create a background and give it some color. this could be another loaded image instead.\ncv::Mat background = cv::Mat(img.rows, img.cols, img.type());\n// give the background some color. here white for example.\nbackground.setTo(cv::Scalar(255,255,255));\n\n// do some computation on the background image, but only where the background mask is not zero!\nfor(int y=0; y&lt;backgroundMask.rows; ++y)\n    for(int x=0; x&lt;backgroundMask.cols; ++x)\n    {\n        cv::Point pixelPos(x,y);\n        if(backgroundMask.at&lt;unsigned char&gt;(pixelPos))\n        {\n            // manipulate the background\n            // I choose to set every 8th pixel in a random color, you could do any filter or something:\n            if(x%8 == 0)\n            {\n                // create random color\n                cv::Vec3b randomColor(rand()%255, rand()%255, rand()%255);\n                // use .at&lt;Vec3b&gt; for 24 bit BGR values\n                background.at&lt;cv::Vec3b&gt;(pixelPos) = randomColor;\n            }\n        }\n        else\n        {\n            // you could process any pixel here which is NOT in your background mask, but I'll process foreground explicitly later\n        }\n    }\n\n\n// just in case that you want to modify the foreground too, here's an example:\nfor(int y=0; y&lt;foregroundMask.rows; ++y)\n        for(int x=0; x&lt;foregroundMask.cols; ++x)\n        {\n            cv::Point pixelPos(x,y);\n            if(foregroundMask.at&lt;unsigned char&gt;(pixelPos))\n            {\n                // manipulate the background\n                // for example, set every 12th row to blue color:\n                if(y%12 == 0)\n                {\n                    // create random color\n                    cv::Vec3b blueColor(255, 0, 0);\n                    // use .at&lt;Vec3b&gt; for 24 bit BGR values\n                    foreground.at&lt;cv::Vec3b&gt;(pixelPos) = blueColor;\n                }\n            }\n            else\n            {\n                // you could process any pixel here which is NOT in your foreground mask, but I've processed background explicitly earlier\n            }\n        }\n\n\ncv::imshow(\"modified background\", background);\ncv::imshow(\"modified foreground\", foreground);\n\n// this is how to copy something using masks:\n\ncv::Mat result;\n// copy background masked pixel from background image to the result:\nbackground.copyTo(result, backgroundMask);\n// copy foreground masked pixel from foreground image to the result:\nforeground.copyTo(result, foregroundMask);\n\n\ncv::imshow(\"result\", result);\ncv::waitKey(-1);\nreturn 0;\n}\n</code></pre>\n\n<p>This is how the images look like:</p>\n\n<p>input like before:</p>\n\n<p><img src=\"https://i.sstatic.net/fqaix.jpg\" alt=\"enter image description here\"></p>\n\n<p>foreground mask: it's just the ellipse that we painted:</p>\n\n<p><img src=\"https://i.sstatic.net/pjuOz.png\" alt=\"enter image description here\"></p>\n\n<p>background mask: it's all that is not foreground:</p>\n\n<p><img src=\"https://i.sstatic.net/tCtib.png\" alt=\"enter image description here\"></p>\n\n<p>the original background image is just a white image in my example.\nthis is the modified background image, where some of the masked background images are modified and everything that is not in the background mask is unmodified (see code):</p>\n\n<p><img src=\"https://i.sstatic.net/lY7lP.png\" alt=\"enter image description here\"></p>\n\n<p>now the modified foreground image, see that no pixel that is not masked as foreground isn modified</p>\n\n<p><img src=\"https://i.sstatic.net/qZuTx.png\" alt=\"enter image description here\"></p>\n\n<p>and finally after using the <code>.copyTo()</code> with using masks:</p>\n\n<p><img src=\"https://i.sstatic.net/eO0b1.png\" alt=\"enter image description here\"></p>\n\n<p>you can see easily that you dont even have to copy both of the images, if you copy one into the other and many other easy simplifications are possible, that code is just to demonstrate how you can use masks in many different ways.</p>\n\n<p>hope it helps and you can do whatever you want after understanding the code =)</p>\n",
        "system": ""
    },
    {
        "instruction": "Disparity calculation of two similar images in matlab",
        "input": "",
        "output": "<p>Rectification guarantees that matches are to be found in the same row (for horizontally separated cameras). If you have doubts about rectification of your images you can try to compare rows by drawing horizontal lines between horizontally separated images. If the lines hit the same features you are fine, see the picture below where images are NOT rectified. The fact that they are distorted means there was a lens distortion correction as well as attempted (but not actually performed correctly) rectification. </p>\n\n<p>Now, let\u2019s see what you meant by the <strong>same images</strong>. Did you mean the images of the same object that were taken from different viewpoints? Note that if the images are literally the same (the same viewpoints) the disparity will be zero as was noted in another answer. The definition of disparity (for horizontally separated cameras) is a value of shift (in the same row) between matching features. The disparity is related to depth (if optical axes of cameras are parallel) as disparity d=f*B/z, where z - depth, B - baseline or separation between cameras and f is a focal length. You can transform the formula above into disparity/B=f/z which basically says that disparity related to camera separation as focal length is related to distance. In other words, the ratios of horizontal and distance measures are equal.</p>\n\n<p><img src=\"https://i.sstatic.net/9fS8L.png\" alt=\"enter image description here\"></p>\n\n<p>If your images are taken with the cameras shifted horizontally the disparity (in a simple correlation algorithm) is typically calculated in 5-embedded loops:</p>\n\n<pre><code>loop over image1 y  \n   loop over image1 x  \n      loop over disparity d  \n         loop over correlation window y  \n            loop over correlation window x  \n</code></pre>\n\n<p>Disparity, or D_best, gives you the best matching window between image1 and image2 across all possible values of d. Finally, scatterplots are for 3D point clouds while disparity can be rather visualized as a heat color map. If you need to visualize 3D reconstruction or simply saying a 3D point cloud calculate X, Y, Z as:\nZ=f<em>B/D, X=u</em>Z/f, Y=v*Z/f, where u and v are related to column and row of wxh image as\nu=col-w/2 and v=h/2-row, that is u, v form an image centered coordinate system.</p>\n",
        "system": ""
    },
    {
        "instruction": "Operation on specific part of image",
        "input": "",
        "output": "<p>Its solution can be found <a href=\"http://answers.opencv.org/question/31132/operation-on-specific-part-of-image/?answer=31139#post-id-31139\" rel=\"nofollow\">here</a> given by the @haris</p>\n\n<p>Solution :</p>\n\n<ol>\n<li><p>Draw your rectangle to new binary image with thickness=CV_FILLED</p></li>\n<li><p>Invert above binary image and create mask for your interested region.</p></li>\n<li><p>Copy the source to new Mat and perform your operation.</p></li>\n<li><p>Finally copy the processed image to your source image using the same mask.</p></li>\n</ol>\n\n<p>I don't know what process you are going to do, and if it not use the neighbor pixel like filter, the above may work fine.</p>\n\n<p>Edit:</p>\n\n<p>While doing filter operation using above method, the black region in the mask boundary will also consider as a neighbor, so better approach is</p>\n\n<ol>\n<li><p>Create mask using your Rect.</p></li>\n<li><p>Copy source to new Mat using above mask((the part you need to exclude).</p></li>\n<li><p>Process the entire source.</p></li>\n<li><p>Later copy back the region of exclusion to processed image using above mask.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How to generate 3D point cloud data file from multiple images of the object?",
        "input": "",
        "output": "<p>Like Naveh said, in OpenCV the building blocks are there, but putting it together is something you would have to do.</p>\n\n<p>That being said, people have generated a number of SfM tools in both C++ and Matlab. Depending on your goals there are a number of prepackaged things you can look at:</p>\n\n<p>-There is a <a href=\"https://code.google.com/p/vincents-structure-from-motion-matlab-toolbox/\" rel=\"nofollow\">SfM Matlab Toolbox here</a>, I have not personally used it but I've seen it a number of times.</p>\n\n<p>-If you are just looking for a black-box solution, check out <a href=\"http://ccwu.me/vsfm/\" rel=\"nofollow\">Visual SfM</a>, it is a GUI-fied version of a common SfM workflow. </p>\n\n<p>-A while ago I put together a guide for installing the Visual SfM components individually <a href=\"http://dirsapps.cis.rit.edu/3d-workflow/install.html\" rel=\"nofollow\">on Fedora</a>, if you wanted to dig into them. I'm not sure how relevant it is now but it might help. </p>\n\n<p>Regardless, you should certainly educate yourself on the processes involved in creating 3D structure from imagery. It is a complicated process with many details which need to be understood. </p>\n",
        "system": ""
    },
    {
        "instruction": "Project image onto notebook using OpenCV",
        "input": "",
        "output": "<p>You want an effect that is called a key stone correction. The problem you are experiencing is most probably due to the fact that optical axes, positions, and focal lengths of a web camera and a projector are different. I suggest to calibrate your setup so you would know their relative pose and incorporate it in your inverse Homography. </p>\n",
        "system": ""
    },
    {
        "instruction": "Decomposition of Essential Matrix: Validation of the four possible solutions for R and T",
        "input": "",
        "output": "<p>There is a theoretical ambiguity when reconstructing the relative euclidian poses of two cameras from their fundamental matrix. This ambiguity is linked to the fact that, given a 2D point in an image, the classic pinhole camera model cannot tell whether the corresponding 3D point is in front of the camera or behind the camera. In order to remove this ambiguity, you need to know one point correspondence in the images: as these two 2D points are assumed to be the projections of a single 3D point lying in front of both cameras (since it is visible in both images), this will enable choosing the right R and T.</p>\n\n<p>For that purpose, one method is explained in \u00a7 6.1.4 (p47) of the following PhD thesis: \"Geometry, constraints and computation of the trifocal tensor\", by C.Ressl (<a href=\"http://www.ipf.tuwien.ac.at/phdtheses/diss_car_03.pdf\" rel=\"nofollow\">PDF</a>). The following gives the outline of this method. I'll denote the two corresponding 2D points by x1 and x2, the two camera matrices by K1 and K2 and the essential matrix by E12.</p>\n\n<p>i. Compute the SVD of the essential matrix <code>E12 = U * S * V'</code>. If <code>det(U) &lt; 0</code> set <code>U = -U</code>. If <code>det(V) &lt; 0</code> set <code>V = -V</code>.</p>\n\n<p>ii. Define <code>W = [0,-1,0; 1,0,0; 0,0,1]</code>, <code>R2 = U * W * V'</code> and <code>T2 = third column of U</code></p>\n\n<p>iii. Define <code>M = [ R2'*T2 ]x</code>, <code>X1 = M * inv(K1) * x1</code> and <code>X2 = M * R2' * inv(K2) * x2</code></p>\n\n<p>iv. If <code>X1(3) * X2(3) &lt; 0</code>, set <code>R2 = U * W' * V'</code> and recompute <code>M</code> and <code>X1</code></p>\n\n<p>v. If <code>X1(3) &lt; 0</code> set <code>T2 = -T2</code></p>\n\n<p>vi. Define <code>P1_E = K1 * [ I | 0 ]</code> and <code>P2_E = K2 * [ R2 | T2 ]</code></p>\n\n<p>The notation <code>'</code> denotes the transpose and the notation <code>[.]x</code> used in step iii. corresponds to the skew-symetric operator. Applying the skew-symmetric operator on a 3x1 vector <code>e = [e_1; e_2; e_3]</code> results in the following (see the <a href=\"http://en.wikipedia.org/wiki/Cross_product#Conversion_to_matrix_multiplication\" rel=\"nofollow\">Wikipedia article on cross-product</a>):</p>\n\n<pre><code>[e]x = [0,-e_3,e_2; e_3,0,-e_1; -e_2,e_1,0]\n</code></pre>\n\n<p>Finally, note that the norm of <code>T2</code> will always be 1, since it is one of the column of an orthogonal matrix. This means that you won't be able to recover the true distance between the two cameras. For that purpose, you need to know the true distance between two points in the scene and take that into account to calculate the true distance between the cameras.</p>\n",
        "system": ""
    },
    {
        "instruction": "Dealing with filters and colour&#39;s",
        "input": "",
        "output": "<p>very hard to say from single non test-screen image. </p>\n\n<ol>\n<li><p><strong>the black and white filter</strong></p>\n\n<p>is easy just convert <strong>RGB</strong> to intensity <code>i</code> and then instead <strong>RGB</strong> write <code>iii</code> color. The simplest not precise conversion is</p>\n\n<pre><code>i=(R+G+B)/3\n</code></pre>\n\n<p>but better way is use of weights</p>\n\n<pre><code>i=w0*R+w1*G+w2*B\n</code></pre>\n\n<p>where <code>w0+w1+w2=1</code> the values can be found by a little google search effort</p></li>\n<li><p><strong>the rest</strong></p>\n\n<p>some filters seem like over exponated colors or weighted colors like this:</p>\n\n<pre><code>r=w0*r; if (r&gt;255) r=255;\ng=w1*g; if (g&gt;255) g=255;\nb=w2*b; if (b&gt;255) b=255;\n</code></pre>\n\n<p>write an app with 3 scrollbars for <code>w0,w1,w2</code> in range <code>&lt;0-10&gt;</code> and redraw image with above formula. After little experimenting you should find <code>w0,w1,w2</code> for most of the filters ... The rest can be mix of colors like this:</p>\n\n<pre><code>r=w00*r+w01*g+w02*b; if (r&gt;255) r=255;\ng=w10*r+w11*g+w12*b; if (g&gt;255) g=255;\nb=w20*r+w21*g+w22*b; if (b&gt;255) b=255;\n</code></pre>\n\n<p>or:</p>\n\n<pre><code>i=(r+g+b)/3\nr=w0*r+w3*i; if (r&gt;255) r=255;\ng=w1*g+w3*i; if (g&gt;255) g=255;\nb=w2*b+w3*i; if (b&gt;255) b=255;\n</code></pre></li>\n</ol>\n\n<p>btw if you want the closest similarity you can:</p>\n\n<ol>\n<li><p><strong>find test colors in input image</strong></p>\n\n<p>like <strong>R</strong> shades, <strong>G</strong> shades , <strong>B</strong> shades , <strong>RG,RB,BG,RGB</strong> shades from <code>0-255</code>. Then get colors from filtered image at the same position and draw depedency graphs for each shade draw <strong>R,G,B</strong> intensities.</p>\n\n<p>One axis is input image color intensity and the other one is <strong>R,G,B</strong> intensity of filtered color. Then you should see which formula is used directly and can also compute the weights from it. This is how over-exponation works for Red color</p>\n\n<p><img src=\"https://i.sstatic.net/esS4n.png\" alt=\"red overexponate filter\"></p></li>\n<li><p><strong>if the lines are not lines but curves</strong></p>\n\n<p>then some kind of gamma correction is used so formulas use polynomial of higher order (power of 2,3,4...) mostly power of 2 suffice. In that case the weights can be also negative !!!</p></li>\n<li><p><strong>some filters could use different color spaces</strong></p>\n\n<p>for example transform <strong>RGB</strong> to <strong>HSV</strong> shift hue and convert back to <strong>RGB</strong>. That will shift colors a little.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Remove noise from the computed optical flow",
        "input": "",
        "output": "<p>Remove noise from optical flow could be a complicated task. A simple and dummy way could be to use a threshold on the optical flow vector intensity.</p>\n\n<p>But if you only need to find bounding boxes why just do not use a simple background/motion object segmentation? Like MOG, GMG, opencv has nice implementations of them and they works well and are quite fast. See <a href=\"http://docs.opencv.org/trunk/doc/tutorials/video/background_subtraction/background_subtraction.html\" rel=\"nofollow\">this</a> tutorial.</p>\n",
        "system": ""
    },
    {
        "instruction": "Application-specific benefit of adaboost over gentleboost, or vice versa?",
        "input": "",
        "output": "<p>GentleBoost is a variant of AdaBoost classifier, from my memory. </p>\n\n<p>For instance, Adaboost are likely to be able to detect things that have hard objects like you have mentioned for GentleBoost, and I too have test Adaboost on stuff like cans and banana, which too works.</p>\n\n<p>While I have never work with GentleBoost before, or rather try it out, according to papers, chances are the computational speed taken to calculate the objects with little features or hard objects as you called it, like bananas, cans, etc are likely to be a lot faster. </p>\n\n<p>You can read more about this here: <a href=\"http://en.wikipedia.org/wiki/AdaBoost\" rel=\"noreferrer\">AdaBoost</a>, the Gentleboost while is only a short and small part in this wiki, it should be more or less able to clarify it up.</p>\n\n<p>Mathematically speaking the main key difference would be the loss function being used. </p>\n\n<p>For GentleBoost, update is fm(x) = P(y=1 | x) \u2013 P(y=0 | x).</p>\n\n<p>While for AdaBoost, update is:\n<img src=\"https://i.sstatic.net/lsWb7.png\" alt=\"enter image description here\"></p>\n\n<p>If I am not wrong, GentleBoost should be less sensitive to noisy data in addition to be faster(where faster is an assumption looking at the mathematical side) than AdaBoost, however in terms of accuracy, I have never played around with it, so I can't be sure. </p>\n\n<p>Hope that helped you somehow(: </p>\n",
        "system": ""
    },
    {
        "instruction": "Find Homography matrix from Fundamental matrix",
        "input": "",
        "output": "<p>First, notice that the equation (C29) you mentioned from your link uses a line l with same coordinates as e<sub>ij</sub>, which is the epipole in image j. Therefore, l=e<sub>ij</sub> is not an epiline, since dot( e<sub>ij</sub> , e<sub>ij</sub> ) == norm(e<sub>ij</sub>)\u00b2 == 1 != 0.</p>\n\n<p>If I stick to the notations given in your link, you can compute the epipole e<sub>ij</sub> as the left null-vector of F<sub>ij</sub>. You can obtain it by calling <code>cv::SVD::solveZ</code> on the transpose of <code>F_ij</code>. Then, as is mentioned in your link, the homography H<sub>ij</sub> (which maps points from image i to image j) can be computed as H<sub>ij</sub> = [e<sub>ij</sub>]<sub>x</sub> F<sub>ij</sub>, where the notation [e<sub>ij</sub>]<sub>x</sub> refers to the 3x3 skew-symmetric operator. A definition of this notation can be found in the <a href=\"http://en.wikipedia.org/wiki/Cross_product#Conversion_to_matrix_multiplication\" rel=\"nofollow\">Wikipedia article on cross-product</a>.</p>\n\n<p>However, be aware that such an homography H<sub>ij</sub> defines a mapping from image i to image j via the plane backprojected from image j using the line with same coordinates as e<sub>ij</sub>. In general, this will give a result which is very different from the result returned by <code>cv::findHomography</code>, where the resulting homography is a mapping from image i to image j via the dominant plane in the observed scene. Hence, you will be able to approximately register the two images using the homography returned by <code>cv::findHomography</code>, but in general this will not be the case for the homography obtained using the method above.</p>\n",
        "system": ""
    },
    {
        "instruction": "C++ code conversion of Mat",
        "input": "",
        "output": "<p>Why don't just do it in place (for level 2):</p>\n\n<pre><code>Mat img = imread(\"messi5.jpg\");\nfor(int i=0;i&lt;img.rows;i++)\n    for(int j=0;j&lt;img.cols;j++) {\n        cv::Vec3b p = img.at&lt;cv::Vec3b&gt;(i,j);\n        for(int k = 0;k &lt; img.channels();k++)\n             p[k] = p[k] &gt; 127 ? 255 : 0;\n        img.at&lt;cv::Vec3b&gt;(i,j) = p;\n}\n// do whatever you want with processed image img\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Accuracy in depth estimation - Stereo Vision",
        "input": "",
        "output": "<p><em>Q. Anyone knows a real stereo vision system that works with some accuracy? Can we achieve 1 mm depth estimation accuracy?</em></p>\n<p>Yes, you definitely can achieve 1mm (and much better) depth estimation accuracy with a stereo rig (heck, you can do stereo recon with a pair of microscopes). Stereo-based industrial inspection systems with accuracies in the 0.1 mm range are in routine use, and have been since the early 1990's at least. To be clear, by &quot;stereo-based&quot; I mean a 3D reconstruction system using 2 or more geometrically separated sensors, where the 3D location of a point is inferred by triangulating matched images of the 3D point in the sensors. Such a system may use structured light projectors to help with the image matching, however, unlike a proper &quot;structured light-based 3D reconstruction system&quot;, it does not rely on a calibrated geometry for the light projector itself.</p>\n<p>However, most (likely, all) such stereo systems designed for high accuracy use either some form of structured lighting, or some prior information about the geometry of the reconstructed shapes (or a combination of both), in order to <strong>tightly</strong> constrain the matching of points to be triangulated. The reason is that, generally speaking, one can triangulate more accurately than they can match, so matching accuracy is the limiting factor for reconstruction accuracy.</p>\n<p>One intuitive way to see why this is the case is to look at the simple form of the stereo reconstruction equation: <em>z = f b / d</em>. Here &quot;f&quot; (focal length) and &quot;b&quot; (baseline) summarize the properties of the rig, and they are estimated by calibration, whereas &quot;d&quot; (disparity) expresses the match of the two images of the same 3D point.</p>\n<p>Now, crucially, the calibration parameters are &quot;global&quot; ones, and they are estimated based on many measurements taken over the field of view and depth range of interest. Therefore, assuming the calibration procedure is unbiased and that the system is approximately time-invariant, the errors in each of the measurements are averaged out in the parameter estimates. So it is possible, by taking lots of measurements, and by tightly controlling the rig optics, geometry and environment (including vibrations, temperature and humidity changes, etc), to estimate the calibration parameters very accurately, that is, with unbiased estimated values affected by uncertainty of the order of the sensor's resolution, or better, so that the effect of their residual inaccuracies can be neglected within a known volume of space where the rig operates.</p>\n<p>However, disparities are point-wise estimates: one states that point <em>p</em> in left image matches (maybe) point <em>q</em> in right image, and any error in the disparity <em>d = (q - p)</em> appears in <em>z</em> scaled by <em>f b</em>. It's a one-shot thing. Worse, the estimation of disparity is, in all nontrivial cases, affected by the (a-priori unknown) geometry and surface properties of the object being analyzed, and by their interaction with the lighting. These conspire - through whatever matching algorithm one uses - to reduce the practical accuracy of reconstruction one can achieve. Structured lighting helps here because it reduces such matching uncertainty: the basic idea is to project sharp, well-focused edges on the object that can be found and matched (often, with subpixel accuracy) in the images. There is a plethora of structured light methods, so I won't go into any details here. But I note that this is an area where using color and carefully choosing the optics <strong>of the projector</strong> can help <em>a lot</em>.</p>\n<p>So, what you can achieve in practice depends, as usual, on how much money you are willing to spend (better optics, lower-noise sensor, rigid materials and design for the rig's mechanics, controlled lighting), and on how well you understand and can constrain your particular reconstruction problem.</p>\n",
        "system": ""
    },
    {
        "instruction": "Decomposition of essential matrix leads to wrong rotation and translation",
        "input": "",
        "output": "<p>I've found the misstake. This code is not doing the right matrix multiplication.</p>\n\n<pre><code>  Mat E = new Mat();\n  Core.multiply(cameraMatrix.t(),fundamental, E); \n  Core.multiply(E, cameraMatrix, E);\n</code></pre>\n\n<p>I changed this to</p>\n\n<pre><code>  Core.gemm(cameraMatrix.t(), fundamental, 1, cameraMatrix, 1, E);\n</code></pre>\n\n<p>which is now doing the right matrix multiplication. As far as I can get ir from the documentation, Core.multiply is doing the multiplication for each element. not the dot product of row*col.</p>\n",
        "system": ""
    },
    {
        "instruction": "Super resolution image from low quality Images",
        "input": "",
        "output": "<p>I doubt if you can find the code covering your specific task exactly. But you're task seems reasonable to me. You can follow the following steps</p>\n\n<ol>\n<li><p>Perform image registration (image 1 to all the rest) considering rotation, scale and gray-scale changes. In your'e case you can find special points like plate corners, middle of digits etc, then find the transformation between the two images by means of linear-regression.</p></li>\n<li><p>If the accuracy achieved in stage 1 is not enough, you can refine the registration using an affine-version of the Lucas-Kanade method</p></li>\n<li><p>Warp the images (all towards image 1)</p></li>\n<li><p>Use the super-resolution code you have, with the warped images and no shift as input      </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Function of Values",
        "input": "",
        "output": "<p>just go and do your own  experiments ?</p>\n\n<pre><code>int f[9] = {24,64,45,0,0,0,0,0,0};  // initial trackbar pos\nMat filt(3,3,CV_32F); \nMat img;\n\nvoid onTrack(int,void*)\n{\n    float *p = filt.ptr&lt;float&gt;(0);\n    for ( int i=0; i&lt;9; i++ )\n    {\n        p[i] = float(f[i]) / 100;   // get it back to [0..1] range for the transform matrix\n    }\n    Mat out;\n    cv::transform(img,out,filt);\n    imshow(\"result\",out);\n}\n\nint main( int argc, const char** argv )\n{\n    img = imread(\"lena.jpg\");\n    namedWindow(\"result\",0);\n    namedWindow(\"sliders\",0);\n    createTrackbar(\"1\",\"sliders\",&amp;f[0],100,onTrack);\n    createTrackbar(\"2\",\"sliders\",&amp;f[1],100,onTrack);\n    createTrackbar(\"3\",\"sliders\",&amp;f[2],100,onTrack);\n    createTrackbar(\"4\",\"sliders\",&amp;f[3],100,onTrack);\n    createTrackbar(\"5\",\"sliders\",&amp;f[4],100,onTrack);\n    createTrackbar(\"6\",\"sliders\",&amp;f[5],100,onTrack);\n    createTrackbar(\"7\",\"sliders\",&amp;f[6],100,onTrack);\n    createTrackbar(\"8\",\"sliders\",&amp;f[7],100,onTrack);\n    createTrackbar(\"9\",\"sliders\",&amp;f[8],100,onTrack);\n\n    onTrack(0,0);\n    waitKey();\n\n    cerr &lt;&lt; filt &lt;&lt; endl;\n    return 0;\n}\n</code></pre>\n\n<p>if you 're happy with the results, just divide each slider value by 100, and compose a filter matrix from that.</p>\n",
        "system": ""
    },
    {
        "instruction": "What does the resultant matrix of Homography denote?",
        "input": "",
        "output": "<p>Start with the following observation: a homography matrix is only defined up to scale. This means that if you divide or multiply all the matrix coefficients by the same number, you obtain a matrix that represent the same geometrical transformation. This is because, in order to apply the homography to a point at coordinates (x, y), you multiply its matrix H on the right by the column vector [x, y, 1]' (here I use the apostrophe symbol to denote transposition), and then divide the result H * x = [u, v, w]' by the third component w. Therefore, if instead of H you use a scaled matrix (s * H), you end up with [s*u, s*v, s*w], which represents the same 2D point. </p>\n\n<p>So, to understand what is going on with your matrices, start by dividing both of them by their  bottom-right component:</p>\n\n<pre><code>octave:1&gt; a = [ \n&gt; 0.2711   -0.0036    0.853\n&gt; -0.0002    0.2719   -0.2247\n&gt; 0.0000   -0.0000    0.2704\n&gt; ];\noctave:2&gt; b=[\n&gt; 0.4787   -0.0061    0.5514\n&gt; 0.0007    0.4798   -0.0799\n&gt; 0.0000   -0.0000    0.4797];\noctave:3&gt; a/a(3,3)\nans =\n1.00259  -0.01331   3.15459\n-0.00074   1.00555  -0.83099\n0.00000  -0.00000   1.00000\noctave:4&gt; b/b(3,3)\nans =\n0.99792  -0.01272   1.14947\n0.00146   1.00021  -0.16656\n0.00000  -0.00000   1.00000\n</code></pre>\n\n<p>Now suppose, for the moment, that the third column elements in both matrices were [0, 0, 1]'. Then the effect of applying it to any point (x, y) would be to move it by approx 1/100 units (say, pixels). Basically, not changing it by much. \nPlugging back the actual values for the third column shows that both matrices are, essentially, translating the whole images by constant amounts.</p>\n\n<p>So, in conclusion, having equal values on the diagonals, and very small values at indices (1,2) and (2,1), means that these homographies are both (essentially) pure translations.</p>\n",
        "system": ""
    },
    {
        "instruction": "I am not getting exactly gray image on converting video to grayscale?",
        "input": "",
        "output": "<p>A few questions: Have you printed out the images before and after to see what's happening at each step? </p>\n\n<p>Why are you converting the frames to type double? </p>\n\n<p>Suggestion: Try converting the image to uint8 by doing <code>uint8(grayVideo(:,:,i))</code> and use a gray colormap <code>colormap(gray(256))</code>.</p>\n\n<p>Let me know if this helps</p>\n",
        "system": ""
    },
    {
        "instruction": "Remove moving objects to get the background model from multiple images",
        "input": "",
        "output": "<p>If you only have 5 images it's going to be hard to identify background and most sophisticated techniques probably won't work. For general background identification methods, see <a href=\"https://ieeexplore.ieee.org/document/280225\" rel=\"nofollow noreferrer\">Link</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Smooth Operation In Image Processing",
        "input": "",
        "output": "<p><a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html#pyrdown\" rel=\"nofollow\">pyrDown()</a> might be doing both required steps in one go.</p>\n",
        "system": ""
    },
    {
        "instruction": "Computer Vision : Pattern Matching using Gaussian Pyramids",
        "input": "",
        "output": "<p>What are you asking is something that is done by SIFT algorithm.</p>\n\n<p>They used the following approach:\n      starting sigma 1.2   (EXPERIMENTAL)\n      step factor sqrt(2)  (EXPERIMENTAL)</p>\n\n<p>Why they use octaves... performance!\n      if they continued to grow sigma they would end in problem not beeing able to convolve an image in  real-time (well decent time), that is why they split the processing into octaves.</p>\n\n<p>When they populate each octave (when they double the scale) they half the image size and convolve the resized image again.</p>\n\n<p>Another paper measures the quality of parameters: sigma, scales per octave, number of octaves:\n<a href=\"http://www.bmva.org/bmvc/2010/workshop/paper5/paper5.pdf\" rel=\"nofollow\">http://www.bmva.org/bmvc/2010/workshop/paper5/paper5.pdf</a></p>\n\n<p>In short:\n    build the Gaussian pyramid of the input image and a template with factor scale of 2 and blur each level with some small sigma.</p>\n\n<pre><code>Matching:\n   match at the highest level (smallest images), then match the already matched object in lower level (in some area) just to improve precision.\n\n I do not think you need octaves since you are not going to convolve images with large sigmas (needed for blob detection like in SIFT)\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV scorebox detection &amp; extraction",
        "input": "",
        "output": "<p>I suggest you to make a filter/mask of portion of score box that is surely going to be there around the score-box and keep all such possible filter for different channel/video (assuming not a huge number of different types of score-boxes). It may be only first row of score-box, or a logo or some other signature e.g.</p>\n\n<p><img src=\"https://i.sstatic.net/d6CLp.png\" alt=\"Mask to be search to find score-box\"></p>\n\n<p>Now you can look for these masks in video frames and you can extract out the remaining part.</p>\n\n<p>As shown in your image, the important data is always in specific location, extract that and you have it.</p>\n\n<p>Algorithm will be fast as you don't need to search whole frame but a really small area and you can search only Y component with great detection rate.</p>\n",
        "system": ""
    },
    {
        "instruction": "Path detection in image",
        "input": "",
        "output": "<p>You have not give enough information about your problem but to start you can use Flood Fill algorithm to mark paths using connected pixel values. Check <a href=\"http://lodev.org/cgtutor/floodfill.html\" rel=\"nofollow\">this</a> link.  Opencv computer vision library has good functions to flood fill algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "Multi otsu(multi-thresholding) with openCV",
        "input": "",
        "output": "<p>To extend Otsu's thresholding method to multi-level thresholding the between class variance equation becomes:</p>\n\n<p><a href=\"https://i.sstatic.net/xnGKm.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xnGKm.jpg\" alt=\"multi between class variance\"></a></p>\n\n<blockquote>\n  <p>Please check out Deng-Yuan Huang, Ta-Wei Lin, Wu-Chih Hu, Automatic\n  Multilevel Thresholding Based on Two-Stage Otsu's Method with Cluster\n  Determination by Valley Estimation, Int. Journal of Innovative\n  Computing, 2011, 7:5631-5644 for more information. </p>\n  \n  <p><a href=\"http://www.ijicic.org/ijicic-10-05033.pdf\" rel=\"noreferrer\">http://www.ijicic.org/ijicic-10-05033.pdf</a></p>\n</blockquote>\n\n<p>Here is my C# implementation of Otsu Multi for 2 thresholds:</p>\n\n<pre><code>/* Otsu (1979) - multi */\n\nTuple &lt; int, int &gt; otsuMulti(object sender, EventArgs e) {\n    //image histogram\n    int[] histogram = new int[256];\n\n    //total number of pixels\n    int N = 0;\n\n    //accumulate image histogram and total number of pixels\n    foreach(int intensity in image.Data) {\n        if (intensity != 0) {\n            histogram[intensity] += 1;\n            N++;\n        }\n    }\n\n    double W0K, W1K, W2K, M0, M1, M2, currVarB, optimalThresh1, optimalThresh2, maxBetweenVar, M0K, M1K, M2K, MT;\n\n    optimalThresh1 = 0;\n    optimalThresh2 = 0;\n\n    W0K = 0;\n    W1K = 0;\n\n    M0K = 0;\n    M1K = 0;\n\n    MT = 0;\n    maxBetweenVar = 0;\n    for (int k = 0; k &lt;= 255; k++) {\n        MT += k * (histogram[k] / (double) N);\n    }\n\n\n    for (int t1 = 0; t1 &lt;= 255; t1++) {\n        W0K += histogram[t1] / (double) N; //Pi\n        M0K += t1 * (histogram[t1] / (double) N); //i * Pi\n        M0 = M0K / W0K; //(i * Pi)/Pi\n\n        W1K = 0;\n        M1K = 0;\n\n        for (int t2 = t1 + 1; t2 &lt;= 255; t2++) {\n            W1K += histogram[t2] / (double) N; //Pi\n            M1K += t2 * (histogram[t2] / (double) N); //i * Pi\n            M1 = M1K / W1K; //(i * Pi)/Pi\n\n            W2K = 1 - (W0K + W1K);\n            M2K = MT - (M0K + M1K);\n\n            if (W2K &lt;= 0) break;\n\n            M2 = M2K / W2K;\n\n            currVarB = W0K * (M0 - MT) * (M0 - MT) + W1K * (M1 - MT) * (M1 - MT) + W2K * (M2 - MT) * (M2 - MT);\n\n            if (maxBetweenVar &lt; currVarB) {\n                maxBetweenVar = currVarB;\n                optimalThresh1 = t1;\n                optimalThresh2 = t2;\n            }\n        }\n    }\n\n    return new Tuple(optimalThresh1, optimalThresh2);\n}\n</code></pre>\n\n<p>And this is the result I got by thresholding an image scan of soil with the above code: </p>\n\n<p>(T1 = 110, T2 = 147).</p>\n\n<p><a href=\"https://i.sstatic.net/bnSAi.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bnSAi.jpg\" alt=\"original scan\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/7WmRF.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7WmRF.jpg\" alt=\"thresholded scan\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/3DcMo.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3DcMo.jpg\" alt=\"image histogram\"></a></p>\n\n<blockquote>\n  <p>Otsu's original paper: \"Nobuyuki Otsu, A Threshold Selection Method\n  from Gray-Level Histogram, IEEE Transactions on Systems, Man, and\n  Cybernetics, 1979, 9:62-66\" also briefly mentions the extension to\n  Multithresholding.</p>\n  \n  <p><a href=\"https://engineering.purdue.edu/kak/computervision/ECE661.08/OTSU_paper.pdf\" rel=\"noreferrer\">https://engineering.purdue.edu/kak/computervision/ECE661.08/OTSU_paper.pdf</a></p>\n</blockquote>\n\n<p>Hope this helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "Why Forward Image Warping leave holes in the image?",
        "input": "",
        "output": "<p>Homography is a transformation matrix to map image pixel coordinate (ref <a href=\"http://en.wikipedia.org/wiki/Homography_(computer_vision)\" rel=\"nofollow noreferrer\">wiki</a>). Thus, assume the homogenous coordinate of the image's top-left corner is: <code>p0 = [0 0 1]^T</code>. Under some general homography <code>H</code>, it is probably that <code>p = H * p0</code> is not the top-left corner of the new warped image so that there will be holes in the top-left corner of the new image. A more intuitive example is shown below (espeically for similarity transformation, the image size will shrink):</p>\n\n<p><img src=\"https://i.sstatic.net/1yV6Q.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "operands could not be broadcast together with shapes (256) (257)",
        "input": "",
        "output": "<p>I'm not exactly sure of what you want to compute, but the lines </p>\n\n<pre><code>x=np.histogram(img1,bins=256)\nhistgray1 =x\n</code></pre>\n\n<p>and </p>\n\n<pre><code>y=np.histogram(img2,bins=256)\nhistgray2 =y\n</code></pre>\n\n<p>seem a bit off given that you are multiplying <code>histgray1[i]*histgray2[i]</code> for each <code>i</code>.</p>\n\n<p>The return value of a <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\" rel=\"nofollow\">numpy histogram</a> is <code>hist, bin_edges</code> so my guess is that you want to change those lines to</p>\n\n<pre><code>histgray1 = np.histogram(img1,bins=256)[0]\n</code></pre>\n\n<p>and </p>\n\n<pre><code>histgray2 = np.histogram(img2,bins=256)[0]\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Remove features from binarized image",
        "input": "",
        "output": "<p>Sure you can just get connected components (of certain size) with findContours or floodFill, and erase them leaving some smear. However, if you like to do it right you would think about why do you have the black area in the first place. </p>\n\n<p>You did not use adaptive thresholding (locally adaptive) and this made your output sensitive to shading. Try not to get the black region in the first place by running something like this:</p>\n\n<pre><code>Mat img = imread(\"desk.jpg\", 0);\nMat img2, dst;\npyrDown(img, img2);\nadaptiveThreshold(255-img2, dst, 255,  ADAPTIVE_THRESH_MEAN_C,\n        THRESH_BINARY, 9, 10); imwrite(\"adaptiveT.png\", dst);\nimshow(\"dst\", dst);\nwaitKey(-1);\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/Yeoap.png\" alt=\"enter image description here\"></p>\n\n<p>In the future, you may read something about adaptive thresholds and how to sample colors locally. I personally found it useful to sample binary colors orthogonally to the image gradient (that is on the both sides of it). This way the samples of white and black are of equal size which is a big deal since typically there are more background color which biases estimation. Using <a href=\"http://research.microsoft.com/pubs/149305/1509.pdf\" rel=\"nofollow noreferrer\">SWT</a> and <a href=\"http://docs.opencv.org/modules/features2d/doc/feature_detection_and_description.html?highlight=mser#MSER%20:%20public%20FeatureDetector\" rel=\"nofollow noreferrer\">MSER</a> may give you even more ideas about text segmentation. </p>\n",
        "system": ""
    },
    {
        "instruction": "How to process Gamma correction if having RGB data",
        "input": "",
        "output": "<p>This should work correctly. For most RGB color encodings, gamma is applied per-channel.</p>\n\n<p>Just be careful, R, G, and B are expected to be on a range [0,1] before applying the inverse gamma function.</p>\n",
        "system": ""
    },
    {
        "instruction": "calcOpticalFlowPyrLK horizontal and vertical flow u,v",
        "input": "",
        "output": "<p>As for dividing for the u and v components, I suggest doing a simple subtraction of point coordinates before and after translation. You can try to speed it up by for example putting all the points into 2 channel matrices and subtracting matrices from each other. </p>\n\n<p>As for a better way <a href=\"http://intel-research.net/Publications/Pittsburgh/092620050634_312.pdf\" rel=\"nofollow\">here</a> is an article I based my masters thesis on. There is a trick in it using amount of optical flow for obstacle detection.</p>\n",
        "system": ""
    },
    {
        "instruction": "Divide an image into 5x5 blocks in python and compute histogram for each block",
        "input": "",
        "output": "<p>Not sure if it is something like this you are looking for,\nThis is the brute-force version.and it's probably quite slow.but it does the job\nYou have to decide what to do with the boundaries though.\nThis will not include the boundary unless the window fits exactly</p>\n\n<pre><code>import numpy as numpy\n\ngrey_levels = 256\n# Generate a test image\ntest_image = numpy.random.randint(0,grey_levels, size=(11,11))\n\n# Define the window size\nwindowsize_r = 5\nwindowsize_c = 5\n\n# Crop out the window and calculate the histogram\nfor r in range(0,test_image.shape[0] - windowsize_r, windowsize_r):\n    for c in range(0,test_image.shape[1] - windowsize_c, windowsize_c):\n        window = test_image[r:r+windowsize_r,c:c+windowsize_c]\n        hist = numpy.histogram(window,bins=grey_levels)\n</code></pre>\n\n<p>Below is the result and the full image is at the end. \nr,c represents the topleft corner of the window</p>\n\n<pre><code>r=0,c=0\n[[ 63 173 131 205 239]\n [106  37 156  48  81]\n [ 85  85 119  60 228]\n [236  79 247   1 206]\n [ 97  50 117  96 206]]\n\nr=0,c=5\n[[108 241 155 214 183]\n [202   2 236 183 225]\n [214 141   1 185 115]\n [  4 234 249  95  67]\n [232 217 116 211  24]]\n\nr=5,c=0\n[[179 155  41  47 190]\n [159  69 211  41  92]\n [ 64 184 187 104 245]\n [190 199  71 228 166]\n [117  56  92   5 186]]\n\nr=5,c=5\n[[ 68   6  69  63 242]\n [213 133 139  59  44]\n [236  69 148 196 215]\n [ 41 228 198 115 107]\n [109 236 191  48  53]]\n\n[[ 63 173 131 205 239 108 241 155 214 183  42]\n [106  37 156  48  81 202   2 236 183 225   4]\n [ 85  85 119  60 228 214 141   1 185 115  80]\n [236  79 247   1 206   4 234 249  95  67 203]\n [ 97  50 117  96 206 232 217 116 211  24 242]\n [179 155  41  47 190  68   6  69  63 242 162]\n [159  69 211  41  92 213 133 139  59  44 196]\n [ 64 184 187 104 245 236  69 148 196 215  91]\n [190 199  71 228 166  41 228 198 115 107  82]\n [117  56  92   5 186 109 236 191  48  53  65]\n [177 170 114 163 101  54  80  25 112  35  85]]\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "C++ BLOB average",
        "input": "",
        "output": "<p>Try this:</p>\n\n<pre><code>int blobSize = contourFinder.blobs.size();\nint sumX = 0;\nint sumY = 0;\n\nstd::vector&lt;ofxCvBlob&gt;::iterator blob;\nfor (auto blob = contourFinder.blobs.begin(); blob!= contourFinder.blobs.end(); blob++) {\n    float xpos = (*blob).center.x;\n    float ypos = (*blob).center.y;\n\n    sumX += xpos;\n    sumY += ypos;\n\n    // Manage each point here ...\n}\n\nfloat averageX = sumX / blobSize;\nfloat averageY = sumY / blobSize;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Where is the Javascript library?",
        "input": "",
        "output": "<p>Once again .. Go to <a href=\"https://github.com/ingenuitas/simplecv-js\" rel=\"nofollow noreferrer\">https://github.com/ingenuitas/simplecv-js</a></p>\n\n<p>Click either of these buttons\n<img src=\"https://i.sstatic.net/A2JHu.png\" alt=\"Img\"></p>\n",
        "system": ""
    },
    {
        "instruction": "error in covariance via openCV calcCovarMatrix(data, covar, means, CV_COVAR_COLS);",
        "input": "",
        "output": "<p>The documentation states that you MUST add either CV_COVAR_NORMAL or CV_COVAR_SCRAMBLED.\nFor normal behaviour use CV_COVAR_NORMAL.</p>\n\n<p>So your call should be</p>\n\n<pre><code>calcCovarMatrix(data, covar, means, CV_COVAR_NORMAL | CV_COVAR_COLS); \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "finding connected component of a given pixel in binary image",
        "input": "",
        "output": "<p>I think you'd be better off representing the connected components as a labeled image</p>\n\n<pre><code>lb = bwlabel( BW );\n</code></pre>\n\n<p>Now, each pixel in <code>BW</code> has a label (1..N) while background pixels remains 0.<br>\nYou can test for pixel <code>x</code>,<code>y</code> what is its label:</p>\n\n<pre><code> lb( y, x )\n</code></pre>\n\n<p>You can compare the labels of two pixels</p>\n\n<pre><code> lb( y1, x1 ) == lb( y2, x2 )\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Face recognition using SVM",
        "input": "",
        "output": "<p>well your code looks dirty so I won't be going over them but I will try to give your some hints on the process.</p>\n\n<p>First you need training set and make sure you have more negative training data than positive for good results. Also note that your will need quiet a large number of faces for good result and perhaps around 100-200 for average or below average simple classifier.</p>\n\n<p>Second extract features from the face, where you can use like, Color, Edge Histograms or Binary like Patters, etc. Choice is yours. But convolution of few will give more better results. </p>\n\n<p>Train SVM using the prepared data and do predicition. </p>\n\n<p>Here is a link from Roy a MIT student where he used SVM to train a food classifier. Code is simple to understand and you can follow them to get a gist of SVM classifier.</p>\n\n<p><a href=\"https://github.com/royshil/FoodcamClassifier\" rel=\"nofollow\">Food Classifier</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Creating vignette filter in opencv?",
        "input": "",
        "output": "<p><strong>First of all, <a href=\"https://stackoverflow.com/a/22843077/176769\">Abid Rahman K</a> describes the easiest way to go about this filter. You should seriously study his answer with time and attention.</strong> Wikipedia's take on <a href=\"https://en.wikipedia.org/wiki/Vignetting\" rel=\"nofollow noreferrer\">Vignetting</a> is also quite clarifying for those that had never heard about this filter.</p>\n\n<p><a href=\"https://github.com/browny\" rel=\"nofollow noreferrer\">Browny's</a> implementation of <a href=\"https://github.com/browny/filters-fun/blob/master/filter/src/vignet.cpp\" rel=\"nofollow noreferrer\">this filter</a> is considerably more complex. However, I ported his code to the C++ API and simplified it so you can follow the instructions yourself.  </p>\n\n<pre><code>#include &lt;math.h&gt;\n\n#include &lt;vector&gt;\n\n#include &lt;cv.hpp&gt;\n#include &lt;highgui/highgui.hpp&gt;\n\n\n// Helper function to calculate the distance between 2 points.\ndouble dist(CvPoint a, CvPoint b)\n{\n    return sqrt(pow((double) (a.x - b.x), 2) + pow((double) (a.y - b.y), 2));\n}\n\n// Helper function that computes the longest distance from the edge to the center point.\ndouble getMaxDisFromCorners(const cv::Size&amp; imgSize, const cv::Point&amp; center)\n{\n    // given a rect and a line\n    // get which corner of rect is farthest from the line\n\n    std::vector&lt;cv::Point&gt; corners(4);\n    corners[0] = cv::Point(0, 0);\n    corners[1] = cv::Point(imgSize.width, 0);\n    corners[2] = cv::Point(0, imgSize.height);\n    corners[3] = cv::Point(imgSize.width, imgSize.height);\n\n    double maxDis = 0;\n    for (int i = 0; i &lt; 4; ++i)\n    {\n        double dis = dist(corners[i], center);\n        if (maxDis &lt; dis)\n            maxDis = dis;\n    }\n\n    return maxDis;\n}\n\n// Helper function that creates a gradient image.   \n// firstPt, radius and power, are variables that control the artistic effect of the filter.\nvoid generateGradient(cv::Mat&amp; mask)\n{\n    cv::Point firstPt = cv::Point(mask.size().width/2, mask.size().height/2);\n    double radius = 1.0;\n    double power = 0.8;\n\n    double maxImageRad = radius * getMaxDisFromCorners(mask.size(), firstPt);\n\n    mask.setTo(cv::Scalar(1));\n    for (int i = 0; i &lt; mask.rows; i++)\n    {\n        for (int j = 0; j &lt; mask.cols; j++)\n        {\n            double temp = dist(firstPt, cv::Point(j, i)) / maxImageRad;\n            temp = temp * power;\n            double temp_s = pow(cos(temp), 4);\n            mask.at&lt;double&gt;(i, j) = temp_s;\n        }\n    }\n}\n\n// This is where the fun starts!\nint main()\n{\n    cv::Mat img = cv::imread(\"stack-exchange-chefs.jpg\");\n    if (img.empty())\n    {\n        std::cout &lt;&lt; \"!!! Failed imread\\n\";\n        return -1;\n    }\n\n    /*\n    cv::namedWindow(\"Original\", cv::WINDOW_NORMAL);\n    cv::resizeWindow(\"Original\", img.size().width/2, img.size().height/2);\n    cv::imshow(\"Original\", img);\n    */\n</code></pre>\n\n<p>What <strong>img</strong> looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/xUj5E.png\" width=\"480\" height=\"360\"></p>\n\n<pre><code>    cv::Mat maskImg(img.size(), CV_64F);\n    generateGradient(maskImg);\n\n    /*\n    cv::Mat gradient;\n    cv::normalize(maskImg, gradient, 0, 255, CV_MINMAX);\n    cv::imwrite(\"gradient.png\", gradient);\n    */\n</code></pre>\n\n<p>What <strong>maskImg</strong> looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/09Ysb.png\" width=\"480\" height=\"360\"></p>\n\n<pre><code>    cv::Mat labImg(img.size(), CV_8UC3);\n    cv::cvtColor(img, labImg, CV_BGR2Lab);\n\n    for (int row = 0; row &lt; labImg.size().height; row++)\n    {\n        for (int col = 0; col &lt; labImg.size().width; col++)\n        {\n            cv::Vec3b value = labImg.at&lt;cv::Vec3b&gt;(row, col);\n            value.val[0] *= maskImg.at&lt;double&gt;(row, col);\n            labImg.at&lt;cv::Vec3b&gt;(row, col) =  value;\n        }\n    }\n\n    cv::Mat output;\n    cv::cvtColor(labImg, output, CV_Lab2BGR);\n    //cv::imwrite(\"vignette.png\", output);\n\n    cv::namedWindow(\"Vignette\", cv::WINDOW_NORMAL);\n    cv::resizeWindow(\"Vignette\", output.size().width/2, output.size().height/2);\n    cv::imshow(\"Vignette\", output);\n    cv::waitKey();\n\n    return 0;\n}\n</code></pre>\n\n<p>What <strong>output</strong> looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/V6Ilb.png\" width=\"480\" height=\"360\"></p>\n\n<p>As stated in the code above, by changing the values of <code>firstPt</code>, <code>radius</code> and <code>power</code> you can achieve stronger/weaker artistic effects.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "prior based image segmentation",
        "input": "",
        "output": "<p>Here is an outline for image segmentation</p>\n\n<p>Sample a small region (possible a rectangle) inside the river, the assumption is that they will belong to the foreground and provide a good estimate about its color distribution. You should have an algorithm which can find a small region inside the river with high confidence, probably this algorithm can be trained on the data you have.</p>\n\n<p>Since you know little about the background, it would to be ideal to chose pixels lying on the image frame as background pixels.</p>\n\n<p>The idea is to use these pre-selected foreground and background pixels as seeds in a graph cut algorithm for segmentation. Selecting seeds is the most important part of a graph cut algorithm for segmentation, once you have good seeds, the segmentation would be more or less correct. There is plenty of literature/code available online on how to do segmentation using graph cuts.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to find Center of Mass for my entire binary image?",
        "input": "",
        "output": "<p>You can define all regions as a single region for <code>regionprops</code></p>\n\n<pre><code>props = regionprops( double( BW ), 'Centroid' ); \n</code></pre>\n\n<p>According to the data type of <code>BW</code> <code>regionprops</code> decides whether it should label each connected component as a different region or treat all non-zeros as a single region with several components.</p>\n\n<hr>\n\n<p>Alternatively, you can compute the centroid by yourself</p>\n\n<pre><code>[y x] = find( BW );\ncent = [mean(x) mean(y)];\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV createsamples - invalid background description file",
        "input": "",
        "output": "<p>If you created the file on Windows, and running it on Linux e.g. Ubuntu, you have to change the &quot;End of line sequence&quot;.</p>\n<p>Click on the bottom of the editor on CR LF and change it to <strong>LF</strong> and try again!</p>\n<p>Notepad++:</p>\n<p><img src=\"https://i.sstatic.net/N4Q8c.png\" alt=\"Notepad++ image:\" /></p>\n<p>Visual Studio Code:</p>\n<p><img src=\"https://i.sstatic.net/VPVlF.png\" alt=\"Visual Studio Code image:\" /></p>\n<p>Geany: Documents &gt; Set Line Endings &gt; Convert and Set to LF (Unix)</p>\n<p><img src=\"https://i.sstatic.net/3T8an.png\" alt=\"Geany image\" /></p>\n",
        "system": ""
    },
    {
        "instruction": "what library is able to extract SIFT features in Python?",
        "input": "",
        "output": "<p>I would like to suggest VLFeat, another open source vision library. It also has a python wrapper. The implementation of SIFT in VLFeat is modified from the original algorithm, but I think the performance is good.</p>\n",
        "system": ""
    },
    {
        "instruction": "How Matlab Computer vision Toolbox Functions were made/written?",
        "input": "",
        "output": "<p>The functionality in Computer Vision System Toolbox is available as MATLAB functions, System Objects, and Simulink blocks.</p>\n\n<p>It's implemented in a mixture of MATLAB code and C code, and as far as I know does not rely on OpenCV at all.</p>\n\n<p>However, it also supports Code Generation using MATLAB Coder and Simulink Coder - in other words, once you have prototyped and developed a system using MATLAB, Computer Vision System Toolbox and maybe Simulink, you can automatically generate much faster C code from it (which, if you want, could be integrated with functionality from OpenCV).</p>\n",
        "system": ""
    },
    {
        "instruction": "Tracking white color using python opencv",
        "input": "",
        "output": "<p>Let's take a look at HSV color space:</p>\n\n<p><img src=\"https://i.sstatic.net/mkq1P.png\" alt=\"enter image description here\"></p>\n\n<p>You need white, which is close to the center and rather high. Start with</p>\n\n<pre><code>sensitivity = 15\nlower_white = np.array([0,0,255-sensitivity])\nupper_white = np.array([255,sensitivity,255])\n</code></pre>\n\n<p>and then adjust the threshold to your needs.</p>\n\n<p>You might also <strong>consider using HSL</strong> color space, which stands for Hue, Saturation, <strong>Lightness</strong>. Then you would only have to look at lightness for detecting white and recognizing other colors would stay easy. Both HSV and HSL keep similar colors close. Also HSL would probably prove more accurate for detecting white - here is why:</p>\n\n<p><img src=\"https://i.sstatic.net/Sd3MJ.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Partial image detection with perceptual hashing",
        "input": "",
        "output": "<p>One approach for doing this can be\n1. Extract geometric markers (special points), for both input and output, for example by using the Harris corner detector.\n2. Compare every possible pair of triangles on each image . If they have the same angles, they define a transformation (resize, rotate,shift, and mirror). Google \"geometric hashing\"\n3 Count how many points in the target agree with the transformed source, and you have a score for the match. \n4. Verify the best match by other means (histogram, edge-registration). </p>\n\n<p>This approach is slow in theory (o(n^6) in the number of points) but I believe it can be practical using an appropriate choice of n and some tricks to speed it up. </p>\n",
        "system": ""
    },
    {
        "instruction": "Motion blur robust edge detection",
        "input": "",
        "output": "<p>I would use <a href=\"http://docs.opencv.org/doc/tutorials/features2d/trackingmotion/harris_detector/harris_detector.html\" rel=\"nofollow\">Harris corner detection</a> to detect the corner points and then use <a href=\"http://en.wikipedia.org/wiki/Hough_transform\" rel=\"nofollow\">Hough transform</a> to <a href=\"http://answers.opencv.org/question/6902/opencv-243-rectangle-detection/\" rel=\"nofollow\">detect the lines</a>. Using the position of the corners and lines it is possible to get the polygons.</p>\n",
        "system": ""
    },
    {
        "instruction": "Train Cascade Object Detector bounding box out of bound?",
        "input": "",
        "output": "<p>I did not run the code, because I don't own the toolbox, but the following lines are very \"suspicious\":</p>\n\n<pre><code>positive_samples(j).objectBoundingBoxes(1,1)=min(tempmat(:,1));\npositive_samples(j).objectBoundingBoxes(1,2)=min(tempmat(:,2));\npositive_samples(j).objectBoundingBoxes(1,3)=max(tempmat(:,2))-min(tempmat(:,2));\npositive_samples(j).objectBoundingBoxes(1,4)=max(tempmat(:,1))-min(tempmat(:,1));\n</code></pre>\n\n<p>I would expect:</p>\n\n<pre><code>positive_samples(j).objectBoundingBoxes(1,1)=min(tempmat(:,2));\npositive_samples(j).objectBoundingBoxes(1,2)=min(tempmat(:,1));\npositive_samples(j).objectBoundingBoxes(1,3)=max(tempmat(:,2))-min(tempmat(:,2));\npositive_samples(j).objectBoundingBoxes(1,4)=max(tempmat(:,1))-min(tempmat(:,1));\n</code></pre>\n\n<p>Some suggestions to shorten your code, they are not related to the problem:</p>\n\n<p>You can shorten line 4 to 9 to a single line, avoiding the loop: <code>[positive_samples(1:L).im]=list(4:end).name</code></p>\n\n<p>And this loop can be replaced as well:</p>\n\n<pre><code>tempmat=[];\ncount=1;\nfor l=1:le\n    for m=1:wi\n        if h1(l,m)==1\n            tempmat(count,1)=l;\n            tempmat(count,2)=m;\n            count=count+1;\n        end\n    end\nend\n</code></pre>\n\n<p>shorter and faster code:</p>\n\n<pre><code>[y,x]=find(h1);\ntempmat=[y x];\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Coordinate mapping at an angle",
        "input": "",
        "output": "<p>I'll try to give an overview over the algorithm used to solve this kind of problem. First, we need to know the physical characteristics of the camera, that is focal length and real size of the images it takes, together with the size in pixels. If I'm saying the \"real\" size of the image that really means the size of the <em>image</em> (or maybe, easier imaginable, the size of the negative of a classical film camera). Example values for a typical medium format camera for aerial mapping would be 50mm focal length, 9000*6800 pixels, with 6microns pixel size, giving ~40x54mm image size. </p>\n\n<p>The algorithm to compute the position of one pixel on the ground is (adapted to use an LSR system, one might do it with geographic coordinates as well): </p>\n\n<pre><code>public void ImageToGround(Camera sensor, double posx, double posy, double posz, \n    double dDeltaX, double dDeltaY, \n    Matrix4D rotationMatrixItg, \n    double groundheight, out double resultx, out double resultx)\n    {\n        // The rotation matrix is right-handed, with x pointing in flight direction, y to the right and z down.\n        // The image cs is increasing x to the right and y to the bottom (y = 0 is front in flight direction)\n        Vector3D imagePointRelativeToFocalPoint = new Vector3D(\n             dDeltaX,\n             dDeltaY,\n             -sensor.MetricFocalLength);\n\n         // Transform from img to camera coord system and rotate.\n         // The rotation Matrix contains the transformation from image coordinate system to camera\n         // coordinate system. \n         Vector3D imagePointRotated = rotationMatrixItg * imagePointRelativeToFocalPoint;\n\n         double dir, dist;\n\n         // Create a horizontal plane at groundheight, pointing upwards. (Z still points down)\n         Plane plane = new Plane(new Vector3D(0, 0, -1), new Vector3D(0, 0, -groundheight));\n\n         // And a ray, starting at the given image point (including the real height of the image position).\n         // Direction is opposite to the vector given above (from image to focal point). \n         Ray start = new Ray(new Vector3D(imagePointRotated.X, imagePointRotated.Y, imagePointRotated.Z - evHASL), \n    -(new Vector3D(imagePointRotated.X, imagePointRotated.Y, imagePointRotated.Z)));\n\n         // Find the point where the ray intersects the plane (this is on the opposite side of the\n         // x and y axes, because the ray goes trough the origin). \n         IntersectionPair p = start.Intersects(plane);\n         if (p.NumIntersections &lt; 1)\n         {\n             resultx = 0;\n             resulty = 0;\n             return;\n         }\n\n         resultx = p.Intersection1.x;\n         resulty = p.Intersection1.y;\n\n   }\n</code></pre>\n\n<p>with posx, posy, posz: Position of image center; dDeltaX, dDeltaY: Position (in meters) of the pixel on the focal plane; rotationMatrixItg: Image to ground rotation matrix, created from yaw, pitch, roll of the image; groundheight: Elevation of the ground; resultx, resulty: Result position on the ground. I have simplified the algorithm, so you might need to adjust it to meet your needs. </p>\n\n<p>The problem gets more complex when the terrain is not flat. If the whole image needs to be projected to the ground, one usually goes the inverse way, because that is more easy for interpolation and can be done in parallel. </p>\n\n<p>I don't exactly know what you mean by \"virtual\" images, because also those are created by a projection, so there exist some theoretical image parameters that can be used.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to decide proper disparity value for stereo vision",
        "input": "",
        "output": "<p>Look at the formula that relates a baseline to disparity</p>\n\n<pre><code>D=focal*Baseline/z\n</code></pre>\n\n<p>where focal length is in pixels, Baseline is in mm and z, a distance along the optical axis, is also in mm. Pick your Baseline so that you have a few pixels of disparity at the longest working distance. Also keep in mind that though a long Baseline will accomplish this, at a closer distance you would have a lager dead zone where the cameras' field of views do not overlap enough to have a meaningful disparity calculation.</p>\n\n<p>Also, when selecting the resolution for your images don't go too high since a stereo processing is very intensive and a higher resolution may have stronger noise. Typically people don't use color in stereo matching for the same reason. For your task, the algorithm that uses gray VGA images and works at least at 20 fps with a Baseline = 40-60 cm may be a reasonable choice given vehicle speed &lt;40mph.</p>\n",
        "system": ""
    },
    {
        "instruction": "Simulate cataract vision in OpenCV",
        "input": "",
        "output": "<p>It would be useful if you described the symptoms of the cataract and what happens to the retinal images since not all the people here are experts in computer vision and eye deceases at the same time. If a retinal image gets out of focus and gets a yellow tint you can used openCV <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=blur#cv2.blur\" rel=\"nofollow noreferrer\">blur()</a> function and also boost RGB values with yellow a bit. If there are different degree of blur across a visual field I recommend using integral images, see <a href=\"https://stackoverflow.com/a/22447727/457687\">this post</a></p>\n\n<p><img src=\"https://i.sstatic.net/Mqm4A.jpg\" alt=\"enter image description here\">\nI guess there are at least three operations to do: add noise, blur, whiten:</p>\n\n<pre><code>Rect rect2(0, 0, w/2, h);\nMat src = I4(rect2).clone();\nMat Mnoise(h, w/2, CV_8UC3);\nrandn(Mnoise, 100, 50);\nsrc = src*0.5+Mnoise*0.5; // add noise\nMat Mblur;\nblur(src, Mblur, Size(12, 12)); // blur\nRect rect3(w, 0, w/2, h);\nMat Mblurw = Mblur*0.8+ Scalar(255, 255, 255)*0.2; //whiten\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "TUIO cursor + openframeworks",
        "input": "",
        "output": "<p>The first thing to mention is </p>\n\n<blockquote>\n  <p><code>m.setAddress(\"/tuio2/2Dcur\");</code></p>\n</blockquote>\n\n<p>which should be </p>\n\n<pre><code>m.setAddress(\"/tuio/2Dcur\");\n</code></pre>\n\n<p>The TUIO Standard (1.1 and 1.0) defines 2Dcur as follows:</p>\n\n<pre><code>/tuio/2Dcur set s x y X Y m\n</code></pre>\n\n<p>In your code you are setting s,x and y and then you add four times 0.0 (addFloatArg(0)) so you get in fact a message like this:</p>\n\n<pre><code>/tuio/2Dcur set s x y 0.0 0.0 0.0 0.0\n</code></pre>\n\n<p>which is one float too much. In OSC you usually subscribe to a message with full signature. That's why you get no message in your Unity application.</p>\n",
        "system": ""
    },
    {
        "instruction": "Understanding Distance Transform in OpenCV",
        "input": "",
        "output": "<p>Look at the picture below (you may want to increase you monitor brightness to see it better). The pictures shows the distance from the red contour depicted with pixel intensities, so in the middle of the image where the distance is maximum the intensities are highest. This is a manifestation of the distance transform. Here is an immediate application - a green shape is a so-called active contour or snake that moves according to the gradient of distances from the contour (and also follows some other constraints) curls around the red outline. Thus one application of distance transform is shape processing. </p>\n\n<p>Another application is text recognition - one of the powerful cues for text is a stable width of a stroke. The distance transform run on segmented text can confirm this. A corresponding method is called <a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=149305\" rel=\"noreferrer\">stroke width transform (SWT)</a></p>\n\n<p>As for aligning two rotated shapes, I am not sure how you can use DT. You can find a center of a shape to rotate the shape but you can also rotate it about any point as well. The difference will be just in translation which is irrelevant if you run <a href=\"http://docs.opencv.org/modules/imgproc/doc/object_detection.html?highlight=matchtemplate#cv.MatchTemplate\" rel=\"noreferrer\">matchTemplate</a> to match them in correct orientation.</p>\n\n<p>Perhaps if you upload your images it will be more clear what to do. In general you can match them as a whole or by features (which is more robust to various deformations or perspective distortions) or even using outlines/silhouettes if they there are only a few features. Finally you can figure out the orientation of your object (if it has a dominant orientation) by running PCA or <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=fit%20ellipse#cv.FitEllipse2\" rel=\"noreferrer\">fitting an ellipse</a> (as rotated rectangle). </p>\n\n<pre><code>cv::RotatedRect rect = cv::fitEllipse(points2D);\nfloat angle_to_rotate = rect.angle;\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/gMM8w.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "PARABOLIC (not panoramic) video stitching?",
        "input": "",
        "output": "<p>I think the demo you linked uses a 360\u00b0 camera (see the black circle on the bottom) and does not involve stitching in any way.</p>\n\n<p>About your question, are you aware of <a href=\"http://phototour.cs.washington.edu/\" rel=\"nofollow\">this</a> work? They don't do stitching either, just blending between different views.</p>\n\n<p>If you use inward views, then the objects you will observe will probably be quite close to the cameras, while standard stitching assumes that objects are far away. Close 3D objects mean high distortion when you change the viewpoint (i.e. parallax &amp; occlusions), which makes it difficult to interpolate between two views. Hence, if you want stitching, then your main problem is to correctly handle parallax effects &amp; occlusions between the views.</p>\n\n<p>In my opinion, the most promising approach would be to do live stereo matching (i.e. dense 3D reconstruction) between the two camera images closest to your current viewpoint, and then interpolate the estimated disparities to generate an expected image. However, it's not likely to run in real-time, as demonstrated in the demo you linked, and the result could be quite ugly...</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>You can also have a look at <a href=\"http://www.cs.ucf.edu/courses/cap6412/spr2005/ECCV2002-NVS.pdf\" rel=\"nofollow\">this paper</a>, which uses a different but interesting approach, however maybe not directly useful in your case since it requires the new viewpoint to be visible in the available images.</p>\n",
        "system": ""
    },
    {
        "instruction": "Vectorization of binary grayscale image",
        "input": "",
        "output": "<p>If you have nice proper uninterrupted shapes you can just trace their contours using something like <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#cv.FindContours\" rel=\"nofollow\">findContours().</a>. But if your input (that you did not describe properly) is noisy and sketchy, the approach should rely on a Hough transform, see below. By the same coin, in fitting curves a lot will depend on the level of noise and the presence of outliers (e.g. background elements that aren't shapes or are inaccurate shapes that only approximate, say a proper ellipse). It is hard to imagine proper clean lines and proper shapes in a typical task unless it is a homework.</p>\n\n<p><a href=\"http://docs.opencv.org/modules/imgproc/doc/feature_detection.html?highlight=hough#cv.HoughLines2\" rel=\"nofollow\">Hough lines</a> and <a href=\"http://docs.opencv.org/modules/imgproc/doc/feature_detection.html?highlight=houghcircles#cv2.HoughCircles\" rel=\"nofollow\">Hough circles</a> are the most widely used functions in openCV library. Note that fitting ellipses is non-trivial since they have 5 parameters (lines have 2 and circles have 3) and Hough space grows too much. Rectangles can be found either with Hough lines or a special rectangle Hough. Other shapes can be detected using generalized non-parametric Hough. </p>\n\n<p>Fitting curves should use RANSAC to get rid of outliers, and geometric (least square in terms of point distances) fit to minimize pixel noise. The latter procedure typically involves non-linear optimization that should be initialized by a simpler algebraic fit. Luckily, for simple geometric primitives fitting functions have already been written, see <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=fit%20line#cv.FitLine\" rel=\"nofollow\">fitLine().</a></p>\n\n<p>The bottom line, given that your shapes are a bit noisy, your task is non-trivial (to the degree you probably don't realize) and thus should be split on several sub-projects like finding shapes, fitting curves, etc. </p>\n",
        "system": ""
    },
    {
        "instruction": "opencv createsamples has no error, but no samples found",
        "input": "",
        "output": "<p>I had a similar problem I'll try to explain what I did, I hope it will solve you problem.</p>\n\n<p>First of all the NULL after -info is normal. As said in the official documentation of OpenCV for opencv_createcamples you need to either input an image (-img) or a collection of images (-info) I'll let you read the documentation for further understanding, but for this example you don't need it.</p>\n\n<p>Secondly you don't need to put absolute paths in this command; here is my command:</p>\n\n<pre><code>perl bin/createsamples.pl  positives.txt negatives.txt samples 1500 \"opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1 -maxyangle 1.1 -maxzangle 0.5 -maxidev 40 -w 42 -h 40\"\n</code></pre>\n\n<p>to do this be sure to be at the root of the gitHub directory you have download (the parent one of bin) be sure that your positives.txt and negatives. txt are in this directory. Also copy paste the opencv_createsamples.exe from OpenCV directory in this one.</p>\n\n<p>This done I'll now explain the main problems: The project was developped for Ubuntu at the beginning so it works for mac easily but I think you must be under Windows. If you hadn't already please download Cygwin because it uses mainly Linux commands.</p>\n\n<p>As I said I was blocked with a similar problem, so I tried to use opencv_createsample directly instead of the perl script to see if the problem was coming from there and <strong>I noticed that the problem was coming from the fact that my positives.txt and negatives.txt were under Windows format and not Unix so the Perl script wasn't able to read them properly</strong>.</p>\n\n<p>The difference between windows and linux are big and Cygwin doesn't bridge the gap so there might be other encoding problems. So what I did is surely not the fastest way to resolve the problem, but it is an easy one.</p>\n\n<p>I just installed an Ubuntu vm on my PC</p>\n\n<p>Installed Opencv with TBB ( a lot of tutorials internet, the best is the one from the <a href=\"http://docs.opencv.org/doc/tutorials/introduction/linux_install/linux_install.html\" rel=\"noreferrer\">OpenCV site</a>).</p>\n\n<p>I Downloaded the gitHub <a href=\"https://github.com/mrnugget/opencv-haar-classifier-training\" rel=\"noreferrer\">Classifier training</a> and then I followed the commands given and it worked well.</p>\n",
        "system": ""
    },
    {
        "instruction": "What is the method to detect whether a given picture is human face or not?",
        "input": "",
        "output": "<p>Eigen decomposition reduces dimensionality in continues domain by finding directions in data space with high variance. K-means finds clusters in space with high density of points. You kind of mixing them together while completely ignoring how would you arrive at the face features on the first place (how would you scale, rotate and crop whatever you want to inspect either).</p>\n\n<p>You don\u2019t need to train Haar detectors since they are already trained for faces. They detect a face, not recognize its identity. ALl you need is to port the code together with a little file with parameters obtained after training (that was already performed) as Shiva suggested above. </p>\n\n<p>Thoughtless copy-pasting of the code doesn\u2019t make much sense though. Read a bit about <a href=\"http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html?highlight=haar#cv.HaarDetectObjects\" rel=\"nofollow\">Haar</a>. Try to understand</p>\n\n<ol>\n<li>Why they work - faces have features most pronounced on the intermediate spatial scale such as eyes, nose, brows. Too small (size of the pupil) or too large (size of the whole face) features are less useful.   </li>\n<li>why Haars are preferred to wavelets or Gabors - Haars are just raw (boxy) approximations of Gabors but since they can be quickly calculated with <a href=\"http://en.wikipedia.org/wiki/Summed_area_table\" rel=\"nofollow\">Integral images</a> they are preferred to more precise but slower counterparts;  </li>\n<li>what are the restrictions - Haars have their own spatial scale and orientation but can be quickly recalculated for another scale.   </li>\n<li>How to train Haar classifier (the most exciting topic you are trying to avoid). <a href=\"http://en.wikipedia.org/wiki/AdaBoost\" rel=\"nofollow\">Ada boost</a> is the one way to train a more complex classifier consisting of several Haars. Finally to speed up processing you can ask a slightly different question instead of <em>find me a face</em>. Namely, you can try to quickly eliminate the areas in the image that cannot be a face. This is called a cascade classification. Study these aspects in a systematic way and you will learn more about face detection than you\u2019d do from the code pasting.  </li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "int&#39; object has no attribute &#39;__getitem__&#39;",
        "input": "",
        "output": "<p>At a guess:</p>\n\n<pre><code>db = [0,file]\ni = 0\nz = [fe(db[i][1]) for i in range(len(db))]\n      # ^ here\n</code></pre>\n\n<p>When <code>i == 0</code>, <code>db[i] == 0</code>. You are then trying to index into an integer - <code>0[1]</code> - which, behind the scenes, effectively calls <code>0.__getitem__(1)</code>. Integers don't implement <code>__getitem__</code>, so this fails with the error you are seeing.</p>\n\n<hr>\n\n<p>Your code revision seems a bit odd, why do:</p>\n\n<pre><code>for file in imlist:\n    db = [[0,file]]   \n    z = [fe(db[i][1]) for i in range(len(db))]\n</code></pre>\n\n<p>And not simply:</p>\n\n<pre><code>for file in imlist:\n    z = [fe(file)]\n</code></pre>\n\n<p>However, without the full traceback, it is hard to precisely diagnose your error.</p>\n",
        "system": ""
    },
    {
        "instruction": "How do you process individual frames in V4L2 of a live stream from the camera?",
        "input": "",
        "output": "<p>Try using <a href=\"http://www.edwardrosten.com/cvd/cvd/html/index.html\" rel=\"nofollow\">libCVD</a>, which provides an interface for v4l2. More details <a href=\"http://www.edwardrosten.com/cvd/cvd/html/group__gVideo.html\" rel=\"nofollow\">here</a>. I've used libCVD in the past and it's quite straightforward and clean. Also lets you convert images easily to whatever format you prefer, such as RGB as you mention.</p>\n\n<p>You can also try using <a href=\"http://opencv.org/downloads.html\" rel=\"nofollow\">OpenCV</a>, which provides a <a href=\"http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture\" rel=\"nofollow\">VideoCapture</a> interface. These forums - <a href=\"http://answers.opencv.org/question/6713/how-to-set-or-get-videocapture-properties/\" rel=\"nofollow\">(1)</a> and <a href=\"http://opencv-users.1802565.n2.nabble.com/V4L2-and-OpenCV-td5810405.html\" rel=\"nofollow\">(2)</a> - contain some stubs of code, which you might find useful. With OpenCV as well, you have the flexibility to convert images to whatever format you need.</p>\n",
        "system": ""
    },
    {
        "instruction": "image processing / computer vision - body part recognition - posture ( standing/ sitting) - supervised learning",
        "input": "",
        "output": "<p>DPM is widely used in computer vision for object detection and it tends to work in the case of occlusion and also when only part of an object is present in the image. The grammar model for humans is very good and has state of the art results on standard datasets. It takes around a second to perform detection on a single image, its matlab code, so its expected to be slow.</p>\n\n<p><a href=\"http://www.cs.berkeley.edu/~rbg/latent/\" rel=\"nofollow\">http://www.cs.berkeley.edu/~rbg/latent/</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Markers with good features for recognizing multiple objects",
        "input": "",
        "output": "<p>Of course there are simpler solutions. They depend on the changes in your environment (e.g. lighting) and background (e.g. color composition). A simple rotation and scale invariant marker that would also tell you the pose as well as identity is a unique color combination. Look at <a href=\"http://www.ski.org/Rehab/Coughlan_lab/General/Publications/Wayfinding.pdf\" rel=\"nofollow noreferrer\"><strong>THIS paper</strong></a> paper for examples of color targets. It describes how you start identification searching for the middle of the target where coloros are located in specific order independent of orientation.</p>\n\n<p>To find orientation you can shape your target in a specific way or provide additional geometric cues on your target. I bet color targets printed on matte paper (to minimize specular reflections) will be more robust to out-of-plane rotation than ORB and much faster to detect. If I am not mistaken, with 4 colors you can create 12 unique orders for color targets.</p>\n\n<p><img src=\"https://i.sstatic.net/mbgs8.png\" alt=\"enter image description here\">\nCertain combination of colors in nature are very rare. They don't give you 100% identity but they reduce your search space tremendously and then you can easily run verification tests to separate your target from False alarms. Though colors change with illumination you can ensure that the ones you use are \"far\" from each other like green and red so illumination changes cannot mix them up even if you have liberal identification thresholds. </p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV camera calibration of an image crop (ROI submatrix)",
        "input": "",
        "output": "<p>If you crop the image, then the principal point is going to have different coordinates. For example, if your original principal point was at (30,40), and you crop 10 pixels around the sides, then your new principal point is going to be at (20,30), because the pixel (20,30) in the cropped image is the same as pixel (30,40) in the original image.</p>\n\n<p>You need to adjust the cx an cy values in your camera matrix accordingly.</p>\n",
        "system": ""
    },
    {
        "instruction": "number of keypoints by SIFT openCV?",
        "input": "",
        "output": "<p>It can be done by using the following line:</p>\n\n<pre><code>//The SIFT feature extractor and descriptor\nSiftDescriptorExtractor detector(20); \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to combine a bunch of feature points into a blob in OpenCV?",
        "input": "",
        "output": "<p>In <code>OpenCV</code>, you can use <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=convexhull#convexhull\" rel=\"nofollow\"><strong><code>convexHull()</code></strong></a> to find the <strong>convex hull</strong> of a point set.</p>\n\n<hr>\n\n<p><strong>Edit:</strong> if you simply need a bounded rectangle, in <code>OpenCV</code>, you can call <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=boundingrect#boundingrect\" rel=\"nofollow\"><strong><code>boundingRect()</code></strong></a> instead.</p>\n",
        "system": ""
    },
    {
        "instruction": "Human Tracking using KLT and mean shift",
        "input": "",
        "output": "<p>KLT isn't an object tracking algorithm. KLT will only estimate the displacement of a keypoint between frame at time <strong>t</strong> with the frame at time <strong>t-1</strong>.</p>\n\n<p>On the other hand, Mean-sift tracking is better suited for tracking objects, altho it's not very robust nor precise. You can watch an example of <a href=\"http://docs.opencv.org/trunk/doc/py_tutorials/py_video/py_meanshift/py_meanshift.html\" rel=\"nofollow\">Mean-Shift here</a>.</p>\n\n<p>If you need to track object, and be safe to possibile probems like occlusion, scale changes, appearance changes you need to adpot a real tracking algorithm. There are many of them you can watch an example of a tracking algorithm using keypoints here: <a href=\"http://www.youtube.com/watch?v=PVWyZse9z4E\" rel=\"nofollow\" title=\"Matrioska: Real-time Tracking using Keypoints\">Matrioska: Real-time Tracking using Keypoints</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Face mask in opencv",
        "input": "",
        "output": "<p>Using this below code the mask creation working perfectly for the sample image you provided in the above commants.</p>\n\n<p>Here assumes, your background is of any colour with no other object.</p>\n\n<p>The below code will do </p>\n\n<ul>\n<li><p>Find edge</p></li>\n<li><p>Strengthen the edge by <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/opening_closing_hats/opening_closing_hats.html\" rel=\"nofollow noreferrer\">morphology</a> operation.</p></li>\n<li><p>Find biggest contour in edge( always the boundary of your foreground ) and draw it by filling.</p></li>\n</ul>\n\n<p>Sometimes your contour may not be closed at the bottom(no edges at bottom)  so the filling contour wont work, so for make it closed the code first find the bounding rect for biggest contour(foreground) and then draw  bottom of  rect to the edge image, then  find largest contour again will give you the proper mask image.</p>\n\n<pre><code>Rect R;\n\nMat findLargestContour(Mat thr){\n\nvector&lt; vector &lt;Point&gt; &gt; contours; // Vector for storing contour\nvector&lt; Vec4i &gt; hierarchy;\nint largest_contour_index=0;\nint largest_area=0;\nMat dst(thr.rows,thr.cols,CV_8UC1,Scalar::all(0)); //create destination image\n\nfindContours( thr, contours, hierarchy,CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE );\nfor( int i = 0; i&lt; contours.size(); i++ ) // iterate through each contour.\n      {\n       double a=contourArea( contours[i],false);  //  Find the area of contour\n       if(a&gt;largest_area){\n       largest_area=a;\n       largest_contour_index=i;                //Store the index of largest contour\n       }\n      }\ndrawContours( dst,contours, largest_contour_index, Scalar(255,255,255),CV_FILLED, 8, hierarchy );\nR= boundingRect( contours[largest_contour_index]);\nreturn dst;\n\n}\n\nint main( )\n{\n\n              Mat right=imread(\"1.jpg\");\n           // blur(right,right,Size(3,3));\n              Mat gray;\n              cvtColor(right, gray, CV_RGB2GRAY);\n\n              int borderW=10;\n              //Mat ROI=gray(Rect(borderW,borderW,img1.cols-2*borderW,img1.rows-2*borderW));\n              Canny(gray, gray, 30, 255);\n\n              Size kernalSize (5,5);\n              Mat element = getStructuringElement (MORPH_RECT, kernalSize, Point(1,1)  );\n              morphologyEx(gray, gray, MORPH_CLOSE, element );\n              imshow(\"canny\", gray);\n\n              Mat largestCon=findLargestContour(gray.clone());\n              line(largestCon, Point(R.x,R.y+R.height), Point(R.x+R.width,R.y+R.height), Scalar(255),2,8,0);\n\n              Mat mask=findLargestContour(largestCon.clone());\n\n              Mat A;\n              right.copyTo(A,mask);\n              imshow(\"original\", right);\n              imshow(\"dst\", A);\n              imshow(\"mask\", mask);\n\n              waitKey(0);\n\n\n  return 0;\n  }\n</code></pre>\n\n<p>See some sample mask</p>\n\n<p><img src=\"https://i.sstatic.net/y7SqX.jpg\" alt=\"enter image description here\">  <img src=\"https://i.sstatic.net/bvtss.jpg\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/9Bjv2.jpg\" alt=\"enter image description here\">  <img src=\"https://i.sstatic.net/5DRZG.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "absdiff work on grayscale and not BGR colourspace",
        "input": "",
        "output": "<p>The cv2.absdiff does indeed work on color images. I verified this. Your error is elsewhere.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can I detect multiple signs using haar training",
        "input": "",
        "output": "<p>The most simple way is to detect signs with different XML files one after another and then combine the results.</p>\n",
        "system": ""
    },
    {
        "instruction": "Contour detection on sketched lines",
        "input": "",
        "output": "<p>You have zero noise in your image, that is there is no salt-and-pepper artefacts just a somewhat curvy lines.  The easiest way to merge them if there are not that many gaps is to use blur or morphological expansion to connect parts together (both may distort a shape just a bit) and then use findContour().</p>\n\n<p><img src=\"https://i.sstatic.net/YjQPl.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/eFeVI.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/udgQT.png\" alt=\"enter image description here\"></p>\n\n<p>If there are larger gaps you have to use convex Hull on convex parts and then on concave residuals. Snake or active contour algorithm is probably an overkill in this situation.</p>\n",
        "system": ""
    },
    {
        "instruction": "When using SVD how do I unsort the scale factors along Sigma&#39;s diagonal?",
        "input": "",
        "output": "<p>the singular values proportional to the length of principal axes that may or may not be oriented along x, y. Check U to see how they are oriented. Its columns show the coordinate of those axes.</p>\n\n<p>The beauty of SVD is that it can be applied to any matrix  <strong>A=USV<sup>T</sup></strong>  and its components have this meaning:</p>\n\n<p><strong>V<sup>T</sup></strong> - rotation matrix that rotates a circle (rotates its point vectors) to orient them properly for the next scaling operation. In other words circle remains a circle but data points slide along it (getting <strong>re-mapped</strong>) to orient them properly for the next operation. For example, if vector 1, 1 is to be squeezed it is better be aligned with 0, 1<br>\n<strong>S</strong> - scaling that always happens along x, y (this means x coordinates are scaled by sigmaX and y coordinates are scaled by sigmaY); thus it was important to pre-rotate data in case scaling is need to be done not at X, Y axis directions but at a different directions; it is like you know you are gonna get <strong>squeezed</strong> and you turn your side towards that force; note that after scaling the circle now turned into an ellipse with its axes oriented along x, y axis; For example, we may expand at x direction (coordinate) by factor 1.5 and squeeze at y direction by 0.9;<br>\n<strong>U</strong> - We may want the ellipse to be <strong>oriented</strong> at angle to x, y axis and the last rotation matrix U does exactly that. It rotates the ellipse to get it a certain desired orientation. For example, its longest axis may be rotated 45 deg. to point in the direction 1, 1 instead of original 1, 0.</p>\n\n<p>To sum up SVD represents any matrix multiplication as 3 consecutive operations: <strong>remap, squeeze, orient</strong>. A nice color illustration of this process can be found here: <a href=\"http://web4.cs.ucl.ac.uk/staff/s.prince/book/AppCPDF.zip\" rel=\"nofollow\">Analyze1SVD.pdf</a> (it is deep in the directory structure of the unpacked folder).</p>\n",
        "system": ""
    },
    {
        "instruction": "how to choose a range for filtering points by RGB color?",
        "input": "",
        "output": "<p>I am not sure I fully grasp what you are asking for, but in my opinion filtering in RGB is not the way to go. You should use a different color space than RGB if you want to compare pixels of similar color. RGB is good for representing colors on a screen, but you actually want to look at the hue, saturation and intensity (lightness, or luminance) for analysing visible similarities in colors.</p>\n\n<p>For example, you should convert your pixels to HSI or HSL color space first, then compare the different parameters you get. At that point, it is more natural to compare the resulting hue in a hue range, saturation in a saturation range, and so on.</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/HSL_and_HSV#Converting_to_RGB\" rel=\"nofollow\">Go here</a> for further information on how to convert to and from RGB.</p>\n",
        "system": ""
    },
    {
        "instruction": "Point Grey firefly MV, dc1394, and USB 3.0",
        "input": "",
        "output": "<p>Turns out that the problem can be solved with by updating to firmware version 1.6, and updating the linux kernel. It doesn't work at 3.5, it does work at 3.11. I'm not sure about in between.</p>\n",
        "system": ""
    },
    {
        "instruction": "Stereocalibration + Rectification in OpenCV",
        "input": "",
        "output": "<p>Please, look at the green matching lines at the bottom - there is no correspondence. In practice you have to use about 20-30 chess board pattern poses (calibration rigs) at different positions and orientation including in rotation in depth (slant, tilt), in-plane rotation and please cover the whole image at least in some of your calibration shots or cover it consistently part by part in different shots. </p>\n\n<p>The reason for needing multiple calibration images is following. Think about the point at infinity (called ideal or vanishing point) in homogeneous coordinates. The ideal point along x direction is<br>\nXinf = [1, 0, 0, 0]<sup>T</sup><br>\nIf you bring it back to Euclidean space you will get [1/0=Inf, 0/0, 0/0]<sup>T</sup>. If you multiply your projection matrix from the left with Xinf (in homog. coord) the result will have zeros everywhere except the first column. <strong>Conclusion:</strong> vanishing point in X direction gives you a first column of a projection matrix. Other columns come from other vanishing points. It is harder to prove the reverse - namely that to get correct P you need vanishing points, let's just assume it for now. </p>\n\n<p>The problem with your calibration is that you don't have clear vanishing points in your rig since it front-faces camera. You have to slant it to make projected line converge (into a vanishing points at multiple directions). Another problem is that your rig occupies only a small portion of the image and that's where optimization happened at the expense of other areas.  Recalibrate with multiple rig poses and you'll get better results. </p>\n",
        "system": ""
    },
    {
        "instruction": "How to match an object within an image to other images using SURF features (in MATLAB)?",
        "input": "",
        "output": "<p>If you have the Computer Vision System Toolbox, take a look at the following examples:</p>\n\n<ul>\n<li><a href=\"http://www.mathworks.com/help/vision/examples/object-detection-in-a-cluttered-scene-using-point-feature-matching.html\" rel=\"nofollow\">Object Detection In A Cluttered Scene Using Point Feature Matching</a></li>\n<li><a href=\"http://www.mathworks.com/help/vision/examples/image-search-using-point-features.html\" rel=\"nofollow\">Image Search using Point Features</a></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Hessian Matrix of the image",
        "input": "",
        "output": "<p>Hessian matrix describes the 2nd order local image intensity variations around the selected voxel. For the obtained Hessian matrix, eigenvector decomposition extracts an orthonormal coordinate system that is aligned with the second order structure of the image. Having the eigenvalues and knowing the \n(assumed) model of the structure to be detected and the resulting theoretical behavior of the eigenvalues, the decision can be made if the analyzed voxel belongs to the structure being searched. </p>\n\n<p>The figure below illustrates the correspondence between eigenvalues of the hessian operation on the image and the local features (corner, edge, or flat region).</p>\n\n<p><img src=\"https://i.sstatic.net/4yCvt.png\" alt=\"enter image description here\"></p>\n\n<p>The Hessian operator is also widely used in 3D images, and it can reflect more local features:</p>\n\n<p><img src=\"https://i.sstatic.net/Uwydf.png\" alt=\"enter image description here\"></p>\n\n<p>It is widely used in vessel detection in medical images. For more details, please see <a href=\"http://mechatronika.polsl.pl/owd/pdf2009/100.pdf\" rel=\"noreferrer\">M.Rudzki et al's Vessel Detection Method Based on Eigenvalues of the Hessian Matrix and its Applicability to Airway Tree Segmentation</a> </p>\n",
        "system": ""
    },
    {
        "instruction": "Why does opencv&#39;s HOG descriptor return so many values",
        "input": "",
        "output": "<blockquote>\n  <p><strong>Why does OpenCV fills the feature vector with so many values?</strong></p>\n</blockquote>\n\n<p>The size of hog features is determined by the following equation (not solely determined on the image dimensions):</p>\n\n<pre><code>size_hog_features = (size_t)nbins * ( blockSize.width/cellSize.width) \n                         * (blockSize.height/cellSize.height) * ((winSize.width \n                         - blockSize.width)/blockStride.width + 1) \n                         * ((winSize.height - blockSize.height)\n                         / blockStride.height + 1);\n</code></pre>\n\n<p>So it's quite normal you got such a long HOG feature vector.</p>\n\n<blockquote>\n  <p><strong>How do I obtain the 3780 long vector to feed to my SVM trainer?</strong></p>\n</blockquote>\n\n<p>You can setup the parameters (i.e. <code>nbins, blockSize, cellSize, winSize</code>) of HOG feature before computing it, in order to get a HOG feature with the size you want.</p>\n\n<blockquote>\n  <p><strong>But why are hogdis.getDescriptorSize() and features.size() inconsistent?</strong></p>\n</blockquote>\n\n<p>They are different. <code>getDescriptorSize()</code> returns the number of coefficients required for the classification. And it can be computed as follows (refer to <a href=\"http://www.juergenwiki.de/work/wiki/doku.php?id=public:hog_descriptor_computation_and_visualization\" rel=\"noreferrer\">here</a>):</p>\n\n<pre><code>HOG descriptor length = #Blocks * #CellsPerBlock * #BinsPerCell\n</code></pre>\n\n<p>On the other hand, <code>features.size()</code> returns all the HOG feature size of the whole image.</p>\n\n<p>To train, you need to pass in <code>features</code>. </p>\n",
        "system": ""
    },
    {
        "instruction": "Discrete Approximation to Gaussian smoothing",
        "input": "",
        "output": "<p>Typically people do it by applying two 1D Gaussian functions sequentially instead of one 2D function (idea of separable filters). You can approximate 1D horizontal Gaussian pretty fast, apply it to each pixel, save result in temp, and then apply a vertical Gaussian to temp. The expected speed up is quite significant: O(ksize*ksize) -> O(ksize+ksize)</p>\n",
        "system": ""
    },
    {
        "instruction": "ROI with SURF invalid argument",
        "input": "",
        "output": "<p>This is apparently a new feature in R2014a. Who knew.</p>\n",
        "system": ""
    },
    {
        "instruction": "About function sp_find_sift_grid(I, grid_x, grid_y, patch_size, sigma_edge)",
        "input": "",
        "output": "<p>I think it is in order to compare images with the same number of features. </p>\n\n<p>If you extract \"standard\" SIFT, you don't know how many interest point you will obtain. So if you want to compare 2 images with a different number of features (different number of points) it will be complicated, you can't use directly SVM nor Neural Network... because the number of features for each image have to be the same. </p>\n\n<p>With standard SIFT you need to match the points, find inliers and do others stuff or compute Bag of Visual Words before computing similarity between two images. </p>\n\n<p>If you just want to know how SIFT work, you can check wikipedia and David Lowe articles. </p>\n",
        "system": ""
    },
    {
        "instruction": "Face recognition Using EigenObjectRecognizer",
        "input": "",
        "output": "<p>your training set should have the same size (dimensions) and gray images</p>\n\n<pre><code>List&lt;Image&lt;Gray, byte&gt;&gt;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "What is the best method to template match a image with noise?",
        "input": "",
        "output": "<p>The fastest method is probably a cascade of boosted classifiers trained with several variations of your logo and possibly a few rotations and some negative examples too (non-logos). You have to roughly scale your overall image so the test and training examples are approximately matched by scale. Unlike SIFT or SURF that spend a lot of time in searching for interest points and creating descriptors for both learning and searching, binary classifiers shift most of the burden to a training stage while your testing or search will be much faster. </p>\n\n<p>In short, the cascade would run in such a way that a very first test would discard a large portion of the image. If the first test passes the others will follow and refine. They will be super fast consisting of just a few intensity comparison in average around each point. Only a few locations will pass the whole cascade and can be verified with additional tests such as your rotation-correlation routine. </p>\n\n<p>Thus, the classifiers are effective not only because they quickly detect your object but because they can also quickly discard non-object areas. To read more about boosted classifiers see a following openCV <a href=\"http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html?highlight=adaboost\" rel=\"nofollow\">section</a>. </p>\n",
        "system": ""
    },
    {
        "instruction": "Compare two bounding boxes with each other Matlab",
        "input": "",
        "output": "<p><strong>Edit</strong>: I have corrected the mistake pointed by other users.</p>\n\n<p>I am assuming you are detecting some object and you are drawing a bounding box around it. This comes under widely studied/researched area of object detection. The best method of evaluating the accuracy will be calculating <em>intersection over union</em>. This is taken from the PASCAL VOC challenge, from <a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00054000000000000000\" rel=\"noreferrer\">here</a>. See <a href=\"http://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png\" rel=\"noreferrer\">here</a> for visuals.</p>\n\n<p>If you have a bounding box detection and a ground truth bounding box, then the area of overlap between them should be greater than or equal to 50%. Suppose the ground truth bounding box is <code>gt=[x_g,y_g,width_g,height_g]</code> and the predicted bounding box is <code>pr=[x_p,y_p,width_p,height_p]</code> then the area of overlap can be calculated using the formula:</p>\n\n<pre><code>intersectionArea = rectint(gt,pr); %If you don't have this function then write a simple one for yourself which calculates area of intersection of two rectangles.\nunionArea = (width_g*height_g)+(width_p*height_p)-intersectionArea;\noverlapArea = intersectionArea/unionArea; %This should be greater than 0.5 to consider it as a valid detection.\n</code></pre>\n\n<p>I hope its clear for you now.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to track object in scene from far to near in opencv",
        "input": "",
        "output": "<p>Gaussian Mixture Models (GMM) isn't a bad choice for modelling cars as objects but I am not sure how you would model background with it since it can be much more heterogeneous and require a large mixture. Learning GMM with EM (Expectation Maximization) can be processing intensive. So my advice is to start using simple histograms that will model the object as a collection of bins with counts instead of collection of Gaussian distributions. Below I describe two such methods implemented in OpenCV.</p>\n\n<p>The simplest way to start tracking using openCV is by using a <a href=\"http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html?highlight=meanshift#cv.MeanShift\" rel=\"nofollow\">meanshift</a> function. A related method, <a href=\"http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html?highlight=camshift#cv.CamShift\" rel=\"nofollow\">Camshift</a> is a modified version of meanshift that also corrects the size and orientation of initial bounding box when the object change its size or orientation. You can find a corresponding demo in camshiftdemo.c of openCV package. </p>\n\n<p>The input to both meanshift and camshift is a probability map where each pixel indicates (approximately) what is the chance that it belongs to the object (and possibly background). One way to create such a map is to calculate the histogram of the object you want to track and then backproject it into the image, so if, for example, your histogram has only two bins with count 90 for I=255 and count 10 for I=100 each pixel with intensity 255 gets 90% probability and each pixel with intensity 100 gets 10% (see more detailed explanation about histogram <a href=\"http://docs.opencv.org/modules/imgproc/doc/histograms.html?highlight=backp#cv.CalcBackProject\" rel=\"nofollow\">backprojection</a>). </p>\n\n<p>Note that these methods are based on histograms of some features such as intensity, color, or virtually anything else. You can try to add more feature histograms and also correct your probability map relating your object histogram to the histogram of the background. Finally you can update your histograms at each frame to compensate for object and background changes.</p>\n\n<p>Now, in short, how it works. The meanshift, as the name suggests, moves your initial window according to the position of the probability mean (calculated within this window) and the mean is typically biased (shifted) away from the window center and towards the highest probability cluster (hopefully your object). This process repeats iteratively until convergence. </p>\n\n<p>Of course, if your initial guess is too far away from the real probability maximum the window can get stuck in the local maximum that has nothing to do with your object. Thus a high frame rate is important since it would guarantee that the object doesn't move much between frames and you can use a previous location as a reasonable guess for a window position initialization.</p>\n",
        "system": ""
    },
    {
        "instruction": "Augmented Reality Glasses + Depth Camera -&gt; Stereo Vision",
        "input": "",
        "output": "<p>Fist,  FindHomography(), GetPerspectiveTransformation() are the same transformations except the former takes repetitive attempts at later in Ransac framework. They match points between planes and thus aren\u2019t suitable for your 3d task. FundamentalMatrix and essentialMatrix aren\u2019t transformation they are buzz words you heard ;).  if you are trying to re-project a virtual object from a camera system into glasses point of view you simply have to apply rotation and translation in 3D to your object coordinates and then re-project them.</p>\n\n<p>The sequence of steps is:<br>\n1. find 3D coordinates of the landmark (say your hand) using a stereo camera;<br>\n2. place your control close to the landmark in 3D space (some virtual button?);<br>\n3. calculate a relative translation and rotation of each of your goggles viewpoints w.r.t stereo camera; for example, you may find that right goggles focal point is 3cm to the right from stereo camera and is rotated 10 deg around y axis or something; importantly left and right goggles focal point will be shifted  in space which creates an image disparity during re-projection (the greater the depth the smaller the disparity) which your brain interprets as a stereo cue for depth. Note that there are plenty of other cues for depth (for example blur, perspective, known sizes, vergence of the eyes, etc.) that may or may not be consistent with the disparity cue.<br>\n4. apply inverse of viewpoint transformation to the virtual 3d object; for example if the viewer goggles move to the right (wrt stereo camera) it is like an object moved left;<br>\n5. project these new 3D coordinates into the image as col=xf/z+w/2 and row=h/2-yf/z; using openGL can help to make projection look nicer.</p>\n",
        "system": ""
    },
    {
        "instruction": "Plot depth image in matlab",
        "input": "",
        "output": "<p>If you are only interested in a point cloud, you might want to consider <code>scatter3</code>.\nYou can select which points to plot (discard those with <code>depth</code> == 0).</p>\n\n<p>You need to have explicit x-y coordinates though.</p>\n\n<pre><code>[y x] = ndgrid( 1:size(img,1), 1:size(img,2) );\nsel = depth &gt; 0 ; % which points to plot\n% \"flatten\" the matrices for scatter plot\nx = x(:);\ny = y(:);\nimg = reshape( img, [], 3 );\ndepth = depth(:);\nscatter3( x(sel), y(sel), depth(sel), 20, img( sel, : ), 'filled' );\nview(158, 38)\n</code></pre>\n\n<p><strong>Edit:</strong> sampled version</p>\n\n<pre><code>[y x] = ndgrid( 1:2:size(img,1), 1:2:size(img,2) );\nsel = depth( 1:2:end, 1:2:end ) &gt; 0;\nx = x(:);\ny = y(:);\nimg = reshape( img( 1:2:end, 1:2:end, : ), [], 3 );\ndepth = depth( 1:2:end, 1:2:end );\nscatter( x(sel), y(sel), depth(sel), 20, img( sel, : ), 'filled' );\nview( 158, 38 );\n</code></pre>\n\n<p>Alternatively, you can directly manipulate <code>sel</code> mask.</p>\n",
        "system": ""
    },
    {
        "instruction": "Connected component labeling in Emgu / Opencv",
        "input": "",
        "output": "<p>I would use the following:</p>\n\n<pre><code>   connected component labeling (AForge / Accord.NET ?)\n   (although for many cases you will find it almost the same for the function that you wrote, give it a try to verify the results)\n</code></pre>\n\n<p>After this step you will probably find more regions that are close together and that belong to the same person.\nThan you can use:\n       implement or search for implementation of hierarhical aglomerative clustering (HCA)\nto combine close region.</p>\n\n<p>P.S.:\n is the chain approximation method for FindContours efficient/appropriate</p>\n\n<p>if you are using NO_APPROX no approximations of chain code will be used. By using this you can get non-smoothed edges (with many small hills and valleys) but if that does not bother you than this parameter value is fine.</p>\n",
        "system": ""
    },
    {
        "instruction": "difference between two images",
        "input": "",
        "output": "<p>You may consider <code>imshowpair</code> . Display a pair of grayscale images.</p>\n\n<pre><code>A = imread('PIX.tif');\nB = imrotate(A,5,'bicubic','crop');\n\nimshowpair(A,B,'diff');\n</code></pre>\n\n<p>other method as described over here. <a href=\"https://stackoverflow.com/questions/5475815/comparing-two-image-using-histogram\">comparing two image using histogram</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Align profile face image with its frontal face image",
        "input": "",
        "output": "<p>As discussed <a href=\"https://stackoverflow.com/a/10159939/2589776\">here</a> by @bytefish, finding the accurate position of the eyes in a given image is far from trivial. The Haar-cascades for finding the eyes in OpenCV produce too many false positive to be useful, moreover this approach won't be robust to image rotation.</p>\n\n<p>You'll need a robust head pose estimation for aligning face images. Here are two most robust ones (with code available):</p>\n\n<ul>\n<li><p><a href=\"http://www.janelia.org/people/scientist/gary-huang\" rel=\"nofollow noreferrer\">Gary B. Huang</a>, <a href=\"http://vis-www.cs.umass.edu/~vidit/\" rel=\"nofollow noreferrer\">Vidit Jain</a>, and <a href=\"http://www.cs.umass.edu/~elm/\" rel=\"nofollow noreferrer\">Erik Learned-Miller</a>. <em>Unsupervised joint alignment of complex images.</em> International Conference on Computer Vision (ICCV), 2007. <a href=\"http://vis-www.cs.umass.edu/congeal_complex.html\" rel=\"nofollow noreferrer\">(Project page)</a>, <a href=\"http://vis-www.cs.umass.edu/congeal_complex.html\" rel=\"nofollow noreferrer\">(PDF Online available)</a>, <a href=\"https://bitbucket.org/gbhuang/congealreal\" rel=\"nofollow noreferrer\">(Source code)</a></p></li>\n<li><p><a href=\"http://www.ics.uci.edu/~xzhu\" rel=\"nofollow noreferrer\">X. Zhu</a>, <a href=\"http://www.ics.uci.edu/~dramanan\" rel=\"nofollow noreferrer\">D. Ramanan</a>. <em>Face Detection, Pose Estimation and Landmark Localization in the Wild</em> Computer Vision and Pattern Recognition (CVPR) Providence, Rhode Island, June 2012. <a href=\"http://www.ics.uci.edu/~xzhu/face/\" rel=\"nofollow noreferrer\">(Project page)</a>, <a href=\"http://www.ics.uci.edu/~xzhu/face/\" rel=\"nofollow noreferrer\">(PDF Online available)</a>, <a href=\"https://github.com/wg-perception/PartsBasedDetector\" rel=\"nofollow noreferrer\">(Source code)</a></p></li>\n</ul>\n\n<hr>\n\n<p>For example, using the method described in the second paper, you will get more robust features like that are shown in the following images. And these robust features will, in turn, ensure to generate more robust face alignment performance.</p>\n\n<p><img src=\"https://i.sstatic.net/ZKHr3.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/cUUTU.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Combining multiple image descriptors (SIFT+HOG)",
        "input": "",
        "output": "<p>Concatenating features does not sound meaningful but you should try. It is called \"early fusion\". And it can works.</p>\n\n<p>Usually late fusion works better (learning the features separately and then merging the results/output of the two machine learning).</p>\n\n<p>I tested it for <a href=\"http://hal.archives-ouvertes.fr/hal-00946712\" rel=\"nofollow\">combining BoVW and BoW</a>, you should have a look in the paper, at section II, part C \"multimodal fusion techniques\".</p>\n",
        "system": ""
    },
    {
        "instruction": "c++ - Head pose variation removal using POSIT",
        "input": "",
        "output": "<p>Let me reeterate what you said: you got 3d coordinates of some points of a head as well as their projections. You plug them into <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=solvepnp#cv2.solvePnP\" rel=\"nofollow\">solvePnP(</a>) and got the pose. You want to reverse the transformation.</p>\n\n<p>The way to get a reverse transformation is to invert a general matrix encompassing all transformations or separetly negate a  translations and transpose a rotation matrix and then put them back together. I assume in a forward model rotation is applied first. In the reverse model apply the translation first. In the case you got a rotation vector instead of matrix you can use <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=solvepnp#void%20Rodrigues%28InputArray%20src,%20OutputArray%20dst,%20OutputArray%20jacobian%29\" rel=\"nofollow\">Rodrigues()</a> to convert it back to matrix form.</p>\n",
        "system": ""
    },
    {
        "instruction": "finding the centroid of words in a document image",
        "input": "",
        "output": "<p>Blurring can be a good way but depending on the font you can start blurring words with the words one string above or below. You can consider two more levels of representation of text elements to make your method more robust. </p>\n\n<p>You can go a level lower in your processing and get segments with letters. One can use <a href=\"http://docs.opencv.org/modules/features2d/doc/feature_detection_and_description.html?highlight=mser#MSER%20%3a%20public%20FeatureDetector\" rel=\"nofollow\">MSER</a> or connected components or <a href=\"http://www.math.tau.ac.il/~turkel/imagepapers/text_detection.pdf\" rel=\"nofollow\">Stroke Width Transform</a> to find letters. Monitoring the distance between letters will allow you to join them into words. A size of a letters and distance between them within a word can also give you a better idea about the possible distance between words. </p>\n\n<p>You can also go a level higher (in the processing hierarchy) and find strings. Then you can traverse a string making a histogram out of the vertical scan lines willed with letter pixels. The throughs in the histograms will indicate word boundaries. The height of the string can also commensurate with the length of white space between words. Finally, a string as a whole seems to less depend on the peculiarities of the font or capitalization compared to words.</p>\n",
        "system": ""
    },
    {
        "instruction": "Position Estimation From Multiple Images",
        "input": "",
        "output": "<p>Based on my experience, the short answer is:</p>\n\n<p>1) <strong>You cannot reliably estimate the 3D pose of the cameras independently from the 3D of the scene.</strong> Moreover, since your cameras are moving independently, I think SfM is the right way to approach your problem.</p>\n\n<p>2) <strong>You need to estimate the cameras' intrinsics</strong> in order to estimate useful (i.e. Euclidian) poses and scene reconstruction. If you cannot use the standard calibration procedure, with chessboard and co, you can have a look at the <a href=\"https://en.wikipedia.org/wiki/Camera_auto-calibration\" rel=\"nofollow\">autocalibration techniques</a> (see also chapter 19 in Hartley's &amp; Zisserman's book). This calibration procedure is done independently for each camera and only require several image samples at different positions, which seems appropriate in your case.</p>\n",
        "system": ""
    },
    {
        "instruction": "Real time ship tracking",
        "input": "",
        "output": "<p>Do you want to track or detect the ships? At least for the videos you posted, the tracking problems seems quite easy. Even a simple x-correlation tracker should have no trouble following points on those ships for quite long subsequences.</p>\n",
        "system": ""
    },
    {
        "instruction": "Figuring out discrete or continuous image in matlab",
        "input": "",
        "output": "<p>Following @chappjc's nice idea: the continuous image is the one that has more colors, and the other is the discrete one.</p>\n\n<pre><code>img1 = rand(100,200,3); %// example continuous image\nimg2 = randi(5,100,200,3)/10; %// example discrete image\n\n[m1 n1 p1] = size(img1);\nc1 = size(unique(reshape(img1, m1*n1, p1), 'rows'),1); %// number of colors\n[m2 n2 p2] = size(img2);\nc2 = size(unique(reshape(img2, m2*n2, p2), 'rows'),1); %// number of colors\n\nif c1&gt;c2\n    disp('First image is continuous, second is discrete')\nelse\n    disp('First image is discrete, second is continuous')\nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "how to detect an object with the same color as the background",
        "input": "",
        "output": "<p>Consider using as many visual cues as possible:\n<a href=\"https://stackoverflow.com/a/22112168/457687\">blur/focus</a><br>\nshape  - you can use <a href=\"https://stackoverflow.com/a/22212591/457687\">active contour or findControus()</a> on a clean image<br>\nlocation,  intensity, and texture in <a href=\"http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=grabcut#cv2.grabCut\" rel=\"nofollow noreferrer\">grabcut</a> framework<br>\nyou can try IR illumination in case moth and glass react to it differently  </p>\n",
        "system": ""
    },
    {
        "instruction": "want to convert the class information of segmented image to node (i.e graph vertex) information in Matlab",
        "input": "",
        "output": "<p>The 'grid' you mention is an image filter which yields a new image where each pixel is 1 where it detects a cell and 0 where no cell is detected.<br>\nAfter that they will use some further processing, image segmentation, blob analysis, bwlabel, regionprops etc, to determine the number, and size and shape of the cells. Have a look at</p>\n\n<p><a href=\"http://blogs.mathworks.com/pick/2009/11/06/segmenting-coinsa-tutorial-on-blob-analysis/\" rel=\"nofollow\">http://blogs.mathworks.com/pick/2009/11/06/segmenting-coinsa-tutorial-on-blob-analysis/</a></p>\n\n<p>Edit: here is the paper:<br>\n<a href=\"http://bioinformatics.oxfordjournals.org/content/20/suppl_1/i145.full.pdf+html\" rel=\"nofollow\">http://bioinformatics.oxfordjournals.org/content/20/suppl_1/i145.full.pdf+html</a></p>\n\n<p>In answer to your questions about the node information - it is in the form of a 2d image, same size as the input:</p>\n\n<blockquote>\n  <p>The next step is to translate the class information obtained in Step 1\n  to node information of a cell graph ...  At the end of this step, the\n  spatial information of the cells is translated to their locations in\n  the two-dimensional grid ... This step can also be considered as\n  downsampling of an image... ... In the last step, we set the links\n  between the nodes found in Step 2 to generate a graph (Fig. 2f ).  We\n  make use of the Waxman model ... the distance between the nodes\n  (hence, the link probability) describes the prevalence of cancer</p>\n</blockquote>\n\n<p>This goes right up to fig 2f in the paper. Then in step 3:</p>\n\n<blockquote>\n  <p>After obtaining the cell graph, we define cell-graph metrics for each\n  node to quantify their cell-network characteristics including cell\n  degree, clustering coefficient and eccentricity... whether a given\n  node of a graph is cancerous, healthy, or inflamed.</p>\n</blockquote>\n\n<p>So in step 3 they will have a list of x,y pairs for the location of each node and all the other necessary parameters.</p>\n",
        "system": ""
    },
    {
        "instruction": "Locating and extracting (unknown) book from an image in OpenCV",
        "input": "",
        "output": "<p>Does calculating homography with 4 lines instead of 4 points help the problem? As you probably know, if points are related as p2=Hp1, the  lines are related as l2=H<sup>-1</sup>l1. The lines on the book border should be quite prominent especially if the deformation is not large. Is you main problem selecting right lines (you did NOT actually said what's your problem was)? May be some kind of Hough-rectangle can help to find lines?</p>\n\n<p>Anyway, selecting lines for homography input has an additional advantage that RANSAC homography with a constraint on aspect ratio is likely to keep right lines as inliners in the presence of numerous outliers from the background. And if those outliers sneak in they probably look like another book. </p>\n",
        "system": ""
    },
    {
        "instruction": "Nose Tip Detection from 3D point cloud",
        "input": "",
        "output": "<p>I used to work with Kinect images that have a limit on depth z > .5m, see below. I hope you don\u2019t have this restriction with your ToF camera. Nose as an object is not very pronounced but probably can be detected using connected components on depth image. You have to find it as a blob on otherwise flat face. You can further confirm that it is a nose by comparing face depth with nose depth and nose position relative to the face. This of course doesn\u2019t apply to the non frontal pose where nose should be found differently.</p>\n\n<p>I suggest inverting your logical chain of processing: find nose then found face and start looking for a head first (as a larger object with possibly better depth contrast) and then for nose. Head is well defined by its size and shape in 3D and a face 2D detection can also fit a raw head model into your 3D point cloud using similarity transform in 3D.</p>\n\n<p><a href=\"https://stackoverflow.com/a/22053017/457687\">link to Kinect depth map</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How to visualize intrinsic/extrinsic camera parameters in OpenCV?",
        "input": "",
        "output": "<p>I don't think there is such a thing in OpenCV.  But you can, in principle, take your calibration data into matlab, and manually construct a <code>cameraParameters</code> object, which you can then pass into <code>showExtrinsics()</code>. </p>\n",
        "system": ""
    },
    {
        "instruction": "find center points/ segment white blobs in binary images",
        "input": "",
        "output": "<p>Seems this is what exactly you are looking for: <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/25157-image-segmentation-tutorial-blobsdemo\" rel=\"nofollow\">Image Segmentation Tutorial (\"BlobsDemo\")</a>.</p>\n\n<p>It contains demo to illustrate simple blob detection, measurement, and filtering. First it finds all the objects, then filters results to pick out objects of certain sizes. The basic concepts of <strong>thresholding</strong>, <strong>labeling</strong>, and <strong>regionprops</strong> are demonstrated with examples.</p>\n",
        "system": ""
    },
    {
        "instruction": "Perspective Compensation when Measuring Distances on an Image with a known reference distance",
        "input": "",
        "output": "<p>A good study on this and similar problems can be found in Antonio Criminisi's papers and Ph.D. thesis on single-view metrology. <a href=\"http://research.microsoft.com/apps/pubs/default.aspx?id=67278\" rel=\"nofollow noreferrer\">This</a> is a good link to start, and the whole paperdump is <a href=\"http://www.robots.ox.ac.uk/~criminis/pubbli.html\" rel=\"nofollow noreferrer\">here</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Is it possible to have a cv::Mat that contains pointers to scalars rather than scalars?",
        "input": "",
        "output": "<pre><code>vtkSmartPointer&lt;vtkImageData&gt; image = vtkSmartPointer&lt;vtkImageData&gt;::New();\n\nint dims[3];\nimage-&gt;GetImageData()-&gt;GetDimensions(dims);\ncv::Mat matImage(cv::Size(dims[0], dims[1]), CV_8UC3, image-&gt;GetImageData()-&gt;GetScalarPointer());`\n</code></pre>\n\n<p>I succeeded implementing <code>vtkimagedata*</code> directly into <code>cv::Mat</code> pointer array..</p>\n\n<p>some functions such as\n <code>cv::flip</code> or <code>matImage -= cv::Scalar(255,0,0)</code></p>\n\n<p>directly working on vtkimagedata.</p>\n\n<p>but functions like <code>cv::resize</code> or <code>cv::Canny</code> doesn't work. </p>\n",
        "system": ""
    },
    {
        "instruction": "Support vector machines for mutliple object categorization",
        "input": "",
        "output": "<p>In one-vs-all approach, you have to check for all 5 models. Then you can take the decision with the most confidence value. <code>LIBSVM</code> gives probability estimates. </p>\n\n<p>In one-vs-one approach, you can take the majority. For example, you test 1 vs. 2, 1 vs. 3, 1 vs. 4 and 1 vs. 5. You classify it as 1 in 3 cases. You do the same for other 4 classes. Suppose for other four classes the values are <code>[0, 1, 1, 2]</code>. Therefore, class 1 was obtained most number of times, making that class as the final class. In this case, you could also do total of probability estimates. Take the maximum. That would work unless in one pair the classification goes extremely wrong. For example, in 1 vs. 4, it classifies 4 (true class is 1) with a confidence 0.7. Then just because of this one decision, your total of probability estimates may shoot up and give wrong results. This issue can be examined experimentally.</p>\n\n<p>LIBSVM uses one vs. one. You can check the reasoning <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f419\" rel=\"nofollow\">here</a>. You can read <a href=\"http://machinelearning.wustl.edu/mlpapers/paper_files/RifkinK03.pdf\" rel=\"nofollow\">this</a> paper too where they defend one vs. all classification approach and conclude that it is <em>not</em> necessarily worse than one vs. one.</p>\n",
        "system": ""
    },
    {
        "instruction": "Camera matrix measurement units",
        "input": "",
        "output": "<p>T is in mm, but it have to be multiplied by KR:\nP = [KR | -KRT];</p>\n<p>For details see good lecture on <a href=\"https://cw.fel.cvut.cz/wiki/lib/exe/fetch.php?tok=0b9c50&amp;media=https%3A%2F%2Fcw.fel.cvut.cz%2Fwiki%2F_media%2Fcourses%2Fgvg%2Fpajdla-gvg-lecture-2021.pdf\" rel=\"nofollow noreferrer\">Geometry for computer vision</a>, p.40</p>\n",
        "system": ""
    },
    {
        "instruction": "Biological Cell shape detection with Matlab",
        "input": "",
        "output": "<p>There are 4 types of cell detection/segmentation: pixel-based, region-based, edge-based and contour-based segmentation. You may use one or several combinations of them for your task. But counting only on the shape may be insufficient. </p>\n\n<p>The main difference between erythrocyte and leukocyte is the existence of nucleus. To my knowledge, the nucleus staining is often applied to microscopy. If that is the case, </p>\n\n<p>(i) the ratio between green and blue channel intensities of each pixel can be used as discriminating feature to separate the nucleus pixels from other foreground pixels;</p>\n\n<p>(ii) After that, it is possible to extract the leukocyte plasma based on the hue-value similarity between the pixel from that region and the nucleus region;</p>\n\n<p>(iii) Contour-based methods such as active-contour methods (snakes) and level-set approaches can be used to refine the boundaries of white blood cells;</p>\n\n<p>(iv) What left to you after (i)-(iii) are probably the erythrocytes. If your task also includes the segmentation of erythrocytes, you may threshold them easily (or search the studies for more accurate segmentation algorithms).</p>\n\n<p>I would recommend <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/19163356\" rel=\"nofollow\">T.Bergen et al, Segmentation of leukocytes and erythrocytes in blood smear images</a>. My description above was included and detailed in this paper, and they applied more sophisticated strategies to improve the boundary accuracy. You may try to follow their steps and reproduce the similar result if your ultimate goal is also segmentation. Yet only detection without extraction might be much easier.  </p>\n",
        "system": ""
    },
    {
        "instruction": "Creating a wrl/obj model from point cloud",
        "input": "",
        "output": "<p>The first goal is to convert your data into a vtkPolyData, then it is straightforward to output it as an OBJ.</p>\n\n<p>You have to compute the 3D points. If your data was acquired in a non-projective manner, You can simply make the 'x' and 'y' values of the coordinates proportional to the pixel indices, and the depth value proportional to the grayscale value. If not, you'd need to know the calibration matrix of the camera you used to acquire the image, then shoot rays from the camera center through each pixel, and move along each ray an amount proportional to the the grayscale value to find the (x,y,z) coordinate of the point.</p>\n\n<p>Once you have the 3D points, it is easy to attach the normals and the colors.</p>\n",
        "system": ""
    },
    {
        "instruction": "calcOpticalFlowPyrLK performance worse at high FPS",
        "input": "",
        "output": "<p>the main cpp</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;queue&gt;\n#include &lt;opencv2/opencv.hpp&gt;\n#include \"corner_tracker.h\"\nusing namespace std;\nusing namespace cv;\n\nint main(int argc, char** argv) {\n  if (argc != 2) {\n    cout &lt;&lt; \"usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;video path&gt;\" &lt;&lt; endl;\n    exit(1);\n  }\n  int frame_lag = 4;\n  string video_filepath(argv[1]);\n  VideoCapture vidcap(video_filepath);\n  Mat ref_frame, curr_frame, prev_frame;\n  queue&lt;Mat&gt; frame_buffer;\n  vector&lt;Point2f&gt; tracked_corners;\n  vector&lt;Point2f&gt; optical_flow;\n  CornerTrackerParameterBlock param;\n  CornerTracker corner_tracker(param);\n  Mat mask;\n  while (true){\n    vidcap &gt;&gt; ref_frame;\n    if (ref_frame.empty()) break;\n    cvtColor(ref_frame, curr_frame, CV_BGR2GRAY);\n\n    Mat tmp_frame;\n    curr_frame.copyTo(tmp_frame);\n    frame_buffer.push(tmp_frame);\n    if ((int)frame_buffer.size() &lt; frame_lag+1 ) {\n      continue;\n    }\n    prev_frame = frame_buffer.front();\n    frame_buffer.pop();\n    corner_tracker.TrackCorners(prev_frame, curr_frame, mask, 100, tracked_corners, optical_flow);\n    for (int i = 0; i &lt; (int)tracked_corners.size(); i++) {\n      //because optical flow is calculated between current frame and the frame_lag frame before it\n      //the actual value of the optical flow vector has to be normalized\n      Point2f normalized_optical_flow = optical_flow[i]*(1.0/(double)frame_lag);\n      line(ref_frame, tracked_corners[i], tracked_corners[i] + normalized_optical_flow, Scalar(0,255,0));\n      circle(ref_frame, tracked_corners[i], 2, Scalar(0,0,255));\n    }\n    imshow(\"window\",ref_frame);\n    if((char)waitKey(30) == 27) {\n      break;\n    }\n  }\n  return 0;\n}\n</code></pre>\n\n<p>corner tracker header file</p>\n\n<pre><code>#ifndef CORNER_TRACKER_H_\n#define CORNER_TRACKER_H_\n#include &lt;opencv2/core/core.hpp&gt;\n\nstruct CornerTrackerParameterBlock {\n  double lkt_max_bidirectioal_error;\n  int lkt_maxlevel;\n  int lkt_winsize;\n  int feature_blocksize;\n  double feature_k;\n  double feature_mindist;\n  double feature_quality_level;\n\n//default constructor\nCornerTrackerParameterBlock(void) :\n  lkt_max_bidirectioal_error(2.0),\n  lkt_maxlevel(3),\n  lkt_winsize(16),\n  feature_blocksize(3),\n  feature_k(0.04),\n  feature_mindist(5.0),\n  feature_quality_level(0.01)\n  {}\n};\n\nclass CornerTracker {\npublic:\n  CornerTracker(const CornerTrackerParameterBlock&amp; param);\n  void TrackCorners(const cv::Mat&amp; prev_frame, const cv::Mat&amp; curr_frame, const cv::Mat&amp; mask, int max_corners, std::vector&lt;cv::Point2f&gt;&amp; tracked_corners, std::vector&lt;cv::Point2f&gt;&amp; optical_flow_vectors) const;\nprivate:\n  void AddAdditionalCorners(const cv::Mat&amp; curr_frame, const cv::Mat&amp; mask, int max_corners, std::vector&lt;cv::Point2f&gt;&amp; tracked_corners) const;\n  CornerTrackerParameterBlock m_param;\n};\n\n#endif //CORNER_TRACKER_H_\n</code></pre>\n\n<p>corner tracker cpp file</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;opencv2/imgproc/imgproc.hpp&gt;\n#include &lt;opencv2/video/tracking.hpp&gt;\n#include \"corner_tracker.h\"\n\nusing namespace std;\nusing namespace cv;\n\nCornerTracker::CornerTracker(const CornerTrackerParameterBlock&amp; param) :\n  m_param(param)\n{}\n\nvoid CornerTracker::AddAdditionalCorners(const cv::Mat&amp; curr_frame, const cv::Mat&amp; mask, int max_corners, std::vector&lt;cv::Point2f&gt;&amp; tracked_corners) const {\n  //detect additional features\n  int additional_corners = max_corners - tracked_corners.size();\n  if (additional_corners &lt;= 0) return;\n  //generate mask\n  Mat tmp_mask;\n  if (mask.rows != curr_frame.rows || mask.cols != curr_frame.cols || mask.type() != CV_8U) {\n    tmp_mask.create(curr_frame.rows, curr_frame.cols, CV_8U);\n    tmp_mask = Scalar::all(255);\n  }\n  else {\n    mask.copyTo(tmp_mask);\n  }\n  //mask out current points\n  for (const Point2f&amp; p : tracked_corners) {\n    circle(tmp_mask, p, m_param.feature_mindist, Scalar::all(0), -1); //filled black circle\n  }\n  vector&lt;Point2f&gt; corners;\n  goodFeaturesToTrack(curr_frame, corners, additional_corners, m_param.feature_quality_level, m_param.feature_mindist, tmp_mask, m_param.feature_blocksize, true, m_param.feature_k );\n  for (const Point2f&amp; p : corners) {\n    tracked_corners.push_back(p);\n  }\n}\n\nvoid CornerTracker::TrackCorners(const cv::Mat&amp; prev_frame, const cv::Mat&amp; curr_frame, const cv::Mat&amp; mask, int max_corners, std::vector&lt;cv::Point2f&gt;&amp; tracked_corners, std::vector&lt;cv::Point2f&gt;&amp; optical_flow_vectors) const {\n  AddAdditionalCorners(curr_frame, mask, max_corners, tracked_corners);\n  vector&lt;Point2f&gt; prev_corners(tracked_corners);\n  vector&lt;Point2f&gt; next_corners(tracked_corners);\n  //optical flow corner tracking\n  vector&lt;uchar&gt; status1,status2;\n  vector&lt;float&gt; error1,error2;\n  calcOpticalFlowPyrLK(curr_frame, prev_frame, tracked_corners, prev_corners, status1, error1, Size(m_param.lkt_winsize,m_param.lkt_winsize), m_param.lkt_maxlevel, TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), OPTFLOW_USE_INITIAL_FLOW);\n  calcOpticalFlowPyrLK(prev_frame, curr_frame, prev_corners, next_corners, status2, error2, Size(m_param.lkt_winsize,m_param.lkt_winsize), m_param.lkt_maxlevel, TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01), OPTFLOW_USE_INITIAL_FLOW);\n  //check tracked corner quality\n  vector&lt;Point2f&gt; temp_corners;\n  optical_flow_vectors.clear();\n  for (unsigned int i = 0; i &lt; tracked_corners.size(); i++) {\n    if (status1[i] == 0 || status2[i] == 0) {\n      continue;\n    }\n    float bidirectional_error = norm(next_corners[i] - tracked_corners[i]);\n    //bidirectional error check\n    if (bidirectional_error &gt; m_param.lkt_max_bidirectioal_error) {\n      continue;\n    }\n    optical_flow_vectors.push_back(tracked_corners[i] - prev_corners[i]);\n    temp_corners.push_back(tracked_corners[i]);\n  }\n  tracked_corners.swap(temp_corners);\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV changing brightness and contrast",
        "input": "",
        "output": "<p>1) I just verified. It was darkened properly. Are you sure that it really became brighter? How are you checking that?</p>\n\n<p>2) You don't need to use functions like convertTo since you are not converting anything. Much more readable way would be:</p>\n\n<pre><code>image = alpha*image + beta;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Optical flow based tracking of quadrilateral points in OpenCV",
        "input": "",
        "output": "<p>The most similar to you requirements is TLD algorithm,</p>\n\n<p>take a look here:</p>\n\n<p><a href=\"http://www.youtube.com/watch?v=W2qR60hrD2w\" rel=\"nofollow\">http://www.youtube.com/watch?v=W2qR60hrD2w</a></p>\n\n<p>you can find description here</p>\n\n<p><a href=\"http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html\" rel=\"nofollow\">http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html</a></p>\n\n<p>The paper:</p>\n\n<p><a href=\"http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/Publications/2009_olcv.pdf\" rel=\"nofollow\">http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/Publications/2009_olcv.pdf</a></p>\n\n<p>And the source code here:</p>\n\n<p><a href=\"https://github.com/zk00006/OpenTLD\" rel=\"nofollow\">https://github.com/zk00006/OpenTLD</a></p>\n",
        "system": ""
    },
    {
        "instruction": "frame difference method[background subtraction]",
        "input": "",
        "output": "<p>You are opening a bmp file with VideoCapture. Try a video, but capturing 2 frames without a delay in between may cause some problems.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to speed up LBP texture matching",
        "input": "",
        "output": "<p>You can create a heirarchy in the image database and avoid template matching with all images.\nYou can use <a href=\"https://ieeexplore.ieee.org/document/711175\" rel=\"nofollow noreferrer\">this idea</a> for it.</p>\n<p>Otherwise, you can try to <a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction\" rel=\"nofollow noreferrer\">reduce the dimension</a> of the LBP histogram. You can even think of using some other technique (like <a href=\"https://en.wikipedia.org/wiki/Gabor_transform\" rel=\"nofollow noreferrer\">Gabor transform</a>) on the LBP image to extract feature. Google tells me that it has been used greatly for texture analysis.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Face Detection function only works at the second call",
        "input": "",
        "output": "<p>Wait for user input is <em>not</em> all.</p>\n\n<p>Calling the <code>waitKey</code> is essential even if you don't care about keyboard input.</p>\n\n<p>From <a href=\"http://docs.opencv.org/modules/highgui/doc/user_interface.html?highlight=waitkey#int%20waitKey%28int%20delay%29\" rel=\"nofollow\">OpenCV DOC</a>:</p>\n\n<blockquote>\n  <p>This function is the only method in HighGUI that can fetch and handle\n  events, so it needs to be called periodically for normal event\n  processing unless HighGUI is used within an environment that takes\n  care of event processing</p>\n</blockquote>\n\n<p>On your code:</p>\n\n<pre><code>imshow(\"original\", frame);\n</code></pre>\n\n<p>The <code>imshow()</code> is a function of HighGui and the code need a call of <code>waitKey</code> reguraly, in order to process its event loop. If you don't call waitKey the HighGui can't process windows events like redraw.</p>\n\n<p>So, basically, your are allowing the HighGui process windows events calling <code>waitKey</code>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Algorithm for ortho-rectification; mosaic-ing aerial images",
        "input": "",
        "output": "<p><strong>Image stitching</strong> usually assumes that the camera center is fixed across all photos, and uses homographies to transform the images so that they seem continuous. When the fixed camera center assumption is not strictly valid, artifacts/distortions may appear due to the 3D of the scene. If the camera center moved by a small distance compared to the relief of the scene, \"seamless image blending\" techniques may be sufficient to blur out the distortions.</p>\n\n<p>In more extreme cases, ortho-rectification is required. <strong>Ortho-rectification</strong> (<a href=\"http://en.wikipedia.org/wiki/Orthophoto\" rel=\"noreferrer\">Wikipedia entry</a>) is the task of transforming an image observed from a given <em>perspective</em> camera into an <em>orthographic</em> (<a href=\"http://en.wikipedia.org/wiki/Orthographic_projection\" rel=\"noreferrer\">Wikipedia entry</a>) and usually vertical point of view. The orthographic property is interesting because it makes the stitching of several images much easier. The following picture from Wikipedia is particularly clear (left is an orthographic or directional projection, right is a perspective or central projection):</p>\n\n<p><img src=\"https://i.sstatic.net/5cAVH.png\" alt=\"enter image description here\"></p>\n\n<p>The task of ortho-rectification usually requires having a <strong>3D model of the scene</strong>, in order to map appropriately intensities observed by the perspective camera to their location with respect to the orthographic camera. In the context of aerial/satellite images, Digital Elevation Models (DEM) are often used for that purpose, but generally have the serious drawback of not including man-made structures (only Earth relief). The NASA provides freely the DEM acquired by the <a href=\"http://en.wikipedia.org/wiki/Shuttle_Radar_Topography_Mission\" rel=\"noreferrer\">SRTM missions</a> (<a href=\"http://www2.jpl.nasa.gov/srtm/\" rel=\"noreferrer\">DEM link</a>).</p>\n\n<p>Another approach, if you have two images acquired at different positions, you could try to do a 3D reconstruction using one of the stereo matching technique, and then to generate the ortho-rectified image by mapping the two images as seen by a third orthographic and vertical camera.</p>\n\n<p>OpenCV has several interesting function for that purpose (e.g. stereo reconstruction, image mapping functions, etc) and might be more appropriate for intensive usage. Matlab probably has interesting functions as well, and might be more appropriate for quick tests.</p>\n",
        "system": ""
    },
    {
        "instruction": "Local Binary Patterns original code and references in matlab",
        "input": "",
        "output": "<pre><code>% clc;    % Clear the command window.\n% close all;  % Close all figures (except those of imtool.)\n% imtool close all;  % Close all imtool figures.\n% clear;  % Erase all existing variables.\n% workspace;  % Make sure the workspace panel is showing.\n% fontSize = 20;\n% % Read in a standard MATLAB gray scale demo image.\n% folder = fullfile(matlabroot, '\\toolbox\\images\\imdemos');\n% baseFileName = 'cameraman.tif';\n% % Get the full filename, with path prepended.\n% fullFileName = fullfile(folder, baseFileName);\n% if ~exist(fullFileName, 'file')\n%   % Didn't find it there.  Check the search path for it.\n%   fullFileName = baseFileName; % No path this time.\n%   if ~exist(fullFileName, 'file')\n%       % Still didn't find it.  Alert user.\n%       errorMessage = sprintf('Error: %s does not exist.', fullFileName);\n%       uiwait(warndlg(errorMessage));\n%       return;\n%   end\n% end\ngrayImage = imread('fig.jpg');\n% Get the dimensions of the image.  numberOfColorBands should be = 1.\n[rows columns numberOfColorBands] = size(grayImage);\n\n% Display the original gray scale image.\nsubplot(2, 2, 1);\nimshow(grayImage, []);\n%title('Original Grayscale Image', 'FontSize', fontSize);\n% Enlarge figure to full screen.\nset(gcf, 'Position', get(0,'Screensize')); \nset(gcf,'name','Image Analysis Demo','numbertitle','off') \n% Let's compute and display the histogram.\n[pixelCount grayLevels] = imhist(grayImage);\nsubplot(2, 2, 2); \nbar(pixelCount);\n%title('Histogram of original image', 'FontSize', fontSize);\nxlim([0 grayLevels(end)]); % Scale x axis manually.\n% Preallocate/instantiate array for the local binary pattern.\nlocalBinaryPatternImage = zeros(size(grayImage));\nfor row = 2 : rows - 1   \n    for col = 2 : columns - 1    \n        centerPixel = grayImage(row, col);\n        pixel7=grayImage(row-1, col-1) &gt; centerPixel;  \n        pixel6=grayImage(row-1, col) &gt; centerPixel;   \n        pixel5=grayImage(row-1, col+1) &gt; centerPixel;  \n        pixel4=grayImage(row, col+1) &gt; centerPixel;     \n        pixel3=grayImage(row+1, col+1) &gt; centerPixel;    \n        pixel2=grayImage(row+1, col) &gt; centerPixel;      \n        pixel1=grayImage(row+1, col-1) &gt; centerPixel;     \n        pixel0=grayImage(row, col-1) &gt; centerPixel;       \n        localBinaryPatternImage(row, col) = uint8(...\n            pixel7 * 2^7 + pixel6 * 2^6 + ...\n            pixel5 * 2^5 + pixel4 * 2^4 + ...\n            pixel3 * 2^3 + pixel2 * 2^2 + ...\n            pixel1 * 2 + pixel0);\n    end  \nend \nsubplot(2,2,3);\nimshow(localBinaryPatternImage, []);\n%title('Local Binary Pattern', 'FontSize', fontSize);\nsubplot(2,2,4);\n[pixelCounts, GLs] = imhist(uint8(localBinaryPatternImage));\nbar(GLs, pixelCounts);\n%title('Histogram of Local Binary Pattern', 'FontSize', fontSize);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Histogram of Oriented Gradients as feature vectors with vlfeat/libsvm",
        "input": "",
        "output": "<p>You need to create a global representation of your local features so that you can feed your data to SVMs. One of the most popular approaches for this task is bag-of-words(features). vlfeat has an excellent demo/example for this. You can check this code from <a href=\"http://www.vlfeat.org/applications/caltech-101-code.html\" rel=\"nofollow\" title=\"vlfeat bow demo\">vlfeat website</a>.</p>\n\n<p>For your particular case, you need arrange your training/testing data in Caltech-101 like data directories:</p>\n\n<ul>\n<li>Letter 1\n<ul>\n<li>Image 1</li>\n<li>Image 2</li>\n<li>Image 3</li>\n<li>Image 4</li>\n<li>...</li>\n</ul></li>\n<li>Letter 2\n<ul>\n<li>...</li>\n</ul></li>\n<li>Letter 3\n<ul>\n<li>...</li>\n</ul></li>\n</ul>\n\n<p>Then you need to adjust following configuration settings for your case:</p>\n\n<p>conf.numTrain = 15 ;</p>\n\n<p>conf.numTest = 15 ;</p>\n\n<p>conf.numClasses = 102 ;</p>\n\n<p>This demo uses SIFT as local features, but you can change it to HOG afterwards.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to take stereo images using single camera?",
        "input": "",
        "output": "<p>Not sure I've understood your problem correclty - will try anyway</p>\n\n<p>I guess your currently working with images from <a href=\"http://vision.middlebury.edu/stereo/\" rel=\"nofollow\">middlebury</a> or something similar. If you want to use similar algorithms you have to rectify your images because they are based on the assumption that corresponding pixels are on the same line in all images. If you actually want depth images (!= disparity images) you also need to get the camera extrinsics.</p>\n\n<p>Your setup should have two cameras and you have to make sure that they don't change there relative position/orientation - otherwise your rectification will break apart. In the first step you have to calibrate your system to get intrinsic and extrinsic camera parameters. For that you can either use some tool or roll your own with (for example) OpenCV (<a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\" rel=\"nofollow\">calib-module</a>). Print out a calibration board to calibrate your system. Afterwards you can take images and use the calibration to rectify the images. </p>\n\n<p>Regarding color-noise:<br/>\nYou could make your aperture very small and use high exposure times. In my own opinion this is useless because real world situations have to deal with such things anyway.</p>\n",
        "system": ""
    },
    {
        "instruction": "Find coordinate of eye corner using difference in color",
        "input": "",
        "output": "<p>I appreciate your aim at simplicity. White object color does not always result in white pixel and non-white color can turn into a white pixel due to specular reflections or overexposure. Your eye corner, by the way, has gray intensity 209, not 255. And very close to this intensity (at the value 200) there are some other pixels as shown in the image below. This means that relying on pixel intensities probably doesn't take you too far.</p>\n\n<p><img src=\"https://i.sstatic.net/t534a.jpg\" alt=\"enter image description here\"></p>\n\n<p>You mentioned a \"kind of difference between the skin color and the first white pixel at the corner of the eye\" but you did not measure the difference - you checked just one of the values. In fact you can check as many differences as you want, as in the filter below. Comparing differences may be a good approach that you can improve but looking at the differences in specific locations.</p>\n\n<p>Just convolve your image with some pattern filter using <a href=\"http://docs.opencv.org/modules/imgproc/doc/object_detection.html?highlight=matchtemplate#cv.MatchTemplate\" rel=\"nofollow noreferrer\">matchTemplate()</a> to find good areas for further analysis. For example, after convolution of your gray blurred image with the filter below (consists of 3 horizontal stripes: white, dark, white), I got this result where you can locate a vertical position of the eyes and even the mouth easily. To get more filters configurations look at this paper early <a href=\"http://students.washington.edu/srke/cv_finalProject.htm\" rel=\"nofollow noreferrer\">Ada boost paper for face detection.</a></p>\n\n<p><img src=\"https://i.sstatic.net/XpUr1.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/eSWe6.jpg\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/RIDAJ.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting triangles in OpenCV Approxpoly",
        "input": "",
        "output": "<p>It's a bit fiddly because of the slightly different types that the contour detection returns and what approxPolyDP is expecting. Have a look at this function that I developed to do pretty much what you are looking for: </p>\n\n<pre><code>public static boolean isContourSquare(MatOfPoint thisContour) {\n\n    Rect ret = null;\n\n    MatOfPoint2f thisContour2f = new MatOfPoint2f();\n    MatOfPoint approxContour = new MatOfPoint();\n    MatOfPoint2f approxContour2f = new MatOfPoint2f();\n\n    thisContour.convertTo(thisContour2f, CvType.CV_32FC2);\n\n    Imgproc.approxPolyDP(thisContour2f, approxContour2f, 2, true);\n\n    approxContour2f.convertTo(approxContour, CvType.CV_32S);\n\n    if (approxContour.size().height == 4) {\n        ret = Imgproc.boundingRect(approxContour);\n    }\n\n    return (ret != null);\n}\n</code></pre>\n\n<p>Ok, so to use this function in your code I would use something like this:</p>\n\n<pre><code>public static List&lt;MatOfPoint&gt; getSquareContours(List&lt;MatOfPoint&gt; contours) {\n\n    List&lt;MatOfPoint&gt; squares = null;\n\n    for (MatOfPoint c : contours) {\n\n        if ((ContourUtils.isContourSquare(c)) {\n\n            if (squares == null)\n                squares = new ArrayList&lt;MatOfPoint&gt;();\n            squares.add(c);\n        }\n    }\n\n    return squares;\n}\n</code></pre>\n\n<p>so in your code, after:</p>\n\n<pre><code> Imgproc.findContours(mDilatedMask, contours, mHierarchy, Imgproc.RETR_EXTERNAL, Imgproc.CHAIN_APPROX_SIMPLE);\n</code></pre>\n\n<p>you can make a call like this:</p>\n\n<pre><code>List&lt;MatOfPoint&gt; squareContours = getSquareContours(contours);\n</code></pre>\n\n<p>squareContours will then just have the square contours (or triangular if as you say, you use the value 3 when checking approxContour.size().height)</p>\n\n<p>you can then continue with the remaining code as follows:</p>\n\n<pre><code>// Filter contours by area and resize to fit the original image size\n    mContours.clear();\n    each = squareContours.iterator();\n\n    while (each.hasNext()) \n    {\n        MatOfPoint contour = each.next();\n        if (Imgproc.contourArea(contour) &gt; mMinContourArea*maxArea) \n        {\n            Core.multiply(contour, new Scalar(4,4), contour);\n            mContours.add(contour);\n        }\n    }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Releasing Mat images with openCV(.release() and =Mat() doesn&#39;t work)",
        "input": "",
        "output": "<p>waitKey(-1) wait for a keyboard input while waitKey(10) uses 10 ms to update UI and then passes to control further on; It seems to that your problem is not release but proper update.</p>\n\n<p>Normally one don\u2019t need to release Mat (though I saw some release() calls in Java), just reassign it. Your problem may also be related to smart pointers that got killed when the number of references goes to zero but it is hard to say without seeing your code. </p>\n\n<p>There are some tricky things happening when creating, assigning, and declaring Mats. Also, note for example that Mat m = n, copies the reference only but Mat m = n + 0, copies the whole object. </p>\n",
        "system": ""
    },
    {
        "instruction": "Algorithms regarding detection of object",
        "input": "",
        "output": "<p>SURF/SIFT are spatial local features. If the dataset is large enough the result should be good. Even for different vehicles only specific structures of that vehicle are available in a scene image. </p>\n\n<p>However false positives might creep in if similar non-vehicle structure is present. (E.g distorted image of a small rectangular house with fence). So, some global feature like road detection might increase accuracy.</p>\n\n<p>So, i think sirf/surf features of vehicles with <a href=\"http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/\" rel=\"nofollow\">single class SVM</a> should help if the false positives are not present in the image of your application.</p>\n",
        "system": ""
    },
    {
        "instruction": "Cannot compile program-exits with code 1",
        "input": "",
        "output": "<pre><code>Cannot compile program-exits with code 1\n</code></pre>\n\n<p>This is a wrong statement. Your program compiles successfully. But when you launch it executes and exits with the code 1. You need to put breakpoints and go through the program to see where it exits.</p>\n",
        "system": ""
    },
    {
        "instruction": "Align images in opencv",
        "input": "",
        "output": "<p>First you have determine the transformation between your images. It can be as simple as shift or as complex as non-linear warping. Focusing operation can not only shift but also slightly scale and even shear the images. In this case you have to use Homography to match them. </p>\n\n<p>Second, a simplest way that you have to try is to manually select at least 4 paris of corresponding points, record their coordinates and feed them into <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=find%20homography#cv.FindHomography\">findHomography()</a> function. The resulting 3x3 transformation can be used in <a href=\"http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html#void%20warpPerspective%28InputArray%20src,%20OutputArray%20dst,%20InputArray%20M,%20Size%20dsize,%20int%20flags,%20int%20borderMode,%20const%20Scalar&amp;%20borderValue%29\">warpPerspective()</a> to match the images. If the outcome is good you can automate the process of finding correspondences by using, say, SURF points and matching their descriptors. </p>\n\n<p>Finally, if the result is unsatisfactory you have to have a more general transformation than homography. I would try a piece-wise Affine trying to match pieces of images separately. Anyway, more info about your input and a final goal will help to solve the problem properly.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to determine the width of the lines?",
        "input": "",
        "output": "<p>You can adapt <strong>barcode reader</strong> algorithms which is the faster way to do it.</p>\n\n<p><img src=\"https://www.olivier-augereau.com/images/s.png\" alt=\"image\"></p>\n\n<p>Scan horizontal and vertical lines.\nLets X the length of the horizontal intersection with black line an Y the length of the vertical intersection (you can have it be calculating the median value of several X and Y if there are some noise).</p>\n\n<pre><code>X * Y / 2 = area\nX\u00b2+Y\u00b2 = hypotenuse\u00b2\nhypotenuse * width / 2 = area\n</code></pre>\n\n<p>So : width = 2 * area / hypotenuse</p>\n\n<p>EDIT : You can also easily find the angle by using PCA.</p>\n",
        "system": ""
    },
    {
        "instruction": "Comparing focus of 2 image",
        "input": "",
        "output": "<p>I have tried to process the images a bit and I saw some progress. THis is what I did using opencv:</p>\n\n<ul>\n<li>converted images to gray using cvtColor(I, Mgrey, CV_RGB2GRAY);  </li>\n<li>downsampled/decimated them a bit since they are huge (several Mb)</li>\n<li>Took the sum of absolute horizontal and vertical gradients using <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=sobel#cv.Sobel\" rel=\"nofollow noreferrer\">http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=sobel#cv.Sobel</a>.\nThe result is below. The foreground when in focus does look brighter than background and vice versa.    </li>\n</ul>\n\n<p><img src=\"https://i.sstatic.net/njPrl.jpg\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.sstatic.net/hcl00.jpg\" alt=\"\"></p>\n\n<p>You can probably try to match and subtract these images using translation from <a href=\"http://docs.opencv.org/modules/imgproc/doc/object_detection.html?highlight=matchtemplate#cv.MatchTemplate\" rel=\"nofollow noreferrer\">matchTemplate()</a> on the original gray images; and then assemble pieces using the convex hull of the results as initialization mask for <a href=\"http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=grab%20cut#cv2.grabCut\" rel=\"nofollow noreferrer\">grab cut</a> and plugging in color images. In case you aren\u2019t familiar with the grab cut, chceck out my answer to this <a href=\"https://stackoverflow.com/a/22053017/457687\">question</a>.</p>\n\n<p>But may be a simpler method will work here as well. You can try to apply a strong <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=blur#cv2.blur\" rel=\"nofollow noreferrer\">blur</a> to your gradient images instead of precise matching and see what the difference give you in this case. The images below demonstrate the idea when I turned the difference in the binary masks.</p>\n\n<p><img src=\"https://i.sstatic.net/ncjen.png\" alt=\"enter image description here\">\n<img src=\"https://i.sstatic.net/34ufQ.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Edge Detection method better than Canny Edge detection",
        "input": "",
        "output": "<p>There are different types of \"edges\", it depends on your task. Have a look at the recent paper \"<a href=\"http://www.cs.cmu.edu/~aayushb/pubs/edges.pdf\" rel=\"noreferrer\">Which edges matters?</a>\" from ICCV-2013, with comparison of several methods:</p>\n\n<ul>\n<li><a href=\"http://www.cs.berkeley.edu/~arbelaez/publications/amfm_pami2011.pdf\" rel=\"noreferrer\">ultrametric contour map</a> - \"Contour Detection and Hierarchical Image Segmentation\" by P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik - best results in comparison above.</li>\n<li><a href=\"http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf\" rel=\"noreferrer\">normalized cuts</a> - \"Normalized cuts and image segmentation\" by J. Shi and J. Malik.</li>\n<li><a href=\"https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf\" rel=\"noreferrer\">mean shift</a> - \"Mean shift: A robust approach\ntoward feature space analysis\" by D. Comanicu and P. Meer.</li>\n<li><a href=\"http://www.cs.cornell.edu/~dph/papers/seg-ijcv.pdf\" rel=\"noreferrer\">Felzenszwalb and Huttenlocher approach</a> - \"Efficient graph-based\nimage segmentation\" by Felzenszwalb and Huttenlocher.</li>\n<li><a href=\"http://research.microsoft.com/en-us/um/people/larryz/bice_eccv10.pdf\" rel=\"noreferrer\">BiCE</a> - \"Binary coherent edge descriptors\" by C. L. Zitnick.</li>\n<li><a href=\"http://sites.skoltech.ru/compvision/projects/n4/\" rel=\"noreferrer\">N4-Fields</a> - \"N4-Fields: Neural Network Nearest Neighbor Fields for Image Transforms\" by Ganin et.al </li>\n<li><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Liu_Learning_Relaxed_Deep_CVPR_2016_paper.html\" rel=\"noreferrer\">RDS</a> - \"Learning relaxed deep supervision for better edge detection\" by Liu and Lew</li>\n<li><a href=\"http://www.vision.ee.ethz.ch/~cvlsegmentation/cob/\" rel=\"noreferrer\">COB</a> - \"Convolutional Oriented Boundaries\" by Maninis et.al.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "How to create perspective projection matrix, given focal points and camera principal center",
        "input": "",
        "output": "<p>You can check the following links</p>\n\n<p><a href=\"http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/\" rel=\"noreferrer\" title=\"Kyle Simek&#39;s explanation\">Kyle Simek's explanation</a> </p>\n\n<p><a href=\"http://kgeorge.github.io/2014/03/08/calculating-opengl-perspective-matrix-from-opencv-intrinsic-matrix\" rel=\"noreferrer\">My explanation</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How do I change camera orientation in Sketchup with 3x4 2D projection(RT) matrix with Sketchup Ruby API?",
        "input": "",
        "output": "<p>Lets take the upper 3x3 of the RT matrix, you get-</p>\n\n<p>RT=</p>\n\n<p>R11 R12 R13 T1</p>\n\n<p>R21 R22 R23 T2</p>\n\n<p>R31 R32 R33 T3</p>\n\n<p>R=\nR11 R12 R13\nR21 R22 R23\nR31 R32 R33</p>\n\n<p>T= T1 T2 T3</p>\n\n<p>yaxis=R21 R22 R23</p>\n\n<p>zaxis=R31 R32 R33</p>\n\n<p>You can change SU's camera with eye, target and up.</p>\n\n<p>eye=-R(transpose)*T</p>\n\n<p>target= eye+zaxis</p>\n\n<p>up=-yaxis</p>\n",
        "system": ""
    },
    {
        "instruction": "How to determine the optimal threshold for Chi-square statistic dissimilarity measure in LBP face recognition?",
        "input": "",
        "output": "<p>In the same/not-same setting, you learn the optimal threshold from the training set. Given, say 1000 same and 1000 not same pairs for training, run a for loop on the threshold. For each threshold value, calculate the precision as 0.5 * (percent of same pairs with distance &lt; current threshold) + 0.5 * (percent of not same pairs with distance >= currentThreshold). Then, keep track of the optimal threshold. </p>\n\n<p>By the way, for same/not-same setting, I would recommend considering using one-shot-similarity</p>\n",
        "system": ""
    },
    {
        "instruction": "classification by multiple samples",
        "input": "",
        "output": "<p>I would imagine that brick pattern and concrete are pretty different visually so using SVM may be an overkill. There are many other machine learning methods such as, for example, a Kmeans. They may be simpler but even so may work better. </p>\n\n<p>May be just plotting your feature vectors for two classes in  multidemsional space would create two distinct clouds so measuring distance from the centers of these clouds (normalized by standard deviation) will give you the answer and more clear understanding of what's going on.</p>\n\n<p>One the difference between Kmeans and SVM is that <a href=\"http://en.wikipedia.org/wiki/K-means_clustering\" rel=\"nofollow\">Kmeans</a> builds a prototype of the object while  SVM collects some exemplar that aren't even representative but just lie close to the border between two classes. OpenCV has the implementaiton of <a href=\"http://docs.opencv.org/modules/core/doc/clustering.html?highlight=kmeans#cv2.kmeans\" rel=\"nofollow\">Kmeans</a> </p>\n\n<p>Performing PCA on the feature difference may give you the most distinct dimensions. This can simplify your algorithm and feature extraction. </p>\n",
        "system": ""
    },
    {
        "instruction": "How do I find the connected components in a binary image?",
        "input": "",
        "output": "<p>Below are a simple code (C++) using simple dfs to mark different component, you may try it out.</p>\n\n<p>For example, if your stdin input is</p>\n\n<pre><code>4 5\n0 0 0 0 1\n0 1 1 0 1\n0 0 1 0 0\n1 0 0 0 1\n</code></pre>\n\n<p>Then the output should be</p>\n\n<pre><code>Graph:\n0 0 0 0 1 \n0 1 1 0 1 \n0 0 1 0 0 \n1 0 0 0 1 \n\nOutput:\n0 0 0 0 1 \n0 2 2 0 1 \n0 0 2 0 0 \n3 0 0 0 4\n</code></pre>\n\n<p>The same number represent that cell belongs to the same component.</p>\n\n<p>I am assuming all 8-directions belongs to the same component, if you only want 4-directions,\nchange dx[] and dy[]</p>\n\n<p>Also I am assuming the input is at most 200*200, and I did something to avoid handling those annoying array outbound issues, you may check it out :)</p>\n\n<pre><code>#include&lt;cstdio&gt;\n#include&lt;cstdlib&gt;\n#include&lt;cstring&gt;\n\nint g[202][202] = {0};\nint w[202][202] = {0};\n\nint dx[8] = {-1,0,1,1,1,0,-1,-1};\nint dy[8] = {1,1,1,0,-1,-1,-1,0};\n\nvoid dfs(int x,int y,int c){\n    w[x][y] = c;\n    for(int i=0; i&lt;8;i++){\n        int nx = x+dx[i], ny = y+dy[i];\n        if(g[nx][ny] &amp;&amp; !w[nx][ny]) dfs(nx,ny,c);\n    }\n}\n\nint main(){\n    int row, col, set = 1;\n    scanf(\"%d%d\", &amp;row, &amp;col);\n\n    for(int i=1; i&lt;=row; i++) for(int j=1; j&lt;=col; j++) scanf(\"%d\", &amp;g[i][j]);\n\n    for(int i=1; i&lt;=row;i++)\n        for(int j=1; j&lt;=col; j++)\n            if(g[i][j] &amp;&amp; !w[i][j])\n                dfs(i,j,set++);\n\n    printf(\"Graph:\\n\");\n    for(int i=1; i&lt;=row;i++){\n        for(int j=1; j&lt;=col;j++)\n            printf(\"%d \", g[i][j]);\n        puts(\"\");\n    }\n    puts(\"\");\n    printf(\"Output:\\n\");\n    for(int i=1; i&lt;=row;i++){\n        for(int j=1; j&lt;=col;j++)\n            printf(\"%d \", w[i][j]);\n        puts(\"\");\n    }\n\n    return 0;\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Finding 3D coordinate when all 3 coordinates can vary in the object coordinate system",
        "input": "",
        "output": "<p>The variable <strong><em>s</em></strong> has a specific meaning: there is one-to-many correspondences between a 2D point and its 3D back-projection. In other words, there is an infinite number of possible 3D points lying on a ray that is eventually terminates in or is emitted from a pixle in the direction u, v, f. This is what s is about: just an indicator of a one-to-many relationship. </p>\n\n<p>It seems that Francesco talks about a general case of structure from motion when a metric reconstruction is ambiguous up to scale. The question however is probably quite different. Let me rephrase it and tell me if I got it right: you have a static object coordinate system which you know. You have a target that rotates in this system around X axis and you know 3d coordinates of 4 points in this system at zero rotation. To get new 3D coordinates after rotation all you need is a rotation angle while you are given a set of 2D projections of your known points. This is an EASY task; if it is what you are really after. </p>\n\n<p>Why the task is easy? Every point generates two constraints as in u= v=; the number of unknown is one - the angle so one point is enough to calculate it. Knowing this angle you can rotate your known 3D points to update their coordinates. Overall only 1 point is enough to solve the task:</p>\n\n<ol>\n<li>Multiply both sides of a pin-hole camera equation with an inverse of an intrinsic matrix from the left to get rid of intrinsic parameters. You will end up with this: \ns\u2019 [u\u2019 v\u2019 1]<sup>T</sup> = A [X Y Z]<sup>T</sup> + t, where A=R*Ralpha</li>\n<li><p>Technically Ralpha - our unknown - depends on angle alpha only but since the dependence is non linear we can use a linear multiplication by a matrix with 2 entries: s = sin(alpha) and c = cos(alpha), alpha - angle for rotation around x axis</p>\n\n<pre><code>          1  0   0\nRalpha =  0  c  -s\n          0  s   c\n</code></pre></li>\n<li><p>Get rid of s\u2019 by noting that s' =  a<sub>31</sub>X + a<sub>32</sub>Y + a<sub>32</sub>Z + tz\nand plugging it in the two constraints:</p>\n\n<p>s\u2019u\u2019 = (a<sub>31</sub>X + a<sub>32</sub>Y + a<sub>32</sub>Z + t<sub>z</sub>)u\u2019 = a<sub>11</sub>X + a<sub>12</sub>Y + a<sub>13</sub>Z + t<sub>x</sub></p>\n\n<p>s\u2019v\u2019 = (a<sub>31</sub>X + a<sub>32</sub>Y + a<sub>32</sub>Z + t<sub>z</sub>)v\u2019 = a<sub>21</sub>X + a<sub>22</sub>Y + a<sub>23</sub>Z + t<sub>y</sub></p></li>\n</ol>\n\n<p>Finding matrix A is now a simple task of solving a linear system of equations Kx=b, where by rearranging terms we have</p>\n\n<p>b = [t<sub>x</sub>-t<sub>z</sub>u\u2019, t<sub>y</sub>-t<sub>z</sub>v\u2019]<sup>T</sup>, </p>\n\n<p>x = [a<sub>11</sub>, a<sub>12</sub>, a<sub>13</sub>, a<sub>21</sub>, a<sub>22</sub>, a<sub>23</sub>, a<sub>31</sub>, a<sub>32</sub>, a<sub>33</sub>]<sup>T</sup> </p>\n\n<p>for a single point correspondence K is</p>\n\n<pre><code>-X, -Y, -Z, 0,   0,  0, Xu\u2019, Yu\u2019, Zu\u2019\n 0,  0,  0, -X, -Y, -Z, Xv\u2019, Yv\u2019, Zv\u2019\n</code></pre>\n\n<p>But one can add more rows if there are more correspondences.\nSolving this with pseudo inverse gives x = (K<sup>T</sup>K)<sup>-1</sup>K<sup>T</sup>b,\nwhich can be further optimized via non-linear minimization of quadratic residuals.</p>\n\n<p>After you calculated x and used it to reassemble A, you have to make sure that it is a true rotation matrix. Normally this is done through SVD: A=ULV<sup>T</sup> and then reassigning A=UV<sup>T</sup>. Finally, get Ralpha = R<sup>T</sup>A, which gives you a rotation matrix that you can apply to your known 3D coordinates to get their new values in the object coordinate system or use the whole matrix A to get them in the camera coordinate system.</p>\n\n<p>This may look messy but it is a typical set of steps for getting, say, extrinsic camera parameters and you have already done this (though you probably used a library function).</p>\n",
        "system": ""
    },
    {
        "instruction": "How do I test image recognition algorithms?",
        "input": "",
        "output": "<p>I'll try to answer:</p>\n\n<ol>\n<li><p>Compute ROC curves. </p></li>\n<li><p>It will be far more realistic and challenging to feed your algorithm with real work, un-cropped images. But for the ground truth, you would have to know the location of the images in the image. </p></li>\n<li><p>you can use \"labelMe video\" for labeling and separate the video into images to feed them to the algorithm.</p></li>\n</ol>\n\n<p>Hope that help, comment if you have any further questions. </p>\n",
        "system": ""
    },
    {
        "instruction": "Transformation Concept in OpenCV",
        "input": "",
        "output": "<p>1) It is not a question about OpenCV but rather about mathematics. Applying affine transformation to point (x,y) means the following:</p>\n\n<pre><code>x_new = a*x + b*y + c;\ny_new = d*x + e*y + f;\n</code></pre>\n\n<p>And so affine transform has 6 degrees of freedom: a, b, c, d, e, f. They are stored in 2x3 matrix: a, b, c in the first row, and d, e, f in the second row. You can apply transform to a point by multiplying of matrix and vector.</p>\n\n<p>Perspective transform of (x,y) would be:</p>\n\n<pre><code>z = g*x + h*y + 1;\nx_new = (a*x + b*y + c)/z;\ny_new = (d*x + e*y + f)/z;\n</code></pre>\n\n<p>As you can see it has 8 degrees of freedom that are stored in 3x3 matrix. Third row is g, h, 1.</p>\n\n<p>See also <a href=\"http://en.wikipedia.org/wiki/Homogeneous_coordinates\" rel=\"nofollow noreferrer\">homogeneous coordinates</a> for more information about why this representation is so convenient.</p>\n\n<p>2) Affine transformation is also called 'weak perspective' transformation: if you are looking at some scene from different perspective but size of the scene is small relatively to distance to the camera (i.e. parallel lines remain more or less parallel), than you may use affine transform. Otherwise perspective transform will be required.</p>\n",
        "system": ""
    },
    {
        "instruction": "Alternative for Threshold in opencv",
        "input": "",
        "output": "<p>Have you tried an <a href=\"http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=adaptive#cv.AdaptiveThreshold\" rel=\"nofollow noreferrer\">adaptive threshold</a>? A single value of threshold rarely works in real life application. Another truism - threshold is a non-linear operation and hence non-stable. Gradient on the other hand is linear so you may want to find a contour by tracking the gradient if your background is smooth and solid color. Gradient is also more reliable during illumination changes or shadows than thresholding. </p>\n\n<p>Grab-cut, by the way, uses color information to improve segmentation on the boundary when you <em>already</em> found 90% or so of the segment, so it is a <em>post processing</em> step. Also your initialization of grab cut with rectangle lets in a lot of contamination from background colors. Instead of rectangle use a mask where you mark as GC_FGD deep inside your initial segment where you are sure the hand is; mark as GC_BGD far outside your segment where you sure background is; mark GC_PR_FGD or probably foreground everywhere else - this is what will be refined by <a href=\"http://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=grabcut#cv2.grabCut\" rel=\"nofollow noreferrer\">grab cut</a>. to sum up - your initialization of grab cut will look like a russian doll with three layers indicating foreground (gray), probably foreground (white) and background (balck). You can use <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=dilate#cv.Dilate\" rel=\"nofollow noreferrer\">dilate and erode</a> to create these layers, see below</p>\n\n<p><img src=\"https://i.sstatic.net/40Uga.jpg\" alt=\"enter image description here\"> </p>\n\n<p>Overall my suggestion is to define what you want to do first. Are you looking for contours of arbitrary objects on arbitrary moving background? If you are looking for a contour of a hand to find fingers on relatively uniform background I would:\n1. <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=contour#cv.FindContours\" rel=\"nofollow noreferrer\">use connected components</a> or <a href=\"http://docs.opencv.org/modules/features2d/doc/feature_detection_and_description.html?highlight=mser#MSER%20:%20public%20FeatureDetector\" rel=\"nofollow noreferrer\">MSER</a> to segment out a hand. Possibly improve results with grab cut initialized with the conservative mask and not rectangle!\n2. use <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=convexity#cv.ConvexityDefects\" rel=\"nofollow noreferrer\">convexity defects</a> to find fingers if this is your goal;</p>\n",
        "system": ""
    },
    {
        "instruction": "Find relative position between 2 cameras",
        "input": "",
        "output": "<p>Use stereoCalibrate. Your setup is exactly like a stereo camera.</p>\n",
        "system": ""
    },
    {
        "instruction": "How can we extract region bound by a contour in OpenCV?",
        "input": "",
        "output": "<p>I would:</p>\n\n<ol>\n<li><p>Use <code>contourArea()</code> to find the largest closed contour.</p></li>\n<li><p>Use <code>boundingRect()</code> to get the bounds of that contour.</p></li>\n<li><p>Draw the contour using <code>drawContours()</code> (with thickness set to -1 to\nfill the contour) and use this as a mask.</p></li>\n<li><p>Use use the mask to set all pixels in the original image not in the\nROI to (0,0,0).</p></li>\n<li><p>Use the bounding rectangle to extract just that area from the\noriginal image.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Can I estimate a camera pose/extrinsic parameters using known object sizes instead of a plane?",
        "input": "",
        "output": "<p>One thing you can do if objects that stand on the plane and have equal height is to determine vanishing points or in other words the points where parallel lines converge at the horizon (Infinity). Why is this useful? Consider a projection matrix P, intrinsic matrix A(hopefully known) and R|T matrix that you are looking for. </p>\n\n<p>P = A*R|T, and R|T = A<sup>-1</sup>P</p>\n\n<p>From vanishing points at infinity you can determine P. The points at infinity has a property that their last homogeneous coordinate is zero. For example p=[1, 0, 0, 0]<sup>T</sup> is a vanishing point in x direction so when you cast it into Cartesian coordinates you'll get [1/0, 0/0, 0/0] = [Inf, 0, 0]. Now if you multiply a 3x4 projection matrix with this point on the right you will get a first column of P. By the same coin finding vanishing points in y and z directions will give you a second and a third columns of P. The last column is the position of your camera center P*[0 0 0 1]T</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV SURFDetector Range of minHessian",
        "input": "",
        "output": "<p>I have the same question regarding an upper bound on the [determinant of the] Hessian, and I haven't found an answer from the theoretical point of view. </p>\n\n<p>However, for your particular purpose, I would recommend you to go to the source code of the SURF detector, in particular, find the part for SURFFindInvoker::findMaximaInLayer and you will see that keypoint detections are accepted/rejected through this conditional: if( val0 > hessianThreshold ). So, i think you could insert a couple of lines of code just before the IF statement in order to store the maximum of val0 and this will tell you the maximum of the Hessian for that particular image. Notice that you must make sure that the maximum is stored across all layers and octaves, otherwise you'll be finding a local maximum only. </p>\n\n<p>Of course you may think of something much more elegant, but basically that's where you'll find the maximum directly. What you have implemented is an indirect way of finding out the value.</p>\n\n<p>Hope this is helpful, Lood Guck!      </p>\n",
        "system": ""
    },
    {
        "instruction": "Why the mat type is different?",
        "input": "",
        "output": "<p>I run your code with a simple image and it is working perfectly fine.. Here is the code and the result. I assume <strong>original_pre</strong> and <strong>original</strong> are of <strong>IplImage*</strong> type as they are not present in you block of code. </p>\n\n<pre><code>IplImage*   original_pre = cvLoadImage(\"G:\\\\test.png\");\nint border = 5;\n    CvMat* original_mat = cvCreateMat(original_pre-&gt;height-2*border, original_pre-&gt;width-2*border, CV_8UC3);\nIplImage* original = cvCreateImage(cvSize(original_pre-&gt;width-2*border, original_pre-&gt;height-2*border), original_pre-&gt;depth, original_pre-&gt;nChannels);\ncvGetSubRect(original_pre, original_mat, cvRect(border,border,original_pre-&gt;width- 2*border,original_pre-&gt;height-2*border));\n\ncvFloodFill(original_mat,cvPoint(50,55),cvScalarAll(255),cvScalarAll(0),cvScalarAll(0));\nCvMat* original_open_mat = cvCreateMat(original_pre-&gt;height-2*border, original_pre-&gt;width-2*border, CV_8UC3);\nCvMat* temp = cvCreateMat(original_pre-&gt;height-2*border, original_pre-&gt;width-2*border,  CV_8UC3);\ncout&lt;&lt;original_mat-&gt;width&lt;&lt;\" \"&lt;&lt;original_mat-&gt;height&lt;&lt;\" \"&lt;&lt;original_open_mat-&gt;width&lt;&lt;\" \"&lt;&lt;original_open_mat-&gt;height&lt;&lt;\" \"&lt;&lt;CV_MAT_TYPE(original_mat-&gt;type)&lt;&lt;\" \"&lt;&lt;CV_MAT_TYPE(original_open_mat-&gt;type)&lt;&lt;\" \"&lt;&lt;CV_8UC3&lt;&lt;endl;\ncvMorphologyEx(original_mat,original_open_mat,temp,NULL,CV_MOP_OPEN);\ncvSaveImage(\"G:\\\\temp_.jpg\",original_open_mat);\n</code></pre>\n\n<p><strong>Original Image</strong></p>\n\n<p><img src=\"https://i.sstatic.net/Ic4kI.png\" alt=\"Original Image\"></p>\n\n<p><strong>Result</strong></p>\n\n<p><img src=\"https://i.sstatic.net/Xmbj7.jpg\" alt=\"Result\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Code not detecting mouth in opencv and c++",
        "input": "",
        "output": "<p>What I could understand from your code, is your pt1 and pt2 are not initialized correctly in mouth detection.</p>\n\n<p>they must be</p>\n\n<pre><code>Point pt1(faces[i].x + mouths[i].x, faces[i].y + mouths[i].y);\nPoint pt2(pt1.x + mouths[i].width, pt1.y + mouths[i].height);\n</code></pre>\n\n<p>Moreover, I don't know where you gonna use your code, but, it looks too long for just face and mouth, I am referring to the codes which you shared in previous questions. Incase, if you arent getting the output yet, just let me know, I am free to share my old code snippet on this thing.</p>\n\n<p>It gave me results.Dont forget to add xml files into project directory.</p>\n\n<pre><code>#include \"stdafx.h\"\n#include \"opencv2\\highgui\\highgui.hpp\"\n#include \"opencv2\\objdetect\\objdetect.hpp\"\n#include \"opencv2\\imgproc\\imgproc.hpp\"\n\nusing namespace cv;\n\nint main(int argc, const char** argv)\n{\n  VideoCapture cap(0);  \n  CascadeClassifier face, mouth;\n  face.load(\"haarcascade_frontalface_default.xml\");\n  mouth.load(\"haarcascade_mcs_mouth.xml\");\n  Mat frame, grayframe, testframe;\n\n  while(1)\n  {\n    cap.read(frame);\n    if(!cap.read(frame))\n    {\n        printf(\"an error while taking the frame from cap\");\n    }\n    vector&lt; Rect &gt; faces;\n    cvtColor(frame,grayframe, CV_BGR2GRAY);\n    equalizeHist(grayframe,testframe);\n    face.detectMultiScale(testframe, faces,1.1,3,   CV_HAAR_SCALE_IMAGE,Size(30,30));\n    for(int i=0;i&lt;faces.size();i++)\n    {\n        rectangle(frame,faces[i],Scalar(255,0,0),1,8,0);\n        Mat face  = frame(faces[i]);\n        cvtColor(face,face,CV_BGR2GRAY);\n        vector &lt;Rect&gt; mouthi;\n        mouth.detectMultiScale(face,mouthi);\n        for(int k=0;k&lt;mouthi.size();k++)\n        {\n        Point pt1(mouthi[0].x+faces[i].x , mouthi[0].y+faces[i].y);\n        Point pt2(pt1.x+mouthi[0].width, pt1.y+mouthi[0].height);\n        rectangle(frame, pt1,pt2,Scalar(255,0,0),1,8,0);\n        }\n\n    }\n\n    imshow(\"output\", frame);\n    waitKey(33);\n  }\n  return 0;\n\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Flandmark detector not working in c++",
        "input": "",
        "output": "<p>I am also using the same library to detect facial feature points recently. </p>\n\n<p>You can make it work by following the follwowing steps:</p>\n\n<ol start=\"3\">\n<li><p>Make sure you add the correct <strong><code>OpenCV-Path\\build\\include</code></strong> and <strong><code>OpenCV-Path\\build\\x86\\vc10\\lib</code></strong> (all OpenCV libs you added to your project should be found here) to the project.</p></li>\n<li><p>For <code>liblbp.h</code> file, change</p>\n\n<pre><code>#include \"msvc-compat.h\"\n</code></pre>\n\n<p>to</p>\n\n<pre><code>#include &lt;stdint.h&gt;\n</code></pre></li>\n<li><p>For <code>flandmark_detector.h</code> file, change</p>\n\n<pre><code>#include \"msvc-compat.h\"\n#include &lt;cv.h&gt;\n#include &lt;cvaux.h&gt;\n</code></pre>\n\n<p>to</p>\n\n<pre><code>#include &lt;stdint.h&gt;\n\n#include \"opencv2/opencv.hpp\"\n#include \"opencv2/objdetect/objdetect.hpp\"\n#include \"opencv2/highgui/highgui.hpp\"\n#include \"opencv2/imgproc/imgproc.hpp\"\n\n#include &lt;iostream&gt;\n#include &lt;stdio.h&gt;\n\nusing namespace std;\nusing namespace cv;\n</code></pre></li>\n</ol>\n\n<hr>\n\n<p><strong><em>You can download the workable VS10 project of this library <a href=\"https://bitbucket.org/herohuyongtao/files/downloads/flandmark-vs10-new.zip\" rel=\"nofollow noreferrer\">here</a>.</em></strong></p>\n\n<p>Note: I also added the OpenCV library in the project so that it should run well without any extra setting.</p>\n\n<p>After running this, you should achieve something like this:</p>\n\n<p><img src=\"https://i.sstatic.net/1IN4p.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Kinect: How do i ID the first tracked skeleton and do stuff with it after",
        "input": "",
        "output": "<p>Kinect track player and set ID in diffrent index of Skeletons array (length == 6) every time when it detects the new one. That's why is necessary to save Player's ID when number of index with tracking id equal zero is 5.  </p>\n\n<pre><code>    int skeletonId = 0;\n    int trackId = 0;\n\n\n    void myKinect_SkeletonFrameReady(object sender, SkeletonFrameReadyEventArgs e)\n    {\n        Skeleton[] skeletons = null;\n\n        using (SkeletonFrame frame = e.OpenSkeletonFrame())\n        {\n            if (frame != null)\n            {\n                skeletons = new Skeleton[frame.SkeletonArrayLength];\n                frame.CopySkeletonDataTo(skeletons);\n            }\n        }\n\n        if (skeletons == null) return;\n\n        //variable for count of null ID\n        int skeletonsNull = 0;\n        //counting...\n        foreach (Skeleton skeletonText in skeletons)\n        {\n            if (skeletonText.TrackingId == 0)\n            {\n                skeletonsNull++;\n            }\n            else\n            {\n                skeletonId = skeletonText.TrackingId;\n            }\n        }\n\n\n        if (skeletonsNull == 5)\n        {\n            trackId = skeletonId;\n        }\n\n\n        foreach (Skeleton skeleton in skeletons)\n        {\n            if (skeleton.TrackingId == trackId)\n            {\n             //do something   \n            }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Why i cannot getting correct Fourier transformed image using matlab?",
        "input": "",
        "output": "<p><code>colormap([0,0,0;1,1,1])</code> works as a binary display, use <code>colormap(gray)</code> instead.</p>\n",
        "system": ""
    },
    {
        "instruction": "Taking R,G,B individually from a &lt;cv::vec3b&gt; vector",
        "input": "",
        "output": "<p><code>Vec3b v_char</code> itself is a vector of 3 values which can be accessed as <code>first values = v_char.val[0]</code>, <code>second values = v_char.val[1]</code>, <code>third values = v_char.val[2]</code>. As you need only the <code>red</code> values so, just access all the values in  <code>v_char.val[0]</code> for the whole vector.</p>\n\n<p>so through a <code>for loop</code> try to get all the values of:</p>\n\n<pre><code>v_char[i].val[0]; // \"i\" represents the index of \"for loop\" for a given row\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "What does an image of Fourier Transformation of an image tell us?",
        "input": "",
        "output": "<p>The central part includes the low frequency components. The bright pixels in frequency image (right) indicate strong intensity at the corresponding positions in spatial image (left). Note that there is no one-on-one mapping from the pixel in frequency domain and spatial domain. One frequency pixel consists all the spatial pixels that have the corresponding frequency.  </p>\n\n<p>The low frequency domain corresponds to the smooth areas in the image (such as skin, hat, background, etc). The high frequency domain, which is shown in the image away from the central part, includes those sharp edges or some structure with the changes of intensity dramatically along one orientation (such as the hairs, boundary of hat, etc). But since the intensity for those parts is not high enough compared with the smooth structure, the corresponding regions appear dark on the right image. </p>\n\n<p>Note that the camera lens focuses on Lenna, so the background is blurred. If the focus region is at background, the vertical lines behind Lenna would be clear, and the sharp edges of the lines will contribute to high frequency magnitude, thus region away from the center on the right image would be bright. </p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV python image translation",
        "input": "",
        "output": "<p>I found out there is a way to create custom matrix via cv.Set2D method.\nFor anywho who want to create a custom transformation matrix in opencv python the code is below.</p>\n\n<pre><code>matrix = cv.CreateMat(2, 3, cv.CV_32FC1 )\ncv.Set2D(matrix,0,0,1)\ncv.Set2D(matrix,0,1,0)\ncv.Set2D(matrix,0,2,x)\ncv.Set2D(matrix,1,0,0)\ncv.Set2D(matrix,1,1,1)\ncv.Set2D(matrix,1,2,y)\n</code></pre>\n\n<p>Beware opencv only accepts 3x2 matricies (not 3x3) for cv.WarpAffine method</p>\n",
        "system": ""
    },
    {
        "instruction": "cvReleaseCapture() error",
        "input": "",
        "output": "<ul>\n<li>avoid the outdated c-api (IplImages, Cv*functions)  stick with the c++ api.</li>\n<li>the images you get from a capture point to memory <em>inside</em> the cam-driver. if you don't clone() the image, and release the capture, you got a dangling pointer.</li>\n<li>don't create a new capture for each shot. (cam needs some 'warm-up' time so it'll be slow as hell).\nkeep one instance around in your class instead</li>\n</ul>\n\n<hr>\n\n<pre><code>class CameraTest \n{\n    VideoCapture capture;         // make it a class - member\n    CameraTest () : capture(0)    // capture from video device #0\n    {\n         capture.set(CV_CAP_PROP_FRAME_WIDTH , 800); \n         capture.set(CV_CAP_PROP_FRAME_HEIGHT , 600); \n    }\n\n    // ...\n};\n\nvoid CameraTest ::on_snapButton_clicked()\n{\n    Mat img;                // temp var pointing todriver mem    \n\n    if(!capture.read(img))  //if no webcam detected or failed to capture anything\n    {\n        cout &lt;&lt; \"Could not grab a frame\\n\\7\";\n        exit(0);\n    }\n\n    image = img.clone();      // keep our member var alive\n    cv::imshow(\"Mat\",image);  \n\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Human Detection using edge detection",
        "input": "",
        "output": "<p>Instead of literally subtracting the background, try using the <code>vision.ForegroundDetector</code> object, which is part of the Computer Vision System Toolbox.  It implements the mixture-of-gaussians adaptive background modeling, and it may give you a cleaner segmentation. </p>\n\n<p>Having said that, it is very unlikely that you will get the \"exact\" silhouette. Some error is inevitable.</p>\n",
        "system": ""
    },
    {
        "instruction": "Sum of elements in a matrix in OpenCV?",
        "input": "",
        "output": "<p>Unlike Matlab, in opencv, <code>cv::sum(A)</code> sums along ALL dimensions and returns a single number (scalar) that is equal to Matlab's <code>sum(sum(A))</code>.<br />\nSo, what you need is</p>\n<pre><code>double s = cv::sum(A)[0];\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Using Vlfeat&#39;s C API for quickshift",
        "input": "",
        "output": "<p>I need to use VLFeat's implementation of Quick Shift, as well. The following snippets illustrate how to use the implementation from C++. As I am using OpenCV to read image, first include OpenCV together with the VLFeat header files:</p>\n\n<pre><code>#include &lt;opencv2/opencv.hpp&gt;\n\nextern \"C\" {\n    #include \"generic.h\"\n    #include \"quickshift.h\"\n}\n</code></pre>\n\n<p>After downloading VLFeat (in my case the archive contains the folder <code>vlfeat-0.9.18</code>), I use CMake to add <code>vlfeat-0.9.18/vl</code> as include directory. Otherwise you have to adjust the above code. Then, the following code reads an image, converts the image to the required format and runs Quick Shift.</p>\n\n<p><strong>Note:</strong> The following snippet is only an extract of my original code and, thus, is not tested as it is presented below.</p>\n\n<pre><code>// Read an image using OpenCV, I assume a color image to be given;\n// the image will be loaded in BGR color space.\ncv::Mat mat = cv::imread(\"Lenna.png\", CV_LOAD_IMAGE_COLOR);\n\n// Convert image to one-dimensional array.\ndouble* image = new double[mat.rows*mat.cols*mat.channels()];\nfor (int i = 0; i &lt; mat.rows; ++i) {\n    for (int j = 0; j &lt; mat.cols; ++j) {\n        image[j + mat.cols*i + mat.cols*mat.rows*0] = mat.at&lt;cv::Vec3b&gt;(i, j)[0];\n        image[j + mat.cols*i + mat.cols*mat.rows*1] = mat.at&lt;cv::Vec3b&gt;(i, j)[1];\n        image[j + mat.cols*i + mat.cols*mat.rows*2] = mat.at&lt;cv::Vec3b&gt;(i, j)[2];\n    }\n}\n\n// Create a new quickshift instance using the image, the height and width of the\n// image as well as the number of channels.\nVlQS* quickShift = vl_quickshift_new(image, mat.rows, mat.cols, mat.channels());\nvl_quickshift_set_kernel_size(quickShift, 5);\n\n// Run Quick Shift.\nvl_quickshift_process(quickShift);\n</code></pre>\n\n<p>However, I could not figure out how to interpret and use the output of the implementation, yet.</p>\n",
        "system": ""
    },
    {
        "instruction": "Given a contour outlining the edges of an &#39;S&#39; shape in OpenCV/Python, what methods can be used to trace a curve along the center of the shape?",
        "input": "",
        "output": "<p>Actually vectorizing fonts  isn't trivial problem and quite tricky. To properly vectorize fonts using bezier curve  you'll need tracing. There are many library you can use for tracing image, for example <a href=\"http://potrace.sourceforge.net/\" rel=\"nofollow noreferrer\">Potrace</a>. I'm not knowledgeable using python but based on my experience, I have done similar project using c++ described below:</p>\n<p><strong>A. Fit the contour using cubic bezier</strong></p>\n<p>This method is quite simple although a lot of work should be done. I believe this also works well if you want to fit skeletons obtained from thinning.</p>\n<ol>\n<li>Find contour/edge of the object, you can use OpenCV function <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/find_contours/find_contours.html\" rel=\"nofollow noreferrer\">findContours()</a></li>\n<li>The entire shape can't be represented using a single cubic bezier, so divide them to several segments using <a href=\"http://karthaus.nl/rdp/\" rel=\"nofollow noreferrer\">Ramer-Douglas-Peucker (RDP)</a>. The important thing in this step, don't delete any points, use RDP only to segment the points. See colored segments on image below.</li>\n<li><strong>For each segments</strong>, where <strong>S</strong> is a set of n points <strong>S = (s0, s1,...Sn)</strong>, fit a cubic bezier using <a href=\"https://www.jimherold.com/computer-science/best-fit-bezier-curve\" rel=\"nofollow noreferrer\">Least Square Fitting</a>. You can find a modernized version of the implementation code <a href=\"https://github.com/cubuspl42/LeastSquaresBezier\" rel=\"nofollow noreferrer\">here</a>.</li>\n</ol>\n<p><img src=\"https://i.sstatic.net/TEYxc.png\" alt=\"enter image description here\" /></p>\n<p>Illustration of least square fitting:</p>\n<p><img src=\"https://i.sstatic.net/rLpce.png\" alt=\"enter image description here\" /></p>\n<p><strong>B. Resolution Resolution Independent Curve Rendering</strong></p>\n<p>This method as described in this <a href=\"http://research.microsoft.com/en-us/um/people/cloop/LoopBlinn05.pdf\" rel=\"nofollow noreferrer\">paper</a> is quite complex but one of the best algorithms available to display vector fonts:</p>\n<ol>\n<li>Find contour <em>(the same with method A)</em></li>\n<li>Use RDP, differently from method A, use RDP to remove points so the contour can be simplified.</li>\n<li>Do delaunay triangulation.</li>\n<li>Draw bezier curve on the outer edges using method described in the paper</li>\n</ol>\n<p><img src=\"https://i.sstatic.net/UWn7F.png\" alt=\"enter image description here\" /></p>\n",
        "system": ""
    },
    {
        "instruction": "&quot;Sometimes&quot; getting garbage value for an integer type varibale in openCV",
        "input": "",
        "output": "<p>There is problem in your findMin_x_pointA function. You should compare with &lt; operator to return min value. </p>\n\n<p>The other problem, as you are getting min_x = 149186927 and min_y = 149186937, to solve this problem check values in pt_reader right after you copy points from squares CvSeq. </p>\n",
        "system": ""
    },
    {
        "instruction": "Efficiently implementing erode/dilate",
        "input": "",
        "output": "<p>I have been following this question for some time, hoping someone would write a fleshed-out answer, since I am pondering the same problem. </p>\n\n<p>Here is my own attempt so far; I have not tested this, but I think you can do repeated dilation and erosion with any structuring element, by only accessing each pixel twice:</p>\n\n<p><em>Assumptions: Assume the structuring element/kernel is a KxL rectangle and the image is a NxM rectangle. Assume that K and L are odd.</em></p>\n\n<p>The basic approach you outlined has four for loops and takes <code>O(K*L*N*M)</code> time to complete.</p>\n\n<p>Often you want to dilate repeatedly with the same kernel, so the time is again multiplied by the desired number of dilations.</p>\n\n<p>I have three basic ideas for speeding up the dilation:</p>\n\n<ol>\n<li><p>dilation by a KxL kernel is equal to dilation by a Kx1 kernel followed by dilation by a 1xL kernel. You can do both of these dilations with only three for loops, in O(K<em>N</em>M) and O(L<em>N</em>M)</p></li>\n<li><p>However you can do a dilation with a Kx1 kernel much faster: You only need to access each pixel once. For this you need a particular data structure, explained below. This allows you to do a single dilation in O(N*M), regardless of the kernel size</p></li>\n<li><p>repeated dilation by a Kx1 kernel is equal to a single dilation by a larger kernel. If you dilate P times with a Kx1 kernel, this is equal to a single dilation with a <code>((K-1)*P + 1) x 1</code> kernel. \nSo you can do repeated dilation with any kernel size in a single pass, in O(N*M) time.</p></li>\n</ol>\n\n<hr>\n\n<p>Now for a detailed description of step 2.<br>\nYou need a queue with the following properties: </p>\n\n<ul>\n<li>push an element to the back of the queue in constant time.</li>\n<li>pop an element from the front of the queue in constant time.</li>\n<li>query the current smallest or largest element in the queue in constant time.</li>\n</ul>\n\n<p>How to build such a queue is described in this stackoverflow answer: <a href=\"https://stackoverflow.com/a/4802260/145999\">Implement a queue in which push_rear(), pop_front() and get_min() are all constant time operations</a>. \nUnfortunately not much pseudocode, but the basic idea seems sound.</p>\n\n<p>Using such a queue, you can calculate a Kx1 dilation in a single pass:  </p>\n\n<pre class=\"lang-js prettyprint-override\"><code>Assert(StructuringElement.dy()==1);\nint kernel_half = (StructuringElement.dx()-1) /2;\n\nfor( y &lt; dy ) { // y loop\n\n    for( x &lt;= kernel_half ) { // initialize the queue \n        queue.Push(src(x, y));\n    }\n\n    for( x &lt; dx ) { // x loop\n\n        // get the current maximum of all values in the queue\n         dst(x, y) = queue.GetMaximum();\n\n        // remove the first pixel from the queue\n        if (x &gt; kernel_half)\n            queue.Pop();\n\n        // add the next pixel to the queue\n        if (x &lt; dx - kernel_half)\n            queue.Push(src(x + kernel_half, y));\n    }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Finding the length/area of the object using 2d web cam",
        "input": "",
        "output": "<p>First, it isn't a line.  It is a parcel.  A line is non-physical.  The parcel of pixels has both area and length.  The natural unit of measurement of images is pixels.  Units of length are both non-physical and require assumptions.</p>\n\n<p>Second, you can do a thresholded 2-d convolution.  <a href=\"http://ltcf.tam.uiuc.edu/downloads.html\" rel=\"nofollow\">PIV-sleuth</a> uses 2d convolution.  It can allow some faster, more accurate measurement in images.  Peak intensity will tell you something about the length or area.  You can also use row-sum and column sum very quickly to get ideas of lengths.  It helps if the images are aligned to the pixel-axes in your image. Use of affine transformations can help you test various rotations for suitability.</p>\n",
        "system": ""
    },
    {
        "instruction": "HOG with cascade or SVM classifier. C++ programe in OpenCV (V2.4.5) ?",
        "input": "",
        "output": "<p>visit <a href=\"https://github.com/DaHoC/trainHOG\" rel=\"nofollow\">https://github.com/DaHoC/trainHOG</a></p>\n\n<p>it has a tutorial and sample code for detecting humans using  HOG + Svmlight.\nYou can use the sample code (with some changes :)) in visual studio. (at least you can get an idea)</p>\n",
        "system": ""
    },
    {
        "instruction": "How to get images of cars samples for HaarCascade training?",
        "input": "",
        "output": "<p>Here are some useful links to dataset:\n<a href=\"http://cogcomp.cs.illinois.edu/Data/Car/\" rel=\"nofollow\">http://cogcomp.cs.illinois.edu/Data/Car/</a>\n<a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/\" rel=\"nofollow\">http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/</a>\n<a href=\"http://cbcl.mit.edu/software-datasets/CarData.html\" rel=\"nofollow\">http://cbcl.mit.edu/software-datasets/CarData.html</a>\n<a href=\"http://www.emt.tugraz.at/~pinz/data/GRAZ_02/\" rel=\"nofollow\">http://www.emt.tugraz.at/~pinz/data/GRAZ_02/</a></p>\n\n<p>OpenCV already have a trained xml file. You could make use of this if required.</p>\n",
        "system": ""
    },
    {
        "instruction": "Connected component labeling (CCL) for vehicle headlight detection",
        "input": "",
        "output": "<p>Depending on the type of image you're using, and depending upon the output of the thresholding operation, you may manage to bypass CCL and go straight to contour tracing. </p>\n\n<p>I'd say the real challenge is to correctly threshold the image, so that the binary resultant contains only the headlight (as close as possible).</p>\n\n<p>For example, if I take this image:</p>\n\n<p><img src=\"https://i.sstatic.net/KCRqM.jpg\" alt=\"enter image description here\"></p>\n\n<p>and do the following steps sequentially:\n1. conversion to grayscale\n2. median filter\n3. Otsu's threshold\n4. Morphological gradient</p>\n\n<p>... I get the following output:</p>\n\n<p><img src=\"https://i.sstatic.net/r64kT.png\" alt=\"enter image description here\"></p>\n\n<p>I guess a simple Hough circle on that image would suffice.</p>\n",
        "system": ""
    },
    {
        "instruction": "Can deformable part based model used for image classification?",
        "input": "",
        "output": "<p>It depends on what you consider \"classification\". In can, for example, classify an object as a dog, a car or a car. Determining the specific breed of the cat/dog would probably be too difficult for that algorithm.</p>\n",
        "system": ""
    },
    {
        "instruction": "Template Matching when image is partially off screen?",
        "input": "",
        "output": "<p>I think that this task is more texture segmentation task then template matching.</p>\n\n<p>Something like that: <a href=\"http://www.cs.utah.edu/~suyash/Dissertation_html/node78.html\" rel=\"nofollow\">http://www.cs.utah.edu/~suyash/Dissertation_html/node78.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "createFisherFaceRecognizer in Java OpenCV",
        "input": "",
        "output": "<p>yes, known bug. </p>\n\n<p>the (underlying) c++ method createFisherFaceRecognizer() returns a <code>Ptr&lt;FaceRecognizer&gt;</code> and the auto-generated java wrappers can't handle those Ptr types.</p>\n\n<p>if you're good with jni, you could reinvent it.</p>\n\n<p>javacv has a crap-api imho, but at least they got this solved manually</p>\n",
        "system": ""
    },
    {
        "instruction": "Sobel filter is working on the original i/p image but not on the copy of it",
        "input": "",
        "output": "<p><code>imshow</code> with a <code>double</code> argument expects the values to be in the range 0.0 (black) to 1.0 (white). Anything greater than 1 is clipped to white. Since presumably <code>inputImage</code> is integer, using <code>double(inputImage)</code> doesn't rescale and you end up with <code>double</code> values in ranges like 0-255 or 0-65535 depending on the bit depth, which therefore just show as white.</p>\n\n<p>If you need <code>double</code> format, use <code>im2double</code> which rescales the values correctly. Otherwise, just create an integer copy with <code>copyImage = inputImage</code>.</p>\n\n<p>If you don't have the Image Processing Toolbox for <code>im2double</code>, you can do an unsigned integer-to-double image conversion with:</p>\n\n<pre><code>dblimg = double(img) ./ double(intmax(class(img)));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Parameters for the wallflower dataset for mixture of gaussian algorithm",
        "input": "",
        "output": "<p>I think I got a good enough decent output with 5 gaussians, 2 VT, 0.99 background ratio and 0.15 learning rate.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finger/Hand Gesture Recognition using Kinect",
        "input": "",
        "output": "<p>You don't need to train your first algorithm since it will complicate things.\nDon't use color either since it's unreliable (mixes with background and changes unpredictably depending on lighting and viewpoint)</p>\n\n<ol>\n<li>Assuming that your hand is a closest object you can simply \nsegment it out by depth threshold. You can set threshold manually, use a closest region of depth histogram, or perform <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=contours#cv.FindContours\" rel=\"nofollow noreferrer\">connected component</a> on a depth map to break it on meaningful parts first (and then select your object based not only on its depth but also using its dimensions, motion, user input, etc). Here is the output of a connected components method:\n<img src=\"https://i.sstatic.net/kxo3j.png\" alt=\"depth image\">\n<img src=\"https://i.sstatic.net/vN4C6.png\" alt=\"connected components\">\n<img src=\"https://i.sstatic.net/EfYIB.png\" alt=\"hand mask improved with grab cut\"></li>\n<li><p>Apply <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=convexity#cv.ConvexityDefects\" rel=\"nofollow noreferrer\">convex defects</a> from opencv library to find fingers;</p></li>\n<li><p>Track fingers rather than rediscover them in 3D.This will increase stability. I successfully implemented such finger detection about 3 years ago. </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Ball detection with OpenCV",
        "input": "",
        "output": "<p>Let me collect all the other advice in one answer:</p>\n\n<ol>\n<li>Use <a href=\"http://opencv.itseez.com/modules/core/doc/operations_on_arrays.html?highlight=inrange#void%20inRange%28InputArray%20src,%20InputArray%20lowerb,%20InputArray%20upperb,%20OutputArray%20dst%29\" rel=\"nofollow noreferrer\">cv::inRange()</a> to get the correct color (<a href=\"https://stackoverflow.com/questions/9018906/detect-rgb-color-interval-with-opencv-and-c\">More Information on that</a>). You might want to use something like <a href=\"http://akash0x53.github.io/blog/2013/04/29/normalization/\" rel=\"nofollow noreferrer\">RGB Normalization</a> beforehand to make sure you retreive the complete ball.</li>\n<li>Now you have all the pixel that relate to the ball (and maybe some noise that you have to get rid of). Retrieve the pixels that are farthest apart left/right and top/bottom in your ball (aka your connected Component that has a plausible size) to get the size of the ball. (If the ball doesn't have to be round you probably want to take the bigger value)</li>\n<li>Compute the distance from the camera with the now known size of the ball in the picture. (You have to know the \"real\" size beforehand for this computation obviously ;))</li>\n</ol>\n\n<p>There obviously are other ways (f.e. use edge detection), but this is imo the easiest.</p>\n\n<p>It is easier to give you an answer if you post an example picture.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect uniform images that (most probably) are not photographs",
        "input": "",
        "output": "<p>This how I would do it (Whole Method below, at the bottom of post, but just read from top to bottom):</p>\n\n<p>Your quote:</p>\n\n<blockquote>\n  <p>\"By photograph I mean a photograph of people, a landscape, an animal\n  etc.\"</p>\n</blockquote>\n\n<p>My response to your quote:</p>\n\n<blockquote>\n  <p>This means that such images have edges, contours. The images you are\n  trying to separate out, no edges or little contours(for the second\n  example image at least)</p>\n</blockquote>\n\n<p>Your quote:</p>\n\n<blockquote>\n  <p>one white or black pixel will ruin tests like that. So I would need to\n  somehow exclude extreme values if there are only few pixels with such\n  extremes</p>\n</blockquote>\n\n<p>My response:</p>\n\n<blockquote>\n  <p>Minimizing the noise through methods like DoG(Difference of Gaussian), etc will reduce the\n  noisy, individual pixels</p>\n</blockquote>\n\n<p>So I have taken your images and run it through the following codes:</p>\n\n<pre><code>cv::cvtColor(image, imagec, CV_BGR2GRAY); // where image is the example image you shown\ncv::GaussianBlur(imagec,imagec, cv::Size(3,3), 0, 0, cv::BORDER_DEFAULT ); //blur image\ncv::Canny(imagec, imagec, 20, 60, 3);\n</code></pre>\n\n<p>Results for example image 1 you gave:\n<img src=\"https://i.sstatic.net/vAHQl.png\" alt=\"enter image description here\"></p>\n\n<p>As you can see after going through the code, the image became blank(totally black). The image quite big, hence bit difficult to show all in one window. </p>\n\n<p>Results for example 2 you showed me:\n<img src=\"https://i.sstatic.net/T50Oz.png\" alt=\"enter image description here\"></p>\n\n<p>The outline can be seen, but one method to solve this, is to introduce an ROI of about 20 to 30 pixels from the dimension of the image, so for instance, if image dimension is 640x320, the ROI may be 610x 290, where it is placed in the center of the image. </p>\n\n<p>So now, let me introduce you my real method:</p>\n\n<blockquote>\n  <p>1) run all the images through the codes above to find edges</p>\n  \n  <p>2) check which images doesn't have any any edges( images with no edges\n  will have 0 pixel with values more then 0 or little pixels with values more then 0, so set a slightly higher threshold to play it safe? You adjust accordingly, how many pixels to identify your images )</p>\n  \n  <p>3) Save/Name out all the images without edges, which will be the images\n  you are trying to separate out from the rest.</p>\n  \n  <p>4) The end.</p>\n</blockquote>\n\n<p>EDIT(TO ANSWER THE COMMENT, would have commented back, but my reply is too long):</p>\n\n<p>true about the blurring part. To minimise usage of blurring, you can first do an \"elimination-like process\", so those smooth like images in image 1 will be already separated and categorised into images you looking for.</p>\n\n<p>From there you do a second test for the remaining images, which will be the \"blurring\". </p>\n\n<p>If you really wish to avoid blurring, what I notice is that your example image 1 can be categorised as \"smooth surface\" while your example image 2 can be categorised as \"rough-like surface\", meaning which it be noisy, which led me to introduce the blurring in the first place.</p>\n\n<p>From my experience and if I do remember correctly, such rough-like surfaces is very good in \"watershed\" or \"clustering through colour\" method, they blend very well, unlike the smooth images.</p>\n\n<p>Since the leftover images are high chances of rough images, you can try the watershed method, and canny, you will find it be a black image, if I am not wrong. Try a line maybe like this: </p>\n\n<p><code>pyrMeanShiftFiltering( image, images, 10, 20, 3)</code> </p>\n\n<p>I am not very sure if such method will be more expensive than Gaussian blurring. but you can try both and compare the computational speed for both. </p>\n\n<p>In regard to your comment on grayscale images:</p>\n\n<blockquote>\n  <p>Converting to grayscale sounds risky - loosing color information may\n  trigger lot's of false positives</p>\n</blockquote>\n\n<p>My answer:</p>\n\n<blockquote>\n  <p>I don't really think so. If your images you are trying to segment out\n  are of one colour, changing to grayscale doesn't matter. Of course if\n  you snap a photo of a blue sky, it might cause to be a false negative,\n  but as you said, those are ok.</p>\n  \n  <p>If you think about it, images with people, etc inside, the intensity\n  change differs quite a lot. (of course unless your photograph have\n  extreme cases, like a green ball on a field of grass)</p>\n  \n  <p>I do admit that converting to grayscale loses information. But in your\n  case, I doubt it will affect much, in fact, working with grayscale\n  images is faster and less expensive.</p>\n</blockquote>\n",
        "system": ""
    },
    {
        "instruction": "Sensitivity of depth, infrared and RGB images",
        "input": "",
        "output": "<p>RGB images would be sensitive to illumination changes, infrared would not be affected as they come from a heat sensor, and for depth images it depends how you are computing depth</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect person in bed",
        "input": "",
        "output": "<p>I dont know about infrared images but for camera based video processing this kind of problem is widely studied.</p>\n<p>If your problem is to detect a person in a bed which is &quot;Normally empty&quot; then I think the simplest algorithm would be to capture successive frames and calculate their difference.\nThe existence of human in the frame would make it different from a frame capturing only empty bed. Depending on various algorithms like <a href=\"https://ieeexplore.ieee.org/document/5223320\" rel=\"nofollow noreferrer\">this</a> you would get different reliability.</p>\n<p>Otherwise you can go directly for human detection in video frames. One possible <a href=\"https://ieeexplore.ieee.org/document/1467360\" rel=\"nofollow noreferrer\">algorithm</a> is described here.</p>\n<p><strong>Edit:</strong></p>\n<p>Your problem is harder than i thought. The following approach might solve the cases.\nThe main idea is to use bunch of features at once to get higher accuracy and remove false positives.</p>\n<ol>\n<li><p>Use HOG person detector at top level to detect a person's entry in the scene. If the position of the possible entry doors are known or detectable using edge lines in the scene use it to increase accuracy. (At the point of entry the diference in successive frames will be located near the doors)</p>\n</li>\n<li><p>Use Edge lines to track the human. And use the bed edges to track the position of the human. The edges of human should be bounded by the edges of the bed.</p>\n</li>\n<li><p>If the difference is located within the bed implies human is in the bed but moving.</p>\n</li>\n<li><p>If needed as a preprocessing step include analysis of texture, connected component to remove possible moving objects in the room for higher accuracy (for example:- movement of clothes because of air).</p>\n</li>\n<li><p>Also use face detectors to increase accuracy.</p>\n</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "detect object from video using LatentSVMDetector",
        "input": "",
        "output": "<p>I suggest you  </p>\n\n<ol>\n<li>to resize the frame to a size that 10x5 pixel is the smallest possible car in the frame;</li>\n<li>to do a blur first; it is possible to get lots of false-positives because there is noise that generate edges that can be similar to the car;</li>\n<li>I suppose that the detector is for side cars (I have not tested it) ant it will not detect cars rotated with more than 60 degrees and it is trained on some database that was not similar to your environment; so maybe it is better to train your own detector (car.xml).</li>\n</ol>\n\n<p>HOG is based on edges and edges are very sensible to light and shadows. Try to preprocess (contrast enhancement) the frame before detecting the cars.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to get the region shown in the image ?",
        "input": "",
        "output": "<p>For this specific image, let's call it <code>BW</code>, you can find the center region as:</p>\n\n<pre><code>BWnoBorder= imclearborder(BW); %# remove the white that touches the border\nOnlyCenter = bwareaopen(BWnoBorder,1000); %# remove all small pixel areas\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How can I capture frames from a PointGrey USB camera in OpenCV/Python/OSX?",
        "input": "",
        "output": "<p>Many years later, we can answer this question. FLIR, which acquired PointGrey, has released Spinnaker, a cross-platform SDK with a UI and a set of pre-compiled examples for accessing the camera: <a href=\"https://www.flir.com/support-center/iis/machine-vision/downloads/spinnaker-sdk-and-firmware-download/\" rel=\"nofollow noreferrer\">https://www.flir.com/support-center/iis/machine-vision/downloads/spinnaker-sdk-and-firmware-download/</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Delaunay Trangulation for VS2012 C++?",
        "input": "",
        "output": "<p>I haven't used VS for quite a few years, but last time I did it was happily linking my application with Triangle, and we were quite satisfied with the results.</p>\n\n<p>What are the compatibility problems you are seeing?</p>\n",
        "system": ""
    },
    {
        "instruction": "Converting grayscale video to binary for code generation",
        "input": "",
        "output": "<p>This line is wrong:</p>\n\n<pre><code>Binary(inputVideo&gt;=threshold)==true;\n</code></pre>\n\n<p>Here you are comparing <code>Binary(inputVideo&gt;=threshold)</code> with <code>true</code>. Correct:</p>\n\n<pre><code>Binary(inputVideo&gt;=threshold)=true;\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Normalisation of (2D) homogeneous coordinates",
        "input": "",
        "output": "<p>Downscaling by the max absolute value of the components is the safer option among all correct ones - you never have to worry about overflows as long as the max > 1.0. Dividing by w is only required to convert to Euclidean points.</p>\n\n<p>Incidentally, working with floats is rarely a good idea in internal calculations, although it may make sense for storing final results that correspond to physical quantities you measure. When doing geometrical calculations, in my experience, truncation errors often propagate fast enough, even with well conditioned algorithms, to make calculated results in single precision rather worthless.</p>\n",
        "system": ""
    },
    {
        "instruction": "program shows unresolved external symbols on every line of opencv",
        "input": "",
        "output": "<p>That error means you have not told visual studio that it needs to link against the right libraries. I'd go back to the instructions on setting up windows (<a href=\"http://docs.opencv.org/doc/tutorials/introduction/windows_visual_studio_Opencv/windows_visual_studio_Opencv.html#windows-visual-studio-how-to\" rel=\"nofollow\">here</a>) and double check all your paths are correct + that the libraries you built all built correctly.</p>\n",
        "system": ""
    },
    {
        "instruction": "Head Pose estimation in real-time video",
        "input": "",
        "output": "<p>Take a look at <a href=\"https://github.com/CMU-Perceptual-Computing-Lab/openpose\" rel=\"nofollow noreferrer\">OpenPose</a>, it's a real-time multi-person system to jointly detect human body, hand, and facial keypoints (in total 130 keypoints) on single images. Using a CNN, it's a great  example of Deep Learning in the field of computer vision. </p>\n\n<p>OpenPose is originally build with Caffe, but ports to TensorFlow are starting to emerge. For example this <a href=\"https://github.com/ildoonet/tf-pose-estimation\" rel=\"nofollow noreferrer\">github repo</a>, is a implementation of OpenPose using TensorFlow. In addition, the new model is bases on Mobilenet, which makes it more fit to run on CPU or low-power embedded devices (like a smartphone for example). BTW, TensorFlow supports both Android and iOS! </p>\n",
        "system": ""
    },
    {
        "instruction": "Computer Vision: Remove Edge Effect",
        "input": "",
        "output": "<p>What you are requesting is essentially a two-part modification to a stitching algorithm (ie: an algorithm that pieces together multiple images into a single one).</p>\n\n<p>The first part is de-vignetting, which is essentially correcting the change in brightness in the corner of the images. This is common when taking pictures with wide-angle lenses. The solution isn't exactly trivial, and I don't have a working C++ source example. For the overall problem, my guess is you'll have to learn the math behind it and implement it yourself. I can provide examples for a shell-script program that passes the image through ImageMagik to resolve this problem though. The algorithm itself is fully worked out in layman's terms too.</p>\n\n<p>The second half, involves stitching the now de-vignetted images together. You seem to already have that part down. Now, even with the images having had the brightness artifacts at their edges corrected, the average intensity of each image will be different. This is fixed easily enough via histogram equalization, for which I do have a C++ example.</p>\n\n<p>So, in short:</p>\n\n<ol>\n<li>Apply de-vignetting algorithm to each frame</li>\n<li>Apply histogram equalization algorithm to each frame</li>\n<li>Stitch all the original frames into a final image</li>\n</ol>\n\n<p>After going through the work involved, you'll see why most of the work involving this is commercial, and the source isn't free.</p>\n\n<p><strong>References</strong></p>\n\n<hr>\n\n<ol>\n<li><p>\"De-vignetting\", Accessed 2014-02-11, <a href=\"http://www.physics.mcmaster.ca/~syam/Photo/\" rel=\"nofollow\">http://www.physics.mcmaster.ca/~syam/Photo/</a></p></li>\n<li><p>\"Image Registration with Global and Local Luminance Alignment\", Accessed 2014-02-11, <a href=\"http://www.cse.cuhk.edu.hk/leojia/all_project_webpages/luminance_alignment/luminance_alignment.html\" rel=\"nofollow\">http://www.cse.cuhk.edu.hk/leojia/all_project_webpages/luminance_alignment/luminance_alignment.html</a></p></li>\n<li><p>\"Histogram equalization using C++: Image Processing\", Accessed 2014-02-11, <a href=\"http://www.programming-techniques.com/2013/01/histogram-equalization-using-c-image.html\" rel=\"nofollow\">http://www.programming-techniques.com/2013/01/histogram-equalization-using-c-image.html</a></p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How to clear history frames after done tracking in life streaming video?",
        "input": "",
        "output": "<p>call the <code>Dispose()</code> method when the images are already processed. This will release the unmanaged IplImage structure that is consuming the memory.</p>\n",
        "system": ""
    },
    {
        "instruction": "nearest City block distance between a point and line",
        "input": "",
        "output": "<p>The point that is perpendicular to the line when connecting with the given point is a good guess. You can start from this point and then go both ends. You can stop if the distances increase on both directions. The solution is the stopping point.</p>\n",
        "system": ""
    },
    {
        "instruction": "nearest Manhattan distance between a point and line",
        "input": "",
        "output": "<p>This is a search problem. You need to start from your point and apply a <a href=\"http://en.wikipedia.org/wiki/Breadth-first_search\" rel=\"nofollow\">breadth first search</a>, grow until you hit a line pixel. The children states for any pixel should be right - up - left - down neighbors. The manhattan distance will be nothing but the depth of the goal.</p>\n\n<p>EDIT: Remember to add some heuristics for faster search, for e.g. if all line pixels are to the left of the starting point; you don't need to visit <code>right</code>. The angle of the line would be another thing to consider, for further reduction of states.</p>\n",
        "system": ""
    },
    {
        "instruction": "openCV confidence value - is it good or bad?",
        "input": "",
        "output": "<p>You should not trust the confidence score too much. It's all-in-all relative. You'd better adjust it according to your real situations.</p>\n<p>There's a brief discussion on what this distance actually is on the <a href=\"http://opencv-users.1802565.n2.nabble.com/Confidence-Level-based-on-Euclidean-Distance-td5876309.html\" rel=\"nofollow noreferrer\">OpenCV-users list</a>. The function they use to compute the confidence is like:</p>\n<pre><code>distance = 1.0f - sqrt( distSq / (float)(nTrainFaces * nEigens) ) / 255.0f \n</code></pre>\n<p>However, the author of the function says that it is a very rough guide and not a full proof guide. See the link to the users list discussion for a reference to the paper and a suggestion for an alternative metric.</p>\n",
        "system": ""
    },
    {
        "instruction": "Mosaicing face problems in opencv",
        "input": "",
        "output": "<p>The registration procedure is maybe working, but you need to define better which features you want to use for the alignment or you are using images with a deformation that is too large.</p>\n\n<ul>\n<li>In the small (color) image, the eyes are aligned.</li>\n<li>In the big (grey) image, parts of the face appear to be correctly aligned, but not the eyes (for example).</li>\n</ul>\n\n<p>If your goal is to have eyes and mouth aligned, then you should detect them and apply some 2D (affine or homographic) mapping that aligns them with your template. if your goal is to have the green / blue points aligned, then you need to use them to feed the 2D mapping estimator, but in this case there will be no guarantee that the eyes, mouth, etc. are aligned: only the points should match.</p>\n",
        "system": ""
    },
    {
        "instruction": "Find corner points of eyes and Mouth",
        "input": "",
        "output": "<p>Active Appearance Model (AAM) could be useful in your case. </p>\n\n<p>AMM is normally used for matching a statistical model of object shape and appearance to a new image and widely used for extracting face features and for head pose estimation. </p>\n\n<p>I believe <a href=\"http://www.mathworks.com/matlabcentral/fileexchange/32704-icaam-inverse-compositional-active-appearance-models\" rel=\"nofollow\">this</a> could be helpful for you to start with.  </p>\n",
        "system": ""
    },
    {
        "instruction": "Getting the most occurrence value in a vector&lt;cv::Vec3b&gt; with C++",
        "input": "",
        "output": "<p>Lets say your 3D array is <code>array[][][]</code> and at some location (x,y) you got R=200, G=100, B=10 then you will increment the value of <code>array[200][100][10]</code> by 1. Lets at some another location also you again get R=200, G=100, B=10 then again you will increment <code>array[200][100][10]</code> by 1 so, now the total value at <code>array[200][100][10]</code> is 2. You will do the same for all pixel location.</p>\n\n<p>Then, at the end you will find the maximum value in your 3D array and lets say that maximum value is 1000 at <code>array[210][15][10]</code>. This represents that the combination of R.G and B which occurs maximum number of times is \"R=210, G=15, B=10\"</p>\n\n<p>So, in short we can say that value at [ ][ ][ ] tells you , how many times the combination [R][G][B] has occurred.</p>\n",
        "system": ""
    },
    {
        "instruction": "JavaCV cvInRangeS Error",
        "input": "",
        "output": "<p><code>cvInRangeS()</code> assumes the type of the input image to be <code>CV_8U</code>, so you have to convert it first.</p>\n\n<pre><code>...\ncvtColor(imghsv, grayscale, CV_BGR2GRAY ); \ncvInRangeS(grayscale,minc,maxc,imgbin);\n...\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Filling Grayscal Image C#",
        "input": "",
        "output": "<p>I suggest you to use opencv library, which works well under C# also that you may be know already about. you find functions to\n - detect outer contours\n - detect inner contours, and make some morphological processes that help you in filling holes.</p>\n",
        "system": ""
    },
    {
        "instruction": "Feature Extraction of image using gabor filter?",
        "input": "",
        "output": "<p>In order to identify the language, you should extract <a href=\"https://www.cs.auckland.ac.nz/courses/compsci708s1c/lectures/Glect-html/topic4c708FSC.htm\" rel=\"nofollow noreferrer\">texture features</a> such as co-occurrence matrices, Tamura features or Gabor Texture Features.</p>\n\n<p>\"The Gabor texture features include the mean and the standard deviation of the magnitude of the Gabor wavelet transform coefficients\". The idea is to apply different Gabor filters (with different scales and orientations) and then to extract features from the obtained images.</p>\n\n<p>You can find more details here : <a href=\"https://stackoverflow.com/questions/20608458/gabor-feature-extraction\">Gabor feature extraction</a></p>\n",
        "system": ""
    },
    {
        "instruction": "comparing blob detection and Structural Analysis and Shape Descriptors in opencv",
        "input": "",
        "output": "<p>As @scap3y suggested in the comments I'd go for a much simpler approach. What I'm always doing in these cases is something similar to this:</p>\n\n<pre><code>// Convert your image to HSV color space\nMat hsv;\nhsv.create(originalImage.size(), CV_8UC3);\ncvtColor(originalImage,hsv,CV_RGB2HSV);\n\n// Chose the range in each of hue, saturation and value and threshold the other pixels\nMat thresholded;\nuchar loH = 130, hiH = 170;\nuchar loS = 40, hiS = 255;\nuchar loV = 40, hiV = 255;\ninRange(hsv, Scalar(loH, loS, loV), Scalar(hiH, hiS, hiV), thresholded);\n\n// Find contours in the image (additional step could be to \n// apply morphologyEx() first)\nvector&lt;vector&lt;Point&gt;&gt; contours;\nfindContours(thresholded,contours,CV_RETR_EXTERNAL,CHAIN_APPROX_SIMPLE);\n\n// Draw your contours as ellipses into the original image\nfor(i=0;i&lt;(int)valuable_rectangle_indices.size();i++) {\n    rect=minAreaRect(contours[valuable_rectangle_indices[i]]);\n    ellipse(originalImage, rect, Scalar(0,0,255));  // draw ellipse\n}\n</code></pre>\n\n<p>The only thing left for you to do now is to figure out in what range your markers are in <a href=\"http://en.wikipedia.org/wiki/HSL_and_HSV\" rel=\"nofollow\">HSV color space</a>. </p>\n",
        "system": ""
    },
    {
        "instruction": "Hough Circles, and an overly complicated solution",
        "input": "",
        "output": "<p>Instead of using houghcircle you can do the following.</p>\n\n<ol>\n<li><p>Segment the blue color.</p></li>\n<li><p>Find contours(largest).</p></li>\n<li><p>Minimum enclosing circle for contour.</p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Geometric transformation using MATLAB Function block",
        "input": "",
        "output": "<p>As an image is warped/rotated the number of rows and/or columns in it will change.</p>\n\n<p>You'll need to modify your code so that outputImage is variable sized.\nTo do this open the code (for the MATLAB Function Block) in the editor and open up the \"Edit Data\" dialog.\nSelect outputImage in the left hand column, then</p>\n\n<ol>\n<li>check the \"variable size\" check box</li>\n<li>enter something like [400 400 3] as the size </li>\n</ol>\n\n<p>You have to make a best guess for the maximum row and column size that you expect.</p>\n\n<p>Note also that the Matlab function block must have a discrete sample time for it to use variable sized signals.\nTo set this, right click on the block and select Block Parameters, then set a sample time.</p>\n",
        "system": ""
    },
    {
        "instruction": "HaarTraining - MATLAB or OpenCV?",
        "input": "",
        "output": "<p>What kind of objects are you trying to detect?  And what kind of negative images are you using?  Ideally, the negative images should be large images of scenes typically associated with your objects of interest.  </p>\n\n<p><strong>Edit:</strong>\nEven if you are providing 30K negative <em>images</em>, the training may still not have enough negative <em>samples</em>.  The trainCascadeObjectDetector function generates negative samples for each stage, by running the detector consisting of the stages it has so far on the negative images.  If the detector detects any objects, they are by construction false positives, and they are used as negative samples for the next stage.  Depending on what kind of negative images you supply, it may well be possible that after some number of stages, the current detector does not detect any false positives in the negative images. </p>\n\n<p>You have said that your negative images come from a video of your room. The problem may be that all your negative images are too similar to each other. So you should probably try including other images among your negative set, to diversify it. Also you should make sure to include images containing hand gestures other than the one you are training for among your negative images.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Image stiching when camera parameters are known",
        "input": "",
        "output": "<p>Since you are using homographies to warp your imagery, I assume you are capturing areas small enough that you don't have to worry about Earth curvature effects. Also, I assume you don't use an elevation model.</p>\n\n<p>Generally speaking, you will always want to tighten your (homography) model using matched image points, since your final output is a stitched image. If you have the RAM and CPU budget, you could refine your linear model using a max likelihood estimator.</p>\n\n<p>Having a prior motion model (e.g. from GPS + IMU) could be used to initialize the feature search and match. With a good enough initial estimation of the feature apparent motion, you could dispense with expensive feature descriptor computation and storage, and just go with normalized crosscorrelation.   </p>\n",
        "system": ""
    },
    {
        "instruction": "Measure of accuracy in pattern recognition using SURF in OpenCV",
        "input": "",
        "output": "<p>I am not 100% sure about what you are really asking, because what you call a \"match\" is vague. But since you said you already matched your SURF points and mentionned pattern recognition and the use of a template, I am assuming that, ultimately, you want to localize the template in your image and you are asking about a localization score to decide whether you found the template in the image or not.</p>\n\n<p>This is a challenging problem and I am not aware that a good and always-appropriate solution has been found yet. </p>\n\n<p>However, given your approach, what you could do is analyze the density of matched points in your image: consider local or global maximas as possible locations for your template (global if you know your template appears only once in the image, local if it can appear multiple times) and use a threshold on the density to decide whether or not the template appears. A sketch of the algorithm could be something like this:</p>\n\n<ol>\n<li>Allocate a floating point density map of the size of your image</li>\n<li>Compute the density map, by increasing by a fixed amount the density map in the neighborhood of each matched point (for instance, for each matched point, add a fixed value <em>epsilon</em> in the rectangle your are displaying in your question)</li>\n<li>Find the global or local maximas of the density map (global can be found using opencv function MinMaxLoc, and local maximas can be found using morpho maths, e.g. <a href=\"https://stackoverflow.com/questions/1856197/how-can-i-find-local-maxima-in-an-image-in-matlab\">How can I find local maxima in an image in MATLAB?</a>)</li>\n<li>For each maxima obtained, compare the corresponding density value to a threshold <em>tau</em>, to decide whether your template is there or not</li>\n</ol>\n\n<p>If you are into resarch articles, you can check the following ones for improvement of this basic algorithm:</p>\n\n<ul>\n<li><p><a href=\"http://arxiv.org/pdf/0910.1273\" rel=\"nofollow noreferrer\">\"ADABOOST WITH KEYPOINT PRESENCE FEATURES FOR REAL-TIME VEHICLE VISUAL DETECTION\", by T.Bdiri, F.Moutarde, N.Bourdis and B.Steux, 2009.</a></p></li>\n<li><p><a href=\"ftp://ftp.vision.ee.ethz.ch/publications/bookchapters/eth_biwi_00422.pdf\" rel=\"nofollow noreferrer\">\"Interleaving Object Categorization and Segmentation\", by B.Leibe and B.Schiele, 2006.</a></p></li>\n</ul>\n\n<p><strong>EDIT</strong>: another way to address your problem is to try and remove accidently-matched points in order to keep only those truly corresponding to your template image. This can be done by enforcing a constraint of consistancy between close matched points. The following research article presents an approach like this: <a href=\"http://perso.telecom-paristech.fr/~sahbi/publication-193.pdf\" rel=\"nofollow noreferrer\">\"Context-dependent logo matching and retrieval\", by H.Sahbi, L.Ballan, G.Serra, A.Del Bimbo, 2010</a> (however, this may require some background knowledge...).</p>\n\n<p>Hope this helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "Haarcascade operates on 348x288 images only?",
        "input": "",
        "output": "<p>It can be used for your images as long as you setup the correct parameters for <a href=\"http://docs.opencv.org/modules/objdetect/doc/cascade_classification.html#cascadeclassifier-detectmultiscale\" rel=\"nofollow\"><code>CascadeClassifier::detectMultiScale()</code></a>, especially the following three:</p>\n\n<ul>\n<li><p><strong>scaleFactor</strong> \u2013 Parameter specifying how much the image size is reduced at each image scale.</p></li>\n<li><p><strong>minSize</strong> \u2013 Minimum possible object size. Objects smaller than that are ignored.</p></li>\n<li><p><strong>maxSize</strong> \u2013 Maximum possible object size. Objects bigger than that are ignored.</p></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Read images from folders in directory",
        "input": "",
        "output": "<p>You can use the <code>dir</code> command to get a list of sub-directories, and then loop through that list with calls to <code>mkdir</code> to create each one in turn. After that, it is just a matter of matching the file paths so you can save the greyscale image loaded from a source subfolder to its corresponding target folder.</p>\n\n<p>Specifically, <code>D = dir('directory')</code> will return a struct where each element of the structure is an element stored in 'directory'. <code>D(i).isdir</code> will be 1 if <code>D(i).name</code> corresponds to the name of one of your subfolders (note that you will need to ignore <code>D(1:2)</code>, as those are the folder navigation tags <code>.</code> and <code>..</code>). So, get your list of directory contents, and then loop through those calling <code>mkdir</code> if <code>D(i).isdir</code> is 1.</p>\n\n<p>I am not sure I understand the rest of your question, but if you just need a random subsample of the entire image set (regardless of subfolder it is stored in), while you are making your subfolders above you can also make secondary calls of <code>dir</code> to the subfolders to get a list of their contents. Loop through and check whether each element is an image, and if it is save it to an array of image path names. When you have compiled this master list, you can grab a random subset from it.</p>\n",
        "system": ""
    },
    {
        "instruction": "Filled circle detection using CV2 in Python?",
        "input": "",
        "output": "<p>I came up with this code, it is tuned to the exact image you have supplied, finds 2943 circles with the radius estimate as well, assuming all circles have the same radius. This is what it produces (cropped, original was too big):</p>\n\n<p><img src=\"https://i.sstatic.net/f5uRx.jpg\" alt=\"Crop of the resulting image\"></p>\n\n<p>You can see that its not completely ideal (the corner circle is a bit off).</p>\n\n<p>It is based around thresholding and then contour operations rather than hough circles.</p>\n\n<pre><code>import cv2\nimport numpy as np\n\noriginal = cv2.imread(\"test.jpg\", cv2.CV_LOAD_IMAGE_GRAYSCALE)\nretval, image = cv2.threshold(original, 50, 255, cv2.cv.CV_THRESH_BINARY)\n\nel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\nimage = cv2.dilate(image, el, iterations=6)\n\ncv2.imwrite(\"dilated.png\", image)\n\ncontours, hierarchy = cv2.findContours(\n    image,\n    cv2.cv.CV_RETR_LIST,\n    cv2.cv.CV_CHAIN_APPROX_SIMPLE\n)\n\ndrawing = cv2.imread(\"test.jpg\")\n\ncenters = []\nradii = []\nfor contour in contours:\n    area = cv2.contourArea(contour)\n\n    # there is one contour that contains all others, filter it out\n    if area &gt; 500:\n        continue\n\n    br = cv2.boundingRect(contour)\n    radii.append(br[2])\n\n    m = cv2.moments(contour)\n    center = (int(m['m10'] / m['m00']), int(m['m01'] / m['m00']))\n    centers.append(center)\n\nprint(\"There are {} circles\".format(len(centers)))\n\nradius = int(np.average(radii)) + 5\n\nfor center in centers:\n    cv2.circle(drawing, center, 3, (255, 0, 0), -1)\n    cv2.circle(drawing, center, radius, (0, 255, 0), 1)\n\ncv2.imwrite(\"drawing.png\", drawing)\n</code></pre>\n\n<p>Hope it helps</p>\n",
        "system": ""
    },
    {
        "instruction": "Python Scikit-image processing of Gel electrophoresis data",
        "input": "",
        "output": "<p>Perhaps get in touch with the authors of <a href=\"https://github.com/hugadams/pyparty\" rel=\"nofollow\">https://github.com/hugadams/pyparty</a>, which is built on top of scikit-image.</p>\n\n<ol>\n<li>You may want to first equalize the image (see the \"exposure\" submodule)</li>\n<li>You'll first have to do some kind of peak detection (see the \"feature\" submodule)</li>\n<li>I'm not quite sure what you are asking here</li>\n<li>Rather image warping (see the \"transform\" submodule)</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Merging with background while thresholding",
        "input": "",
        "output": "<ol>\n<li><p>Preprocessing: find big black areas on your image and mark it as background. \nDo this for example with treshold. Another way might be to use <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/find_contours/find_contours.html\" rel=\"nofollow\">findContours</a> (contourArea to get the size on the result).\nThis way you know what areas you can colour black after step 1.</p></li>\n<li><p>Use OTSU (top image, right column, blue title background).</p></li>\n<li>Colour everything you know to be background black.</li>\n<li>Use Opening/Closing or Erode/Dilate (not sure which will work better) to get rid of small lines and to refine your results\nAlternatively you could make an edge detection and merge all areas that are \"close together\", like the second 3 in your example. You could check if areas are close together with a distance between the bounding box of your contours.</li>\n</ol>\n\n<p>ps: I don't think you should blur your image, since it seems to be pretty small already.</p>\n",
        "system": ""
    },
    {
        "instruction": "Haarcascade detects only face area and not ears",
        "input": "",
        "output": "<p>The cascade was trained for faces and not ears, so what you are seeing is \"normal\".</p>\n\n<ul>\n<li>If you want detect faces that include ears also, then you can simply expand the face rectangle by some percentage of their width in order to retain them in the crop;</li>\n<li>or if you want to really only detect the ears, then you have to train your own cascade. Like for the eyes, you wold need to apply face detection first in order to reduce the number of false positives. Here is a guide on <a href=\"http://docs.opencv.org/doc/user_guide/ug_traincascade.html\" rel=\"nofollow\">how to train a cascade</a> to help you.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "How does one convert a grayscale image to RGB in OpenCV (Python)?",
        "input": "",
        "output": "<p>I am promoting my comment to an answer:</p>\n<p>The easy way is:</p>\n<blockquote>\n<p>You could draw in the original 'frame' itself instead of using gray image.</p>\n</blockquote>\n<p>The hard way (method you were trying to implement):</p>\n<pre><code>backtorgb = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n</code></pre>\n<p>is the correct syntax.</p>\n",
        "system": ""
    },
    {
        "instruction": "Initialize rectangle in the center which is equal to 50% of the image size using Grabcut",
        "input": "",
        "output": "<p>Well, they are assuming the interested figure is bounded by a rectangle like:</p>\n\n<pre><code>Rect(Point(image.width/4, image.height/4), Point(image.width*3/4, image.height*3/4);\n</code></pre>\n\n<p>or the rectangle having <strong>area</strong> of half of the image (1/root2 size); again centered on image center point.</p>\n\n<p>Then grabcut algorithm can be called within that rectangle.</p>\n\n<p>If the figure is somehow close to the boundaries, then they select the rectangle by hand (supervised initiation). Since the purpose is to train the SVM with grabcut results, automation is not a must here.</p>\n",
        "system": ""
    },
    {
        "instruction": "Demons algorithm for image registration (for dummies)",
        "input": "",
        "output": "<p>I can give you an overview of general algorithms for deformable image registration, demons is one of them</p>\n\n<p>There are 3 components of the algorithm, a similarity metric, a transformation model and an optimization algorithm.</p>\n\n<p>A similarity metric is used to compute pixel based / patch based similarity between pixels/patches. Common similarity measures are SSD, normalized cross correlation for mono-modal images while information theoretic measures like mutual information are used in the case of multi-modal image registration.</p>\n\n<p>In the case of deformable registration, they generally have a regular grid super-imposed over the image and the grid is deformed by solving an optimization problem which is formulated such that the similarity metric and the smoothness penalty imposed over the transformation is minimized. In deformable registration, once there are deformations over the grid, the final transformation at the pixel level is computed using a B-Spine interpolation of the grid at the pixel level so that the transformation is smooth and continuous.</p>\n\n<p>There are 2 general approaches towards solving the optimization problem, some people use discrete optimization and solve it as a MRF optimization problem while some people use gradient descent, I think demons uses gradient descent. </p>\n\n<p>In case of MRF based approaches, the unary cost is the cost for deforming each node in grid and it is the similarity computed between patches, the pairwise cost which imposes the smoothness of the grid, is generally a potts/truncated quadratic potential which ensures that neighboring nodes in the grid have almost the same displacement. Once you have the unary and pairwise cost, you feed it to a MRF optimization algorithm and get the displacements at the grid level, then you use a B-Spline interpolation to compute pixel level displacement. This process is repeated in a coarse to fine fashion over several scales and also the algorithm is run many times at each scale (reducing the displacement at each node every time).</p>\n\n<p>In case of gradient descent based methods, they formulate the problem with the similarity metric and the grid transformation computed over the image and then compute the gradient of the energy function which they have formulated. The energy function is minimized using iterative gradient descent, however these approaches can get stuck in a local minima and are quite slow.</p>\n\n<p>Some popular methods are DROP, Elastix, itk provides some tools </p>\n",
        "system": ""
    },
    {
        "instruction": "Auto Focus webcam before snapping image with OPENCV",
        "input": "",
        "output": "<p>According to this page \"<a href=\"http://lieksa.pp.fi/oh7fes/cameras/focus.htm\" rel=\"nofollow\">Logitech webcam B910, C910 and C920 Auto-focus fix</a>\" it seems that the focus on Logitech C920 webcam is factory adjusted for short and medium distances. The same page has some instructions about changing the factory setup.</p>\n",
        "system": ""
    },
    {
        "instruction": "Generalising found transformation matrix to a whole image",
        "input": "",
        "output": "<p>If you already have the transformation matrix, then cv::warpAffine is the right way to go. Your error message seems to be about the type of the transformation matrix and/or its size, which should be 2x3 float or double precision.</p>\n",
        "system": ""
    },
    {
        "instruction": "Resolving rotation matrices to obtain the angles",
        "input": "",
        "output": "<p>Why do you expect them to be the same? They are not the same thing at all.</p>\n\n<p>The Euler angles are three angles of rotation about one axis at a time, starting from the world frame. </p>\n\n<p>Rodriguez's formula gives components of one vector in the world frame, and an angle of rotation about that vector.</p>\n",
        "system": ""
    },
    {
        "instruction": "Homography decomposition of the matrix obtained in findHomography()",
        "input": "",
        "output": "<p>Zhang's calibration procedure will produce a rotation matrix. That's representable as 3 (Euler) angles, not one. Or, equivalently (via Rodriguez's formula), as one unit vector and an angle of rotation about that vector - i.e. three numbers again.</p>\n\n<p>Unless your camera and target are very carefully positioned with respect to each other, there is no reason to expect that there be just one non-zero angle of rotation.</p>\n",
        "system": ""
    },
    {
        "instruction": "Java library or strategy to tell if an image contains a signature",
        "input": "",
        "output": "<p>I do not know of any simple solutions. You can wrap over <a href=\"http://quexf.sourceforge.net/\" rel=\"nofollow\">queXF</a>  or write something similar in Java. <a href=\"http://www.docstoc.com/docs/6587611/A-Colour-Code-Algorithm-For-Signature-Recognition\" rel=\"nofollow\">This paper</a> talks about color code algorithm to recognize signatures.</p>\n",
        "system": ""
    },
    {
        "instruction": "Sift Implementation :: Keypoints from different octaves",
        "input": "",
        "output": "<p>It is not clear, what do you mean by \"last 3 octaves\"? \nAbout translation - you multiply obtained scale (from blurring kerkel) and (x,y) by factor of two for 2nd octave, by 4 for 3rd octave, etc... </p>\n",
        "system": ""
    },
    {
        "instruction": "Performing simple inverse and multiplication operations on Mat matrices",
        "input": "",
        "output": "<p>You should  make sure at least two things before Mat inverse.</p>\n\n<ol>\n<li><p>The Matrix should be square.</p></li>\n<li><p>The Matrix should be non-singular, that is determinant should be non-zero.</p></li>\n</ol>\n\n<p><strong>Eg:</strong></p>\n\n<pre><code>    Mat A(3,3,CV_32FC1);\n    A=(Mat_&lt;float&gt;(3,3)&lt;&lt; 0,2,3,\\\n                          4,5,6,\\\n                          7,8,9);\n    cout&lt;&lt;A &lt;&lt;endl;\n\n    if(determinant(A)!=0){\n    Mat B=A.inv();\n    cout&lt;&lt;B &lt;&lt;endl;\n    }\n</code></pre>\n\n<p>Also see the answer <a href=\"https://stackoverflow.com/questions/15051669/mat-inv-yielding-all-zeroes-in-opencv\">Mat.inv() yielding all zeroes in opencv</a></p>\n\n<p><strong>Edit:-</strong></p>\n\n<p>Here is some bug I found in your code</p>\n\n<p>The piece of code</p>\n\n<pre><code>    Mat matrix_f(2,3,CV_32F);\n    matrix_f=Mat(coordinates_f);\n</code></pre>\n\n<p>should changed to </p>\n\n<pre><code>    Mat matrix_f(2,3,CV_32F);\n    matrix_f=Mat(coordinates_f);\n    matrix_f=matrix_f.reshape(1,2);\n</code></pre>\n\n<p>because later you are going to multiply with a 3X3 Mat, So we will make it's rows to 3</p>\n\n<p>And next is change the lines</p>\n\n<pre><code>    Mat new_left(3,3,CV_32F);\n    new_left=Mat(coordinates_l_new);\n</code></pre>\n\n<p>to </p>\n\n<pre><code>      Mat new_left(3,3,CV_32F);\n      new_left=Mat(coordinates_l_new);\n      new_left=new_left.reshape(1,3);\n</code></pre>\n\n<p>as you are going to find the inverse of new_left, it should be square matrix. </p>\n\n<p>And finally make sure the Mat is non-singular by finding determinant</p>\n\n<pre><code> if(determinant(new_left)!=0) {\n        Mat T(3,3,CV_32F);\n        T=matrix_f * (new_left.inv());\n  } \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Navigation with camera and bounding boxes",
        "input": "",
        "output": "<p>Anything wrong with <a href=\"http://www.mmrc.iss.ac.cn/~xgao/paper/ieee.pdf\" rel=\"nofollow\">P3P</a>, for example as <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#solvepnp\" rel=\"nofollow\">implemented</a> in OpenCV? Or am I misunderstanding the question?</p>\n\n<p>There are, in fact, fast and low-storage algorithms to solve this problem. One became a fairly successful commercial product in 1999, when a fast PC's CPU was clocked at somewhat less than 0.7 GHz, and RAM cost about 10Mbit/USD. It's name was <a href=\"http://www.canoma.com/\" rel=\"nofollow\">Canoma</a>.</p>\n\n<p>A very similar system was described in Paul Debevec's PhD thesis at about the same time, see <a href=\"http://www.pauldebevec.com/Thesis/debevec-phdthesis-1996_ch5_facade.pdf\" rel=\"nofollow\">here</a> for details</p>\n",
        "system": ""
    },
    {
        "instruction": "Detecting color range from &quot;average&quot; in OpenCV",
        "input": "",
        "output": "<p>Get the 3D color histogram within the detected region of interest. That is, not three 1D histograms for each channel, but one 3D histogram for all 3 channels together. OpenCV's <a href=\"http://docs.opencv.org/modules/imgproc/doc/histograms.html?highlight=calchist#calchist\" rel=\"nofollow noreferrer\">calcHist</a> has options for that. Here's <a href=\"https://stackoverflow.com/questions/15834602/how-to-calculate-3d-histogram-in-python-using-open-cv\">an example</a> which does that. This example is using Python bindings for OpenCV, but it shows how to set the parameters.</p>\n\n<p>Also, set the range of the histogram within a reasonable range for skin color. As MSalters suggested in the comments, HSV is a better color space for things like this. Perhaps you can disregard the S and V channels and only do the 1D histogram for V. Try is out.</p>\n\n<p>The bin with the highest count is going to be your \"average\" skin color.</p>\n",
        "system": ""
    },
    {
        "instruction": "Facial feature points using flandmark library - MATLAB r2013d",
        "input": "",
        "output": "<p>I have the same problem for compiling flandmark . To produce Flandmark_static library ,first produce a solution using CMAKE then run the project flandmark-static. This way you do not get the warning . See if you can produce MEX files this way.</p>\n",
        "system": ""
    },
    {
        "instruction": "Real time face detection using Viola and Jones in MATLAB",
        "input": "",
        "output": "<p>A more recent and updated approach than Viola and Jones's detector is <a href=\"https://sites.google.com/site/leeplus/publications/facedetectionusingsurfcascade\" rel=\"nofollow\">SURF cascade</a>. It should run faster with better results.<br>\nI also think there is an implementation of this method for windows platforms <a href=\"https://sites.google.com/site/leeplus/publications/learningsurfcascadeforfastandaccurateobjectdetection\" rel=\"nofollow\">here</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Removal of vertical HoughLines/Detection of Horizontal HoughLines only",
        "input": "",
        "output": "<p>Python version</p>\n\n<pre><code>import numpy as np\nangle = np.arctan2(y2 - y1, x2 - x1) * 180. / np.pi\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Frame rate drops when using background subtraction MOG2?",
        "input": "",
        "output": "<p>I'm assuming when you say \"choppy\" you mean real time processing has become slow. The MOG model is pretty computationally expensive.</p>\n\n<p>Do you need a MOG model though? MOG is good if you have pixels which can take more than one expected range of values, and yet are still background. For example, trees moving in the wind: sky color and also leaf color. If you just subtract consecutive frames you get a lot of variation, but the MOG model will treat both as background. So if your application is not like that, maybe you shouldn't use MOG. </p>\n\n<p>If you do decide to use the MOG model, setting the maximum number of mixture components to something like 2 (if that makes sense for your application) will likely make it faster.</p>\n",
        "system": ""
    },
    {
        "instruction": "How I can detect a set of dots is either rhombic or square lattice",
        "input": "",
        "output": "<p>One way to do it might be following:</p>\n\n<ol>\n<li><p>Detect the circle using Hough transform but of course the centerpoints are approximate;</p></li>\n<li><p>Choose arbitrary number of points to test. Number of points depends on how accurate you must be / how fast you need to process an image. Multiple points are needed to implement voting to reduce impact of errors. You can use any method to choose points - either complete random or semi random (random with certain point distribution across the image);</p></li>\n<li><p>To detect orientation of grid for each point chosen do nearest neighbor search and find 4 nearest neighbors. Those neighbors will be on edges of the grid lines that goes through particular point. Depending on data-set size you could find neighbors by yourself with two <code>for</code> loops or you could use OpenCV <code>knnSearch</code> method from FLANN library (Fast Library for Approximate Nearest Neighbors);</p></li>\n<li><p>When you have 4 nearest neighbors for each point, you need to determine orientation of lattice at specified point. For that you need to calculate vertical and horizontal distance between each sample point and it's neighbor. If the lattice is squared then Min(deltaX, deltaY) should be close to 0. If it is rhombic, then it will be about half of distance between each two points. Process all the neighbors and make a decision whether lattice it is square or rhombic at this point. Do that for every test point and collect results;</p></li>\n<li><p>Process results and make final decision on lattice orientation depending on votes from each test point. </p></li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV : FREAK Descriptor arguments",
        "input": "",
        "output": "<p>As old-ufo said: with \"orientationNormalized\" the orientation of the keypoint will be estimated and stored in the keyPoint's parameter 'angle'. Which can than be used for better matching.</p>\n\n<p>The scaleNormalization indeed influences the border-check, but the background is that if you turn on scaleNormalization, the size of the pattern (used to extract the descriptor) will be scaled according to <a href=\"https://github.com/kikohs/freak/blob/master/src/freak.cpp#L332\" rel=\"nofollow\">this</a> formula. (having in mind, that the 'minimum' keypoint size is by <a href=\"https://github.com/kikohs/freak/blob/master/src/freak.cpp#L73\" rel=\"nofollow\">default</a> 7)<br>\nTo better understand the scaling also be sure to look into the <a href=\"https://github.com/kikohs/freak/blob/master/src/freak.cpp#L208\" rel=\"nofollow\">buildPattern()</a> method!\n(and to read the according <a href=\"http://www.ivpe.com/papers/freak.pdf\" rel=\"nofollow\">paper</a>)</p>\n",
        "system": ""
    },
    {
        "instruction": "Transforming a Mat matrix",
        "input": "",
        "output": "<p>You need to use <a href=\"http://docs.opencv.org/modules/core/doc/basic_structures.html#mat-push-back\" rel=\"nofollow\">Mat::push_back</a> which will adds elements to the bottom of the matrix.</p>\n\n<p>For example</p>\n\n<pre><code>          Mat A = (Mat_&lt;uchar&gt;(3,4) &lt;&lt; 1, 2,  3,  4,\\\n                                       5, 6,  7,  8,\\\n                                       9, 10, 11, 12);  // 3X4  matrix.\n\n          Mat B = (Mat_&lt;uchar&gt;(1,4) &lt;&lt; 13, 14,  15,  16); // 1X4 matrix\n\n          A.push_back(B); // Now A become 4X4 matrix\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Printing Mat matrix on different lines",
        "input": "",
        "output": "<p>In this case (referring to the comments) you need to print your matrices row-wise. Make use of m.row() and m.at(). Supposed that you have 3x3 matrices as in the image:</p>\n\n<pre><code>for (int i = 0; i &lt; 3; ++i)\n{\n    Mat row1 = m1.row (i);\n    Mat row2 = m2.row (i);\n    Mat row3 = m3.row (i);\n\n    // this can be replaced by a loop, I spell it out for the sake of clearness\n    cout &lt;&lt; row1.at(0, 0) &lt;&lt; \" \" &lt;&lt; row1.at (0, 1) &lt;&lt; \" \" &lt;&lt; row1.at (0, 2) &lt;&lt; \"\\t\"\n         &lt;&lt; row2.at(0, 0) &lt;&lt; \" \" &lt;&lt; row2.at (0, 1) &lt;&lt; \" \" &lt;&lt; row2.at (0, 2) &lt;&lt; \"\\t\"\n         &lt;&lt; row3.at(0, 0) &lt;&lt; \" \" &lt;&lt; row3.at (0, 1) &lt;&lt; \" \" &lt;&lt; row3.at (0, 2) &lt;&lt; endl;\n\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Triangulation to find distance to the object- Image to world coordinates",
        "input": "",
        "output": "<p>So you already computed the triangulation? Well, then you have points in camera coordinates, i.e. in the coordinate frame centered on one of the cameras (the left or right one depending on how your code is written and the order in which you feed your images to it).</p>\n\n<p>What more do you want? The vector length (square root of the sum of the square coordinates) of those points <em>is</em> their estimated distance from the same camera. If you want their position in some other \"world\" coordinate system, you need to give the coordinate transform between that system and the camera - presumably through a calibration procedure. </p>\n",
        "system": ""
    },
    {
        "instruction": "Recognition and counting of books from side using OpenCV",
        "input": "",
        "output": "<p>I ran a sobel edge detector and used Hough transform to detect lines on the last image and it seemed to be working okay for me. You can then link the edges on the output of the sobel edge detector and then count the number of horizontal lines. Or, you can do the same on the output of the lines detected using Hough.</p>\n\n<p>You can further narrow down the area of interest by converting the image into a binary image. The outputs of all of these operators can be seen in following figure ( I couldn't upload an image so had to host it here) <a href=\"http://www.pictureshoster.com/files/v34h8hvvv1no4x13ng6c.jpg\" rel=\"nofollow\">http://www.pictureshoster.com/files/v34h8hvvv1no4x13ng6c.jpg</a></p>\n\n<p>Refer to <a href=\"http://www.mathworks.com/help/images/analyzing-images.html#f11-12512\" rel=\"nofollow\">http://www.mathworks.com/help/images/analyzing-images.html#f11-12512</a> for some more useful examples on how to do edge, line and corner detection. </p>\n\n<p>Hope this helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "TrainCascade stuck on getting new negatives",
        "input": "",
        "output": "<p>Around 24 hours sounds normal to me. Haar training can actually take up to days depending on size and number of samples. And of course on the computer as well. The longest my training took was approximately a week for hand detection.</p>\n\n<p>If you are really worried, to check whether the haar training is still on-going, you can try to generate an intermediate haar cascade xml file, from the data available. If you are able to generate the xml file, it would show that it's still running(albeit slow) and not stuck.</p>\n\n<p>How to improve the haar training speed, the only solution I know or used before is \"paralleling\"</p>\n\n<p>A quick search on google about that leads to a few link, here's one of them: <a href=\"http://www.computer-vision-software.com/blog/2009/06/parallel-world-of-opencv/\" rel=\"nofollow\">http://www.computer-vision-software.com/blog/2009/06/parallel-world-of-opencv/</a></p>\n\n<p>I have used such methods, and it's pretty efficient in cutting the time taken to train the Haar Cascade. So hope this method suits you well. Do try my method of generating an immediate xml file from the current data available first though. If there is any needs, do comment, I try get back to you soon. Cheers.</p>\n",
        "system": ""
    },
    {
        "instruction": "In feature matching, is the &#39;second best ratio&#39; test asymmetric?",
        "input": "",
        "output": "<p>Shortly, yes, it is assymetric. You don't miss the step. Is it really problem for stitching? You have have already hundreds of the fine matchings...</p>\n",
        "system": ""
    },
    {
        "instruction": "Rectangular Blank Space Detection (OpenCV)",
        "input": "",
        "output": "<p>The simplest thing I would do is; take a white pixel (assume your image is binary), and start to enlarge it with in x and y direction separately. Once an enlargement process hit a black pixel stop enlarging in that direction. But you should keep enlarging the region in other direction until it hits a black pixel too. Otherwise you will get only square pixels.</p>\n\n<p>Then you must save which positions assigned to a region so a pixel should not be assigned to more than one region. Also you must have threshold values for the size in each direction (length and width), therefore you can avoid so many regions.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to filter OpenSURF invalid matches using RANSAC in C#",
        "input": "",
        "output": "<p>Accord.Imaging allows find SURF features and estimate points using RANSAC. </p>\n\n<pre><code>var correlationPoints1 = matches[0];\nvar correlationPoints2 = matches[1];\nvar ransac = new RansacHomographyEstimator(0.10, 0.99);\nvar homography = ransac.Estimate(correlationPoints1, correlationPoints2);\nvar inliers1 = correlationPoints1.Submatrix(ransac.Inliers);\nvar inliers2 = correlationPoints2.Submatrix(ransac.Inliers);\nvar result = new IntPoint[][]\n                    {\n                        inliers1,\n                        inliers2\n                    };\n</code></pre>\n\n<p><a href=\"http://www.nuget.org/packages/Accord.Imaging/\" rel=\"nofollow\">http://www.nuget.org/packages/Accord.Imaging/</a></p>\n\n<p><a href=\"http://accord-framework.net/samples.html\" rel=\"nofollow\">http://accord-framework.net/samples.html</a></p>\n",
        "system": ""
    },
    {
        "instruction": "PTZ Camera Roll at Pan/Tilt",
        "input": "",
        "output": "<p>I think I figured it out.</p>\n\n<p>If I cross the Up Vector, from my rotation matrix, with the Forward Vector, calculated from the camera current Pan/Tilt, I will get the Side Vector which I can use to calculate the Roll angle.</p>\n\n<pre><code>double roll = Math.atan(side.z / Math.sqrt(side.x * side.x + side.y * side.y));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Dilate/erode - anchor point in matrix kernel topleft vs. middle",
        "input": "",
        "output": "<p>In the first case, the anchor point should be set to [0,0], in the second case should be set in the center, which is [1,1]. If you use [-1,-1] as anchor point, it will default to the center of the mask, useful for example when testing different kernel sizes. (see <a href=\"http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=dilate#dilate\" rel=\"nofollow\">http://docs.opencv.org/modules/imgproc/doc/filtering.html?highlight=dilate#dilate</a>)</p>\n\n<p>For the rest you can check the tutorial:\n<a href=\"http://docs.opencv.org/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html\" rel=\"nofollow\">http://docs.opencv.org/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html</a></p>\n\n<p>Regarding your specific question, \napart for situations at boundaries, you can get a submatrix of your output matrix (using the operator parenthesis from cv::Mat) in which the coordinates will be as you need.</p>\n",
        "system": ""
    },
    {
        "instruction": "opencv_traincascade crash without explanation",
        "input": "",
        "output": "<p>This is happening because of the window size you chose. The memory traincascade.exe uses for training a classifier grows exponentially with window size, and there are few home computers that would handle a 100x100 window size in traincascade..</p>\n\n<p>Do this exercise: open your task manager and monitorize the memory usage growing when you start training, you'll notice that the program crashes when it can't allocate more memory.\nTo fix this you'll really have to choose a smaller window size. </p>\n",
        "system": ""
    },
    {
        "instruction": "Algorithms to classify and extract crossword grids from an image",
        "input": "",
        "output": "<p>This is a really broad question, but I wil try to give you some pointers. \nThese are the steps you need to take:</p>\n\n<ol>\n<li>Detect the <strong>position</strong> of the crossword.</li>\n<li>Detect the <strong>grid</strong> of the crossword. For this, you will need some Computer Vision algorithm (for example the <a href=\"http://en.wikipedia.org/wiki/Hough_transform\" rel=\"nofollow noreferrer\">Hough lines detector</a>).</li>\n<li>For each cell, you need to find if it have a <strong>character</strong> or not. To do so, you just simply analize the \"amount\" of white color does the cell have</li>\n<li>For the cells containing a character you need to recognize it. To do so, you need an <a href=\"http://en.wikipedia.org/wiki/Optical_character_recognition\" rel=\"nofollow noreferrer\">OCR</a>, and I recommend you <a href=\"http://code.google.com/p/tesseract-ocr/\" rel=\"nofollow noreferrer\">Tesseract</a>.</li>\n<li>Create your own algorithm for <strong>solving</strong> the crosswords. You can use <a href=\"https://stackoverflow.com/questions/15099510/python-how-to-make-crossword-solver-faster\">this</a>.</li>\n</ol>\n\n<p>And here (<a href=\"http://opencvpython.blogspot.com.es/2012/06/sudoku-solver-part-1.html\" rel=\"nofollow noreferrer\">1</a>,<a href=\"http://opencvpython.blogspot.com.es/2012/06/sudoku-solver-part-2.html\" rel=\"nofollow noreferrer\">2</a>,<a href=\"http://opencvpython.blogspot.com.es/2012/06/sudoku-solver-part-3.html\" rel=\"nofollow noreferrer\">3</a>) you have an example of a Sudoku Solver in Python. The first steps are common to your problem so you can use OpenCV to solve it like this:</p>\n\n<pre><code>import cv2\nimport numpy as np\n\n#Load the Black and White image\nimg =  cv2.imread('sudoku.jpg')\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\ngray = cv2.GaussianBlur(gray,(5,5),0)\nthresh = cv2.adaptiveThreshold(gray,255,1,1,11,2)\n\n#Detect the lines of the sudoku\ncontours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n#Detect the square of the Sudoku\nbiggest = None\nmax_area = 0\nfor i in contours:\n        area = cv2.contourArea(i)\n        if area &gt; 100:\n                peri = cv2.arcLength(i,True)\n                approx = cv2.approxPolyDP(i,0.02*peri,True)\n                if area &gt; max_area and len(approx)==4:\n                        biggest = approx\n                        max_area = area\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Image comparison or processing library in python?",
        "input": "",
        "output": "<p>If you have no prior experience with OpenCV I'd start with <a href=\"http://simplecv.org/\" rel=\"nofollow\">SimpleCV</a>. It is built on top of OpenCV but is much easier to use</p>\n",
        "system": ""
    },
    {
        "instruction": "Remove unnecessary histogram peaks from a histogram.",
        "input": "",
        "output": "<p>Can you use a larger number of consecutive frames? If so, and the peaks that you wish to keep are consistent enough (which looks like they are) then you can take the median values across multiple frames.</p>\n\n<p>So instead of computing the median for the entire histogram, do it for each individual intensity value with the previous frame and the next frame (or with more frames for more accuracy). Then use the median value as the new intensity value in the histogram. This will remove the outliers (i.e. the noise you're seeing in the second frame).</p>\n",
        "system": ""
    },
    {
        "instruction": "How to detect the difference between two 3D point clouds?",
        "input": "",
        "output": "<p>there are some &quot;Spatial change detection&quot; solutions offered by PCL.</p>\n<p>take look at this link: <a href=\"http://pointclouds.org/documentation/tutorials/octree_change.html#octree-change-detection\" rel=\"nofollow noreferrer\">change detection</a></p>\n<p>It uses the octree structures (build from point clouds) and compare the two octrees for differences.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to estimate the disparity/depth map limits from stereo geometry parameters?",
        "input": "",
        "output": "<p>Some clarification first - the question seems to confuse disparity with distance, whereas they are (roughly) the inverse of each other. The disparity is large in absolute value for close points, small for far away ones. E.g., for cameras with parallel focal axes, a point at infinity in the direction of the same axis would have exactly zero disparity.</p>\n\n<p>As for the sensor resolution, if you have the EXIF headers, see if it's actually there. The keys of interest are \"FocalPlaneXResolution\" etc. The one you quote, XResolution, is the print resolution, which is something else (and completely useless). See <a href=\"http://www.media.mit.edu/pia/Research/deepview/exif.html\" rel=\"nofollow\">this page</a> for details.</p>\n\n<p>The distance resolution and bounds are easier to work out for parallel cameras. See, for example <a href=\"http://pub1.willowgarage.com/~konolige/svs/disparity.htm\" rel=\"nofollow\">this article</a>. For toed-in cameras the geometry is a little more complicated, especially if you cannot assume that the focal axes are convergent (so that baseline and both focal axes lie in the same plane). Either way, you compute them as follows:</p>\n\n<ul>\n<li><p>For the closest distance, you consider one plane and two rays: </p>\n\n<ul>\n<li>The plane is the back-projection of the <em>right</em> edge of the <em>left</em> image, i.e. the plane containing the left of optical center and the right edge of the left image.</li>\n<li>The rays are the back-projections of the top-left and bottom-left corners of the <em>right</em> image.</li>\n<li>The closest of the two points of intersection between the plane and the rays is the closest point that's visible and measurable by the stereo pair.  </li>\n</ul>\n\n<p>-</p></li>\n<li>For the farthest distance, you consider: \n<ul>\n<li>The ray Rf in the <em>right</em> camera that's parallel to the focal axis F of the <em>left</em> camera. Backproject Rf to find its corresponding pixel. The disparity at this pixel is, of course, exactly zero, since it is the disparity of two parallel rays. </li>\n<li>Now move one pixel toward the left in the same plane Rf-F, and find the ray Rd that backprojects this second pixel. Rd must intersect F, since it's in the same plane and is not parallel to F. The intersection is at the farthest distance that can be measured units of disparity of one pixel.</li>\n</ul></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Using SVM to train my Dataset",
        "input": "",
        "output": "<p>Are you trying to implement the classification training with two classes? Your <code>label = [ones(size(C2res{1},2),1)];</code> contains only one value, and it seems to be expected two values (as two categories). I think that's the reason the error comes out.</p>\n\n<p>If you are applying one-class SVM, try to add such option (see <a href=\"http://www.openpr.org.cn/files/help/rn01re19.html\" rel=\"nofollow\">this page</a> for more references on the option): </p>\n\n<pre><code>SVMStruct = svmtrain(XTrain , label,'-t 0 -s 2');\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Is this the right way of projecting the training set into the eigespace? MATLAB",
        "input": "",
        "output": "<p>By signals, I assume you mean to ask why are we subtracting the mean from raw vector form of image. </p>\n\n<p>If you think about PCA; it is trying to give you best direction where the data varies most. However, as your images contain pixel probably only positive values those pixels will always be on positive which will mislead, especially, your first and most important eigenvector. You can search more about second moment matrix. But I will share a bad paint image that explains it. Sorry about my drawing.</p>\n\n<p>Please ignore the size of stars;</p>\n\n<p>Stars: Your data</p>\n\n<p>Red Line: Eigenvectors;</p>\n\n<p>As you can easily see in 2D, centering the data can give better direction for your principal component. If you skip this step, your first eigenvector will bias on mean and cause poorer results.</p>\n\n<p><img src=\"https://i.sstatic.net/ThGpk.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Selection of window size in Adaptive thresholding",
        "input": "",
        "output": "<p>Your example images are one of those kind on which researchers are trying to find robust algorithms. And in my opinion <a href=\"http://homepages.inf.ed.ac.uk/rbf/HIPR2/adpthrsh.htm\" rel=\"nofollow\">adaptive thresholding</a> approach might not be the best option. The reason is as follows:-</p>\n\n<p>In this technique the region within the window size is assumed to be uniform . Now in your case the characters size and distance between camera to license plate (Guessing the images to be of license plates) are important factor in determining correct window size. But those factors again are difficult to detect.</p>\n\n<p>You might consider <a href=\"http://www.cse.oulu.fi/CMV/Downloads/LBPMatlab\" rel=\"nofollow\">Local binary pattern</a> based tecnique or look into this paper.</p>\n\n<p><em><strong>Yoon, Youngwoo, et al. \"Best Combination of Binarization Methods for License Plate Character Segmentation.\" ETRI Journal 35.3 (2013).</em></strong></p>\n",
        "system": ""
    },
    {
        "instruction": "Motion based human detection",
        "input": "",
        "output": "<p>Once you have the blobs of the body parts, get the bounding boxes of them and merge the boxes that touch each other, or that are close enough.</p>\n\n<p>To merge the boxes just create a box that completely contains the other boxes: with minX=min(minX of all other boxes), maxX=max(maxX of all other boxes)  ...</p>\n",
        "system": ""
    },
    {
        "instruction": "Deriving edges when not continuous",
        "input": "",
        "output": "<p>What you can do is apply <code>Gaussian Blur</code> on image 2 , so that the image is smooth and then apply the <code>canny edge detection</code> method.</p>\n\n<p>so the code might look something like this.</p>\n\n<pre><code>Image&lt;Gray, Byte&gt; imgaux = new Image&lt;Gray, Byte&gt;(\"example.bmp\");\nimgaux = imgaux .SmoothGaussian(3,3,34.3,45.3);\nimgaux =imgaux.Canny(100, 300);\n</code></pre>\n\n<p>You can play around with the values in the SmoothGaussian function's parameters  to match your needs.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to use BestOf2NearestMatcher",
        "input": "",
        "output": "<p>BestOf2NearestMatcher is implementation of the Lowe`s 2nd nearest neighbor ratio criterion described in SIFT paper, where he recommend to use ratio = 0.8. The same holds for the SURFs. Sometimes even 0.85. Value in 0.3 is too strictive.</p>\n",
        "system": ""
    },
    {
        "instruction": "building positive samples for rotated images for cascaded training in OpenCV",
        "input": "",
        "output": "<p>Q: Will a classifier recognize a specific angle of a vehicle regardless of which way it's rotated on the background?</p>\n\n<p>A: Nope. A classifier cannot recognize a specific angle of vehicle regardless of what way it is rotated. (Talking about Haar-like features here).There is a concept created to introduce tilted 45\u00b0 Haar-like feature to improve \"dimensionality\" which is pretty successful from research papers I read on it in the past.</p>\n\n<p>There was also an attempt to introduce generic rotated haar-like features, but it was pretty much unsuccessful from my memory. However if you are going to use super high resolution image, chances are it will work. But I won't bet money on it.</p>\n\n<p>Q: To your whole problem</p>\n\n<p>A: Background in positive image samples may not necessarily affect detection badly. From case to case basis, it may actually help in your detection. </p>\n\n<p>My solution(if you really have to use classifiers), at least for image based detection, is to make use of OpenCV rotate() function. Where you keep rotating the image from 1\u00b0 to 360\u00b0(like maybe 10\u00b0 each time) and keep applying the classifiers each time. Detection time may take slightly longer, but I don't think it will be more than a few seconds. </p>\n\n<p>For video wise, it will be super laggy if I am not wrong. Do give it a shot if time permits. </p>\n\n<p>Another thing I wish to raise is that your vehicles(like the truck and the car), have very different features. If I were you, I would split them into different classifiers and run them at the same time for vehicle detection (Done it before with 3 different classifiers, eyes, hand and face with real time results). </p>\n\n<p>If you are intending to train them into one classifier, it may or may not work, so do look into my suggestion to be on the safe side.</p>\n\n<p>You may also want to look at this links: <a href=\"http://docs.opencv.org/modules/objdetect/doc/latent_svm.html\" rel=\"noreferrer\">http://docs.opencv.org/modules/objdetect/doc/latent_svm.html</a> (classifier more commonly used for objects detection. I always used haar, so have no experience in regards to this clasifier. Sorry)</p>\n\n<p><a href=\"http://www.araa.asn.au/acra/acra2006/papers/paper_5_63.pdf\" rel=\"noreferrer\">http://www.araa.asn.au/acra/acra2006/papers/paper_5_63.pdf</a> (rotation-based, contradicts my answer a little. But I stand on my case that if the image's resolution is not high enough, there maybe a lot of rounding errors.)</p>\n\n<p>Hope my answer helped you. Good luck (: Do comment if you need anymore help or anything is unclear.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to implement camshift or mean shift for hand tracking in C# with EmguCV",
        "input": "",
        "output": "<p>There is a code for camshift algorithm in OpenCV already.</p>\n\n<blockquote>\n  <p>../OpenCV-2.4.5/samples/cpp/camshiftdemo.cpp</p>\n</blockquote>\n\n<p>You need to port this code and maybe some functions to C#.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding word&#39;s bounding box on a low quality image",
        "input": "",
        "output": "<p>Check out stroke width transform. That is used to text detection.</p>\n",
        "system": ""
    },
    {
        "instruction": "aligning 2 face images based on their marker points",
        "input": "",
        "output": "<p>In your case, you cannot apply the homography based alignment procedure. Why not? Because it does not fit in this use case. It was designed to align flat surfaces. Faces (3D objects) with markers at different places and depths are clearly no planar surface.</p>\n\n<p>Instead, you can:</p>\n\n<ol>\n<li>try to match the markers between images then interpolate the displacement field of the other pixels. Classical ways of doing it will include <a href=\"http://faculty.cs.tamu.edu/schaefer/research/mls.pdf\" rel=\"nofollow\">moving least squares</a> interpolation or <a href=\"http://hal.archives-ouvertes.fr/docs/00/09/47/64/PDF/Bartoli_Zisserman_BMVC04.pdf\" rel=\"nofollow\">RBF</a>'s;</li>\n<li>otherwise, a more \"Face Processing\" way of doing it would be to use the decomposition of faces images between a texture and a face model (like <a href=\"http://en.wikipedia.org/wiki/Active_appearance_model\" rel=\"nofollow\">AAM</a> does) and work using the decomposition of your faces in this setup.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Algorithm used in Descriptor Matcher trainer in OpenCV",
        "input": "",
        "output": "<p>matcher.train(); for Bruteforce matcher has a void implementation , there is no index or a KD tree built, matching happens based on hamming distance between two keypoints identified as descriptors . If you use FLANN based matcher then the method matcher.train(); builds indexes and these indexes are used for matching the keypoints. If you want to use FLANN based matcher for your application following is the syntax for using FLANN based matching with binary descriptors(ORB,FREAK,BRISK).</p>\n\n<pre><code>  // paramerters for LSH index\n //LshIndexParams(int table_number, int key_size, int multi_probe_level);    \nFlannBasedMatcher matcher(new flann::LshIndexParams(20,10,2));\n</code></pre>\n\n<p>It is advisable to use bruteforce matcher if the number of descriptors are less(i.e less than 100000 )in your train set because if you use FLANN based matching then most of the time will be lost in building and retrieving indexes, hence your application performance will come down</p>\n",
        "system": ""
    },
    {
        "instruction": "Essential Matrix from 8 points algorithm",
        "input": "",
        "output": "<p>Short answer, yes. See also longer explanation on <a href=\"http://en.wikipedia.org/wiki/Essential_matrix#Determining_R_and_t_from_E\" rel=\"nofollow\">Wikipedia</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Haar Cascade xml format",
        "input": "",
        "output": "<p>if those delphi bindings are using the outdated c-api, (cvHaarDetectObjects) - you can only use the old format. </p>\n\n<p>the c++ api ( cv::CascadeClassifier ) supports both, as well as hog and lbp cascades.</p>\n\n<p>again, it's a limitation of the old c-api. if you can, avoid it !</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculate FOV from matching pixels",
        "input": "",
        "output": "<p>There are various methods for self-calibrating with a known rotation. </p>\n\n<p>I have used this one by Hartley: <a href=\"http://users.cecs.anu.edu.au/~hartley/Papers/calibration/eccv94/calib.pdf\" rel=\"nofollow\">http://users.cecs.anu.edu.au/~hartley/Papers/calibration/eccv94/calib.pdf</a></p>\n\n<p>See also this one: <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.5281&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.5281&amp;rep=rep1&amp;type=pdf</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How to get better results with OpenCV face recognition Module",
        "input": "",
        "output": "<p>First thing, as commented, increase the number of samples if possible. Also include the variations (like illumination, slight  poses etc) you expect to be in the video. However, especially for eigenfaces/ fisherfaces so many images will not help to increase performance. Sadly, the best number of training samples can depend on your data.</p>\n\n<p>The more important point is the hardness of the problem is totally depends on your video. If your video contains variations like illumination, pose; then you can't expect using purely appearance based methods(e.g Eigenfaces) and texture descriptor(LBP) will be succesful. First, you might want to detect faces. Then:</p>\n\n<ul>\n<li>You might want to estimate face position and warp to frontal; check\nfor Active Appearance Model and Active Shape Model </li>\n<li>Use histogram of equalization to attenuate illumination problem</li>\n<li>Fitting an ellipse to detected face region will help against background noise.</li>\n</ul>\n\n<p>Of course, there are many other methods available in literature; the steps I wrote is implemented in OpenCV and commonly known.</p>\n\n<p>Hope it helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "Easiest-encoded 4 visual bits?",
        "input": "",
        "output": "<p>What about one of these sixteen characters <code>0123456789ABCDEF</code> printed with OCR-A (or OCR-B) font?</p>\n\n<p>OCR-A is described in <em>Alphanumeric character sets for optical recognition -- Part 1: Character set OCR-A -- Shapes and dimensions of the printed image.</em> ISO 1073-1:1976</p>\n\n<p>OCR-B is described in <em>Alphanumeric character sets for optical recognition -- Part 2: Character set OCR-B -- Shapes and dimensions of the printed image.</em> ISO 1073-2:1976</p>\n",
        "system": ""
    },
    {
        "instruction": "Light intensity as function of exposure and image brightnes",
        "input": "",
        "output": "<p>You say \"I'm neither interested in measuring absolute light intensity nor in units.\". So I guess you only want to answer questions like: \"The light source in this image was shining N-times as bright as in this other image: what is N?\". </p>\n\n<p>Of course estimating an answer to such a question from images makes sense only if everything else stays (approximately) the same: microscope, camera, transmission (or reflection) of the imaged sample, etc. Is this the case?</p>\n\n<p>If the content of the images is approximately the same, I'd just start by comparing image-wide statistics: ratio of the median/average/n-th quantile intensities, and see if there is a common shift. Be careful if your image are 8-bit per channel: you will probably have to linearize them by removing whatever gamma compression was applied before computing the stats.</p>\n\n<p>As you notice, however, things get more complicated when the variation in exposure increase, probably because on nonlinear effects (cutoff at the lower end or saturation at the higher end). </p>\n",
        "system": ""
    },
    {
        "instruction": "use of t-test to compare performance of algorithms",
        "input": "",
        "output": "<p>classification accuracy difference between different algorithms can be evaluated by statistical methods, e.g. pairwise student T-test</p>\n",
        "system": ""
    },
    {
        "instruction": "Correcting fisheye distortion when camera model is known and fixed",
        "input": "",
        "output": "<p>Take two pictures of a flat surface having distinct feature points. One with as little distortion as possible, the other with fisheye distortion. For every feature point, note the undistorted (x, y) and distorted (X, Y) coordinates.</p>\n\n<p>Then you will need to use some 2D interpolation scheme on irregular points. You can think of the Radial Basis Function approach or Shepard's Inverse Distance Weighting. Keep in mind that you will interpolate separately x and y seen as two functions of (X, Y).</p>\n\n<p>Once you have established the interpolation, any pixel in the distorted image (having coordinates (X, Y)) will lead you to the corresponding (x, y) in the undistorted image. You will store all precomputed (x, y) in a huge LUT.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV positive samples dimensions?",
        "input": "",
        "output": "<p>First Question: Yes, all the images to be used for training have to be the same size. (at least for the last time I did face detection sample training. Should be the same here. If I am not wrong, there will be an error if the images are not of same size. But u can try it out and see if time permits.)</p>\n\n<p>Second Question: Not really sure what you are asking here. But the classifier is not that sensitive as u think. A few pixels off the object of interest, let's say the hand for instance, if the little finger is missing a few pixels(due to cropping) and other images have few pixels missing for the thumb, etc... the classifier will still be able to detect the hand. So a few pixels missing here and there or a few background pixels added in, will not affect the classifier much at the end of the day.</p>\n\n<p>Third Question: You should crop the image to consist of the car only for maximum result. try eliminate as much background as possible. I did a research based on samples with noisy background, black background and cropped samples with minimum background. Cropped samples with minimum background shows the best results in terms of false positive and false negative, from what I remember. </p>\n\n<p>U can use object marker to do it: <a href=\"http://achuwilson.wordpress.com/2011/02/13/object-detection-using-opencv-using-haartraining/\" rel=\"nofollow\">http://achuwilson.wordpress.com/2011/02/13/object-detection-using-opencv-using-haartraining/</a></p>\n\n<p>The tedious way would be to use paint to resize all the image to the same pixel value after cropping.</p>\n\n<p>This link should also answer your question: <a href=\"http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html\" rel=\"nofollow\">http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html</a></p>\n\n<p>I also agree with GilLevi that there are much better detection methods compared to Haar, HoG, LBP cascade. training of the images can take days(depends on number of images trained). If you really have to use the cascade methods and you are looking to minimise training time, \ntraining with Haar-like features takes much longer than with HoG or LBP. But results wise, I am not really sure which will ensure better performance and robustness. </p>\n\n<p>Hope my answer helped you. Should there be more questions, do comment. </p>\n",
        "system": ""
    },
    {
        "instruction": "Parse Error using vision.VideoPlayer",
        "input": "",
        "output": "<p>I don't think you can use <code>vision.VideoPlayer</code> with deploytool.  If you are on Windows, use <code>vision.DeployableVideoPlayer</code> instead.</p>\n",
        "system": ""
    },
    {
        "instruction": "Tesseract OCR: Parameter for Font Size (Single Character)",
        "input": "",
        "output": "<p>If your font size is too small then increase the image height and width ,so that tesseract will provide more accurate output.</p>\n\n<pre><code>        var srcImage1 = System.Drawing.Image.FromFile(@\"D:\\Image\\font_english.jpg\");\n        var newWidth1 = (int)(srcImage1.Width * 2);\n        var newHeight1 = (int)(srcImage1.Height * 2);\n\n        var image  = new Bitmap(srcImage1, new Size(newWidth1, newHeight1));\n        var ocr = new Tesseract();\n\n          ocr.Init(@\"D:\\OCRTEST\\tessdata\\\", \"eng\", false);\n          ocr.SetVariable(\"tessedit_char_whitelist\", \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-0123456789'?.;=,()\");\n          var result = ocr.DoOCR(image, Rectangle.Empty);\n          foreach (Word word in result)\n          {\n              Response.Write(word.Text+\" \");\n\n          }\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Image Enhancement using combination between SVD and Wavelet Transform",
        "input": "",
        "output": "<ol>\n<li><p>You need to define, what do you mean by \"USEFUL\" or \"important\" information. And only then do some steps. </p></li>\n<li><p>Histogram equalization is global transformation, which gives different results on different images. You can make an experiment - do histeq on image, that benefits from it. Then make two copies of the original image and draw in one black square (30% of image area) and white square on second. Then apply histeq and compare results.</p></li>\n</ol>\n\n<blockquote>\n  <p>Low frequency component contains the useful information, also,\n  unimportant information gets lost in this component.</p>\n</blockquote>\n\n<p>Really? Edges and shapes - which are (at least for me) quite important are in high frequencies. Again we need definition of \"useful\" information.</p>\n\n<p>I cannot see theoretical background why and how your approach would work. Could you a little bit explain, why do you choose this method?</p>\n\n<p>P.S. I`m not sure if this papers are relevant to you, but recommend \"<a href=\"http://www.ri.cmu.edu/pub_files/2013/12/which_edges_matter.pdf\" rel=\"nofollow\">Which Edges Matter?</a>\" by Bansal et al. and \"Multi-Scale Image Contrast Enhancement\" by V. Vonikakis and I. Andreadis.</p>\n",
        "system": ""
    },
    {
        "instruction": "Find density of the Edges in a image",
        "input": "",
        "output": "<p>Here is an idea.</p>\n\n<ol>\n<li>Traverse the external contour and remove it.</li>\n<li>Apply Horizontal histogram to get separate the strips.</li>\n<li>In each take vertical histogram and locate the bins with values within a neighbourhood of the peak. (Lets call these as Peak Bins)</li>\n<li>The longest contiguous sequence of peak bins should give the answer.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Reading a map programmatically",
        "input": "",
        "output": "<p>If you first do the flood filling at every pixel, you'll probably also get small regions inside letters like O, B, A. I think you could try the following:</p>\n\n<ol>\n<li>Detect areas containing labels and remember the coordinates of the rectangle around it.</li>\n<li>For each area containing text use tesseract or similar to read the label and remember it.</li>\n<li>Remove the rectangles containing text. After that you could try doing some morphological operations to try and close the regions that aren't completely closed. Start the flood fill from locations where you found text to get the regions.</li>\n</ol>\n\n<p>This could work in theory, but the results will depend on what the images look like, how well you detect the text and things like that.</p>\n\n<p>Good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "Crop the region of interest with few points available",
        "input": "",
        "output": "<p>Since you already know the red position, all you want is to crop this region?</p>\n\n<p>It's very easy, you just need to set a ROI (Region of interest) and to copy this region to another image. Like this (in pseudo-code since I don't have an open CV project up and running)</p>\n\n<pre><code>img1.ROI = varRedRectangle\nimg1.copyTo(img2)\nimg1.ROI = null;\n</code></pre>\n\n<p>If your question is how to detect the red section, I think you need to do like anyone in image recognition and work a lot because there is tons of way to do it nobody here will find them for you.</p>\n\n<p>Hope it helps!</p>\n",
        "system": ""
    },
    {
        "instruction": "Joining discontinuous skeleton shapes and discontinuous lines",
        "input": "",
        "output": "<p>I found your problem interesting and I will try to give you just some ideas but unfortunately not a complete algorithm (you know, it takes time...). I will also leave you with some unanswered questions.</p>\n\n<p>I consider the image you posted as a binary image, that is the black pixels have value of zero and the white pixels have value of one. I ignore the red pixels because I think you drew them in order to highlight where you would like to connect the broken lines; ignoring the red pixels means I will set their value to zero.</p>\n\n<p>First of all we need some definitions.</p>\n\n<p>A non-border pixel has 8 neighbor (north-west, north, north-east, east, south-east, south, south-west, west) pixels:</p>\n\n<pre><code>abc\nh*d\ngfe\n</code></pre>\n\n<p>in the above diagram the pixel is indicated by <code>*</code> and its 8 neighbor by <code>a,b,c,d,e,f,g</code> and <code>h</code>.</p>\n\n<p>I define an <em>endpoint pixel</em> as a pixel with value of one and just one neighbor with value of one, the remaining neighbor have a value of zero, so for example this diagram shows an endpoint pixel</p>\n\n<pre><code>000\n011\n000\n</code></pre>\n\n<p>because <code>d=1</code> and all the remaining neighbors are zero.\nThe following diagram shows instead a pixel which is not and endpoint pixel because it has two neighbors equal to one (<code>a=1</code> and <code>e=1</code>)</p>\n\n<pre><code>100\n010\n001\n</code></pre>\n\n<p>Now we can start to describe a part of a simple algorithm.</p>\n\n<p>In the first step find all the endpoint pixels and put them in a vector: in the following image I marked the endpoints from 1 to 15 (note that the endpoint 15 was not highlighted in the image you posted).</p>\n\n<p><img src=\"https://i.sstatic.net/coDsJ.png\" alt=\"enter image description here\"></p>\n\n<p>In the second step, for each endpoint, find its closest endpoint: for example consider the endpoint 4, its closest endpoint is 5. Now, if you follow the simple rule of connecting one endpoint with its closest endpoint you will have segments connecting 4-5, 10-11, 13-14, which are all fine. But consider 1: its closest endpoint is 2 or maybe it is 3, but I would like that the algorithm just connected 2 and 3 while connecting 1 to the leftmost vertical line. I would also like the same behavior for 6, 9 and 12.</p>\n\n<p>Now a different situation: what about 6, 7 and 8? Ignore for a moment 8, the closest endpoint of 6 is then 7 but they are already connected, how we can manage this case?</p>\n\n<p>And, last, consider 15: why did not highlight it in the image you posted? Maybe it should be ignored?</p>\n",
        "system": ""
    },
    {
        "instruction": "Why is the reconstructed model a scaled version using SfM(Structure from motion)?",
        "input": "",
        "output": "<p>If both the cameras and the scene are correspondingly scaled, the change will not be discernible in the captured images. That's why the scale factor is unknown in SfM. To obtain it, some physical measurement of the scene or the camera motion is typically needed.</p>\n\n<p>If you're not convinced, simply do the math:</p>\n\n<p>Let <code>(P1 p1)</code> and <code>(P2 p2)</code> be the 3x4 projection matrices of two cameras 1 and 2 (<code>P1</code> is a 3x3 matrix and <code>p1</code> a column vector), <code>M</code> a point in the scene, and <code>m1</code> and <code>m2</code> the respective projections of <code>M</code> in cameras 1 and 2. We have (<code>~=</code> means \"is proportional to\", because of the perspective division):</p>\n\n<pre><code>m1 ~= P1 M + p1\nm2 ~= P2 M + p2\n</code></pre>\n\n<p>Introducing the camera centers <code>C1 = -P1^-1 p1</code>, <code>C2 = -P2^-1 p2</code> and the translation <code>T = C2 - C1</code> between the cameras, this can be written:</p>\n\n<pre><code>m1 ~= P1 (M - C1)\nm2 ~= P2 (M - C2) = P2 (M - (C1 + T))\n</code></pre>\n\n<p>Now scale the whole scene by a factor of <code>s</code> and translate its origin it by <code>o</code>: <code>M' = s M + o</code>. Introduce two cameras 1' and 2', that are versions of 1 and 2 with the inverse scaling factor, i.e. <code>P1' = 1/s P1</code> and  <code>P2' = 1/s P2</code>. Scale and offset their centers <code>C1' = s C1 + o</code> and <code>C2' = s (C1 + T) + o</code>. The relative translation between the two cameras is now: <code>C2' - C1' = s T</code>. The projections of <code>M'</code> in 1' and 2' are:</p>\n\n<pre><code>m1' ~= P1' (M' - C1') = 1/s P1 (s M + o - s C1 - o) = P1 (M - C1)\n    ~= m1\nm2' ~= P2' (M' - C2') = 1/s P2 (s M + o - s (C1 + T) - o) = P2 (M - C2)\n    ~= m2\n</code></pre>\n\n<p>So in the end, you get the same projections (your input in an SfM problem) with a scene that has a different scale and origin and correspondingly scaled and translated cameras. This can be generalized to more than two cameras.</p>\n",
        "system": ""
    },
    {
        "instruction": "Strange coordinate appears after finding moments of circles",
        "input": "",
        "output": "<p>Sorry it was not about the type that i used but it was the image quality which had problems. I had to blur that part so that it does not detect that color there.</p>\n",
        "system": ""
    },
    {
        "instruction": "Opencv Imgproc.HoughLines() tuning length parameters",
        "input": "",
        "output": "<p>A very good example can be found here: <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html\" rel=\"nofollow\">http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html</a></p>\n\n<p>Try using <code>CV_PI</code> instead of <code>Math.PI</code>? Other reasons could be because of your threshold. Try inserting a value like 50(play around with the numbers). The last 2 values, u can try leaving it to be zero and test it first before inserting values. Default Values for the last 2 is usually zero. </p>\n\n<p>There could be many reasons why its not working, so let's slowly find the cause one by one. Also, you did Canny it or something before you applied Hough right?</p>\n\n<p>Hope that helps, let me know if my suggestions are useful and helped(: Cheers.</p>\n",
        "system": ""
    },
    {
        "instruction": "Headlights detection using Difference of Gaussian (DoG)",
        "input": "",
        "output": "<p>I think you may be able to get a better segmentation using a slightly different approach.</p>\n\n<p>There is already strong contrast between the lights and the background, so you can take advantage of this to segment out the bright spots using a simple threshold, then you can apply some blob detection to filter out any small blobs (e.g. streetlights). Then you can proceed from there with contour detection, Hough circles, etc. until you find the objects of interest.</p>\n\n<p>As an example, I took your source image and did the following:</p>\n\n<ol>\n<li>Convert to 8-bit greyscale</li>\n<li>Apply Gaussian blur</li>\n<li>Threshold</li>\n</ol>\n\n<p>This is a section of the source image:</p>\n\n<p><img src=\"https://i.sstatic.net/h15hC.png\" alt=\"before\"></p>\n\n<p>And this is the thresholded overlay:</p>\n\n<p><img src=\"https://i.sstatic.net/7lamQ.png\" alt=\"after\"></p>\n\n<p>Perhaps this type of approach is worth exploring further. Please comment to let me know what you think.</p>\n",
        "system": ""
    },
    {
        "instruction": "Finding individual center points of circles in an image",
        "input": "",
        "output": "<p>Found a solution!</p>\n\n<ol>\n<li>load original image to grayscale</li>\n<li>convert original image to gray</li>\n<li>set range of intensity value depending on color that needs to be detected</li>\n<li>vector of contours and hierarchy</li>\n<li>findContours</li>\n<li>vector of moments and point</li>\n<li>iterate through each contour to find coordinates</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "Kalman filter in computer vision: the choice of Q and R noise covariances",
        "input": "",
        "output": "<p>R is the covariance matrix of the measurement noise, assumed to be Gaussian. In the context of tracking objects in video it means your detection error.  Let's say you are using a face detector to detect faces, and then you want to track them using the Kalman filter. You run the detector, you get a bounding box for each face, and then you use the Kalman filter to track the centroid of each box. The R matrix must describe how uncertain you are about the location of the centroid. So in this case for the x,y coordinates the corresponding diagonal values of R should be a few pixels. If your state includes velocity, then you need to guess the uncertainty of the velocity measurement, and take the units into account. If your position is measured in pixels and your velocity in pixels per frame, then the diagonal entries of R must reflect that.</p>\n\n<p>Q is the covariance of the process noise. Simply put, Q specifies how much the actual motion of the object deviates from your assumed motion model.  If you are tracking cars on a road, then the constant velocity model should be reasonably good, and the entries of Q should be small.  If you are tracking people's faces, they are not likely to move with a constant velocity, so you need to crank up Q.  Again, you need to be aware of the units in which your state variables are expressed.</p>\n\n<p>So this is the intuition.  In practice you start with some reasonable initial guess for R and Q, and then you tune them experimentally.  So setting R and Q is a bit of an art. Also, in most cases using diagonal matrices for R and Q is sufficient.  </p>\n\n<p>Here is an <a href=\"http://www.mathworks.com/help/vision/examples/motion-based-multiple-object-tracking.html\" rel=\"nofollow noreferrer\">example</a> that uses the <code>vision.KalmanFilter</code> in Matalb for tracking multiple people.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: Essential Matrix Decomposition",
        "input": "",
        "output": "<p>I see two issues. </p>\n\n<p>First, discounting the negligible value of the third diagonal term, your E is about 6% off the ideal one: err_percent = (21.48 - 20.29) / 20.29 * 100 .  Sounds small, but translated in terms of pixel error it may be an altogether larger amount.</p>\n\n<p>So I'd start by replacing E with the ideal one after SVD decomposition: Er = U * diag(1,1,0) * Vt.</p>\n\n<p>Second, the textbook decomposition admits 4 solutions, only one of which is physically plausible (i.e. with 3D points in front of the camera). You may be hitting one of non-physical ones. See <a href=\"http://en.wikipedia.org/wiki/Essential_matrix#Determining_R_and_t_from_E\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Essential_matrix#Determining_R_and_t_from_E</a> .</p>\n",
        "system": ""
    },
    {
        "instruction": "Custom Cascade Classifier frequency of detection is lacking",
        "input": "",
        "output": "<p>First a question. How much in-plane rotation is there in the hand gestures you are trying to detect?  A cascade detector is not rotationally invariant.  That means that if your hand gestures can be tilted to the left or to the right by more than about 10 degrees, you would not be able to detect them.  The only solution there would be to rotate the image and try detecting again.</p>\n\n<p>Now some pointers:</p>\n\n<ul>\n<li>1000 positive samples is not a large number.  For detecting hand gestures, you probably need at least 10 times that if not more.</li>\n<li>Check what the minimum and the maximum window size of your detector is.  Make sure that matches the size of the hands in your images.</li>\n<li>Try a bigger window size.</li>\n<li>Try other features. OpenCV lets you use LBP and HOG, in addition to Haar. I would guess that HOG may be good for this problem, and it takes much less time to train.</li>\n</ul>\n\n<p><strong>Edit:</strong> opencv_traincascade, which replaces haartraining supports HOG features.  Alternatively, there is a trainCascadeObjectDetector function in the Computer Vision System Toolbox for Matlab that does the same thing and gives you a nicer interface.  LBP is slightly less accurate than Haar on some benchmarks, but it is much faster to train with and takes much less memory.</p>\n\n<p>If you have a lot of variation in orientation you definitely need more data.  You also need to understand the range of possible rotations.  Can your signs be upside down?  Can they be rotated by 90 degrees?  If your range is 30 degrees, maybe you can try 3 rotations of the image, or train 3 different detectors for each sign.</p>\n\n<p>Also, if you use Haar features, you may benefit from enabling the 45-degree features.  I think they are off by default.</p>\n",
        "system": ""
    },
    {
        "instruction": "Robust tracking of blobs",
        "input": "",
        "output": "<p>I would try to fit a gaussian mixture model and then use the mean and the covariance matrix to fit ellipses over the data. Such a model would work even with overlap and small noisy blobs. The data which you have would be the the coordinates of the pixels which are black and you could fit a GMM over the data. One issue with this approach is that you need to know the number of blobs you need to track in advance, if you can come up with a heuristic for that, GMM should solve this problem pretty efficiently.</p>\n",
        "system": ""
    },
    {
        "instruction": "Getting number of rows/columns from MAT Image",
        "input": "",
        "output": "<p>You are working with color image which means that its size is 3*m*n bytes. You turned only first m*n pixels to white.</p>\n",
        "system": ""
    },
    {
        "instruction": "Working on Separable Gabor filters in matlab",
        "input": "",
        "output": "<p>I think the code is correct provided your previous version of Gabor filter code is correct too. The only thing is that if <code>theta = k * pi/4;</code>, your formula <a href=\"https://stackoverflow.com/questions/20921698/how-to-create-64-gabor-features-at-each-scale-and-orientation-in-matlab\">here</a> should be separated to:</p>\n\n<pre><code>fx = exp(-(x^2)/(2*sigmaq))*cos(2*pi*x/lambda(k));\ngy = exp(-(G^2 * y^2)/(2*sigmaq));\n</code></pre>\n\n<p>To be consistent, you may use </p>\n\n<pre><code>f1(1,x+center) = fx;\nf2(y+center,1) = gy;\n</code></pre>\n\n<p>or keep <code>f1</code> and <code>f2</code> as it is but transpose your <code>filters1</code> and <code>filters2</code> thereafter.\nEverything else looks good to me.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>My answer above works for <code>theta = k * pi/4;</code>, with other angles, based on your paper,</p>\n\n<pre><code>x = i*cos(theta) - j*sin(theta);\ny = i*sin(theta) + j*cos(theta);\nfx = exp(-(x^2)/(2*sigmaq))*exp(sqrt(-1)*x*cos(theta));\ngy = exp(-(G^2 * y^2)/(2*sigmaq))*exp(sqrt(-1)*y*sin(theta));\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Python CV Webcam - Null Argument To Internal Routine",
        "input": "",
        "output": "<p>Sounds like something that would require digging into the source of PyCV (or whatever the interface to OpenCV is called) to fix. My tip to you would be to go to the OpenCV IRC-channel and ask or/and file a bug report.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to detect if an image is a texture or a pattern-based image?",
        "input": "",
        "output": "<p>I cannot open your first image. I implemented the Fourier transform on your second one, and you can see frequency responses at specific points:</p>\n\n<p><img src=\"https://i.sstatic.net/4M9Bd.jpg\" alt=\"enter image description here\"></p>\n\n<p>You can further process the image by extract the local maximum of the magnitude, and they share the same distance to the center (zero frequency). This may be considered as repetitive patterns.</p>\n\n<p>Regarding the case that patterns share major similarity instead of repetitive feature, it is hard to tell whether the frequency magnitude still has such evident response. It depends on how the pattern looks like. </p>\n\n<p>Another possible approach is the auto-correlation on your image.</p>\n",
        "system": ""
    },
    {
        "instruction": "HOG features MATLAB visualization code",
        "input": "",
        "output": "<p>If you looking for visualizing HOG, you can have a look here, <a href=\"http://web.mit.edu/vondrick/ihog/#code\" rel=\"nofollow\">http://web.mit.edu/vondrick/ihog/#code</a></p>\n\n<p>It was recently published in iccv 2013</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV cascade-training using images bounded/outlined by irregular polygons?",
        "input": "",
        "output": "<p>Data sets annotated with LabelMe are used for many different purposes.  Some of them, like image segmentation, require tight boundaries, rather than bounding boxes.</p>\n\n<p>On the other hand, the cascade classifier in OpenCV is designed to classify rectangular image regions.  It is then used as part of a sliding-window object detector, which also works with bounding boxes.</p>\n\n<p>Whether tight boundaries help improve object detection is an interesting question.  There is evidence that the background pixels caught by the bounding box actually help the classification.</p>\n",
        "system": ""
    },
    {
        "instruction": "Getting different gray-level values from a matrix (Gray-level image)",
        "input": "",
        "output": "<p>The <code>unique</code> function does exactly what you are looking for.</p>\n",
        "system": ""
    },
    {
        "instruction": "Perspective distortions correction",
        "input": "",
        "output": "<p>The usual approach, which assumes that the document's page is approximately flat in 3D space, is to warp the quadrangle encompassing the page into a rectangle. To do so you must estimate a <a href=\"http://en.wikipedia.org/wiki/Homography_%28computer_vision%29\" rel=\"nofollow\">homography</a>, i.e. a (linear) projective transformation between the original image and its warped counterpart. </p>\n\n<p>The estimation requires matching points (or lines) between the two images, and a common choice for documents is to map the page corners in the original images to the image corners of the warped image. This will in general produce a rectangle with an incorrect aspect ratio (i.e. the warped page will look \"wider\" or \"taller\" than the real one), but this can be easily corrected if you happen to know in advance what the real aspect ratio is (for example, because you know the type of paper used, whether letter, A4, etc.). </p>\n\n<p>A simple algorithm to perform the estimation is the so-called <a href=\"http://en.wikipedia.org/wiki/Direct_linear_transformation\" rel=\"nofollow\">Direct Linear Transformation</a>.</p>\n\n<p>The OpenCV library contains routines to help accomplishing all these tasks, look into it.</p>\n",
        "system": ""
    },
    {
        "instruction": "Face recognition with a small number of samples",
        "input": "",
        "output": "<p>Actually, for giving you a proper answer, I'd be happy to know some details of your task and your data. Face Recognition is a non-trivial problem and there is no general solution for all sorts of image acquisition.</p>\n<p>First of all, you should define how many sources of variation (posing, emotions, illumination, occlusions or time-lapse) you have in your sample and testing sets. Then you should choose an appropriate algorithm and, very importantly, preprocessing steps according to the types.</p>\n<p>If you don't have any significant variations, then it is a good idea to consider for a small training set one of the <a href=\"https://ieeexplore.ieee.org/document/5605541\" rel=\"nofollow noreferrer\">Discrete Orthogonal Moments</a> as a feature extraction method. They have a very strong ability to extract features without redundancy. Some of them (Hahn, Racah moments) can also work in two modes - local and global feature extraction. The topic is relatively new, and there are still few articles about it. Although, they are thought to become a very powerful tool in Image Recognition. They can be computed in near real-time by using recurrence relationships. For more information, have a look <a href=\"https://ieeexplore.ieee.org/document/941859\" rel=\"nofollow noreferrer\">here</a> and <a href=\"http://www.researchgate.net/publication/5606888_Image_analysis_by_Krawtchouk_moments/file/72e7e52a11c6fd7f2b.pdf\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>If the pose of the individuals significantly varies, you may try to perform firstly pose correction by <a href=\"https://link.springer.com/chapter/10.1007/978-3-540-79547-6_51\" rel=\"nofollow noreferrer\">Active Appearance Model</a>.</p>\n<p>If there are lots of occlusions (glasses, hats) then using one of the <a href=\"https://arxiv.org/ftp/arxiv/papers/0907/0907.4984.pdf\" rel=\"nofollow noreferrer\">local feature extractors</a> may help.</p>\n<p>If there is a significant time lapse between train and probe images, the local features of the faces could change over the age, then it's a good option to try one of the algorithms which use <a href=\"https://ieeexplore.ieee.org/document/5634496\" rel=\"nofollow noreferrer\">graphs for face representation</a> so as to keep the face topology.</p>\n<p>I believe that non of the above are implemented in OpenCV, but for some of them you can find <a href=\"https://www.mathworks.com/matlabcentral/fileexchange/26706-active-shape-model-asm-and-active-appearance-model-aam\" rel=\"nofollow noreferrer\">MATLAB implementation</a>.</p>\n<p>I'm not native speaker as well, so sorry for the grammar</p>\n",
        "system": ""
    },
    {
        "instruction": "Haar training - where to obtain eyeglasses images?",
        "input": "",
        "output": "<p>If you simply want images, it looks like @herhuyongtao pointed you to a good place. Then you can follow opencv's <a href=\"http://docs.opencv.org/doc/user_guide/ug_traincascade.html\" rel=\"nofollow\">tutorial on training</a>.</p>\n\n<p>Another option is to see what others have trained:\nThere's a trained data set found <a href=\"http://www.myexperiment.org/files/422.html\" rel=\"nofollow\">here</a> that might be of use, which states simply that it is \"better\". I'm assuming that it's supposed to be better than opencv. </p>\n\n<p>I didn't immediately see any other places for trained or labeled data. </p>\n",
        "system": ""
    },
    {
        "instruction": "metrics for feature detection/extraction methods",
        "input": "",
        "output": "<p>It is very hard to estimate feature detectors <em>per se</em>, because features are only computation artifacts and not things that you are actually searching in images. Feature detectors do not make sense outside their intended context, which is <em>affine-invariant image part matching</em> for the descriptors that you have mentioned.</p>\n\n<p>The very first usage of SIFT, SURF, MSER was multi-view reconstruction and automatic 3D reconstruction pipe-lines. Thus, these features are usually assessed from the quality of the 3D reconstrucution or image part matching that they provide. Roughly speaking, you have a pair of images that are related by a known transform (an affinity or an homography) and you measure the difference between the estimated homography (from the feature detector) and the real one.\nThis is also the method used in the blog post that you quote by the way.</p>\n\n<p>In order to assess the practical interest of a detector (and not only its precision in an ideal multi-view pipe-line) some additional measurements of <em>stability</em> (under geometric and photometric changes) were added: does the number of detected features vary, does the quality of the estimated homography vary, etc.</p>\n\n<p>Accidentally, it happens that these detectors may also work (also it was not their design purpose) for object detection and track (in tracking-by-detection cases). In this case, their performance is classically evaluated from more-or-less standardized image datasets, and typically expressed in terms of precision (probability of good answer, linked to the false alarm rate) and recall (probability of finding an object when it is present). You can read for example <a href=\"http://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"nofollow\">Wikipedia</a> on this topic.</p>\n\n<p><strong>Addendum: What exactly do I mean by accidentally?</strong></p>\n\n<p>Well, as written above, SIFT and the like were designed to match <em>planar</em> and <em>textured</em> image parts. This is why you always see example with similar images from a <a href=\"http://lear.inrialpes.fr/people/mikolajczyk/Database/index.html\" rel=\"nofollow\">dataset of graffiti</a>.</p>\n\n<p>Their extension to detection and tracking was then developed in two different ways:</p>\n\n<ul>\n<li>While doing multiview matching (with a spherical rig), Furukawa and Ponce built some kind of 3D locally-planar object model, that they applied then to <a href=\"http://www.di.ens.fr/willow/pdfs/ijcv06b.pdf\" rel=\"nofollow\">object detection</a> in presence of severe occlusions. This worlk exploits the fact that an interesting object is often locally planar and textured;</li>\n<li>Other people developed a less original (but still efficient in good conditions) approach by considering that they had a query image of the object to track. Individual frame detections are then performed by matching (using SIFT, etc.) the template image with the current frame. This exploits the fact that there are few false matchings with SIFT, that objects are usually observed in a distance (hence are usually almost planar in images) and that they are textured. See for example <a href=\"http://www.dis.uniroma1.it/~nardi/Didattica/SAI/matdid/tracking/SIFT.pdf\" rel=\"nofollow\">this paper</a>.</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "Detect angle of text in image with horizontal and rotate in matlab",
        "input": "",
        "output": "<p>First, let's find x-y coordinates of all the dark pixels</p>\n\n<pre><code>bw = imread('http://i.imgur.com/0LxC6bd.png');\nbw = min( bw, [], 3 ) &lt; 50 ; % dark pixels - intensity lower than 50\n[y x] = find( bw ); % note that find returns row-col coordinates.\n</code></pre>\n\n<p>Compute the covariance matrix of the text coordinates</p>\n\n<pre><code>mx = mean(x);\nmy = mean(y);\nC = [ mean( (x-mx).^2 ),     mean( (x-mx).*(y-my) );...\n      mean( (x-mx).*(y-my) ) mean( (y-my).^2 ) ];\n</code></pre>\n\n<hr>\n\n<p>You can get the orientation of the ellipse from the eigen vectors and eigen values of <code>C</code>:</p>\n\n<pre><code>[V D] = eig( C );\nfigure; imshow( bw ); hold on;\nquiver( mx([1 1]), my([1 1]), (V(1,:)*D), (V(2,:)*D), .05 );  \n</code></pre>\n\n<p>Looking at the eigenvectors and eigen values:</p>\n\n<pre><code>V =\n-0.9979   -0.0643\n-0.0643    0.9979\n\nD = \n1.0e+003 *\n0.1001         0\n     0    1.3652\n</code></pre>\n\n<p>You can see that the eigen-vectors (columns of <code>V</code>) are approximately the pointing to the <code>-X</code> direction (first column) and the <code>Y</code> direction (second column). Examining the eigen values (diagonal of <code>D</code>) you can see that the second eigen value is much larger than the first - this is the major axis of your ellipse. Now you can recover the orientation of the ellipse:</p>\n\n<pre><code>[~, mxi] = max(diag(D)); % find major axis index: largest eigen-value\n</code></pre>\n\n<p>Recover the angle from the corresponding eigen-vector</p>\n\n<pre><code>or = atan2( V(2,mxi), V(1,mxi) ) * 180/pi ; % convert to degrees for readability\nor =\n93.6869\n</code></pre>\n\n<p>As you can see the major axis of the ellipse is almost 90deg off the horizon.\nYou can rotate the image back</p>\n\n<pre><code>imrotate( bw, -or );\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/KDfd6.png\" alt=\"enter image description here\"> </p>\n\n<hr>\n\n<p>Drawing an ellipse given the covariance matrix:</p>\n\n<pre><code>th = linspace(0, 2*pi, 500 );\nxy = [cos(th);sin(th)];\nRR = chol( C ); % cholesky decomposition\nexy = xy'*RR; %//'\nfigure;imshow( bw ); hold on;\nplot( 2*exy(:,1)+mx, 2*exy(:,2)+my, 'r', 'LineWidth', 2 );\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/52uQJ.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Iterate within bins of an histogram using EmguCV (C#)",
        "input": "",
        "output": "<p>You simply have to use the <a href=\"http://www.emgu.com/wiki/files/2.4.0/document/html/d3a9f67f-f4cc-c543-210e-1e01af36fcc1.htm\" rel=\"nofollow\">Item property</a> of the DenseHistogram.</p>\n",
        "system": ""
    },
    {
        "instruction": "Opencv createsamples utility: do tight bounding boxes substitute uncropped images for positive samples?",
        "input": "",
        "output": "<p>You don't need to resize or crop your samples but you need to use an annotation tool to create a list of positive samples in the following format:</p>\n\n<pre><code>path_to\\sample1.png 1 10 10 20 22\npath_to\\sample2.png 2 10 10 20 22 40 40 30 33\n...\n</code></pre>\n\n<p>That takes care of all of the issues your were concerned about. Depending on your samples, creating this text file can become time consuming.</p>\n\n<p>Too much background inside the bounding box of your positive samples may affect the effectiveness of the weak classifiers in your model, but it may or may not be important in the final cascade of classifiers model. All you should be concerned about is to create good positive samples where the object is captured inside the bounding box as precisely as possible. It means: </p>\n\n<ol>\n<li>having each sample in the same relative location to the top-right corner of its corresponding bounding box, and </li>\n<li>having same object width/bounding box width ratio for all samples. </li>\n</ol>\n\n<p>In other words, try centering all objects in your bounding box and add the same \"percentage\" of padding for all of them so that when they are cropped and resized by createsamples into a vec file, all of them look similar in locations and size.</p>\n",
        "system": ""
    },
    {
        "instruction": "Extract all bounding boxes using OpenCV Python",
        "input": "",
        "output": "<p>there you go:</p>\n\n<pre><code>import cv2\n\nim = cv2.imread('c:/data/ph.jpg')\ngray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\ncontours, hierarchy = cv2.findContours(gray,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\nidx =0 \nfor cnt in contours:\n    idx += 1\n    x,y,w,h = cv2.boundingRect(cnt)\n    roi=im[y:y+h,x:x+w]\n    cv2.imwrite(str(idx) + '.jpg', roi)\n    #cv2.rectangle(im,(x,y),(x+w,y+h),(200,0,0),2)\ncv2.imshow('img',im)\ncv2.waitKey(0)    \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Training a Haar cascade took very little time",
        "input": "",
        "output": "<p>I'm probably late with my answer. But for the sake of others I'll post it anyway;</p>\n\n<p>First, I'm astonished that the training took so little time. Especially since your samples are very large. 80x40 is really HUGE! Myself I usually train with samples around 20 pixels for the largest dimension. In your case 22x11 would be fine. With increasing sample size the training time increases almost square-wise.</p>\n\n<p>Next, 2000 positives should work quite well. That is, if they are not too much alike. It's better to have samples with as big a difference as you would want to detect in action. Remove samples that are very much alike. They have a negative impact on the quality of the detector.</p>\n\n<p>Then, 3000 negatives is a bit on the low side. Around 6000 negatives gives me good results. But there again, it's better to have negatives that are very different. It's also better to have negatives with a lot of detail. Pictures of a bright blue sky won't work very well. If you can't get that many negatives easily, you can also present rotated versions of the ones you have.</p>\n\n<p>Finally, the more I think of it, the more I'm convinced that you did something wrong. 20 stages for 2000 samples of 80x40 in just 34 minutes?? No way! Do you really have all 2000 positives in that vector file?</p>\n\n<p>Regards and good luck!</p>\n",
        "system": ""
    },
    {
        "instruction": "What is consistency map (confidence map)?",
        "input": "",
        "output": "<p><strong>Confidence map</strong> is your resultant knowledge, using a priori information &amp; and an iteration of an algorithm. Prediction &amp; measurement update style visual tracking algorithms are great examples. From wiki (mean-shift):</p>\n\n<blockquote>\n  <p>The mean shift algorithm can be used for visual tracking. The simplest\n  such algorithm would create a confidence map in the new image based on\n  the color histogram of the object in the previous image, and use mean\n  shift to find the peak of a confidence map near the object's old\n  position. The confidence map is a probability density function on the\n  new image, assigning each pixel of the new image a probability, which\n  is the probability of the pixel color occurring in the object in the\n  previous image. A few algorithms, such as ensemble tracking,\n  CAMshift, expand on this idea.</p>\n</blockquote>\n\n<p>As far as I know, the term <strong>consistency map,</strong> on the other hand, is used in image registration problems, in general. A voxel needs to be \"photo-consistent\" to be rendered(modeled) in 3d. From wiki again (photo-consistency):</p>\n\n<blockquote>\n  <p>In computer vision photo-consistency determines whether a given voxel\n  is occupied. A voxel is considered to be photo consistent when its\n  color appears to be similar to all the cameras that can see it. Most\n  voxel coloring or space carving techniques require using photo\n  consistency as a check condition in Image-based modeling and rendering\n  applications.</p>\n</blockquote>\n\n<p><strong>EDIT:</strong> OpenCV has built-in kalman filter, mean-shift and camshift algorithms. It also has a <a href=\"http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html\" rel=\"nofollow\">3D rendering API</a></p>\n",
        "system": ""
    },
    {
        "instruction": "How calculate histogram OPENCV using mask?",
        "input": "",
        "output": "<p>The mask you create needs to be <code>uint8</code> type , so when creating the mask make it uint8, and then pass it to compute the histogram.</p>\n\n<pre><code>mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n</code></pre>\n\n<p>and now compute histogram by passing original image, and the curresponding mask.</p>\n\n<pre><code>hist_item = cv2.calcHist([image],[ch],mask,[256],[0,255])\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Haar-cascade training took very little time and no xml was produced",
        "input": "",
        "output": "<p>See my answer to <a href=\"https://stackoverflow.com/questions/21093268/training-a-haar-cascade-took-very-little-time/21093695#21093695\">your other question</a>.  If no xml file was produced, it is very likely that you have run out of positive samples.  Try using 1500 instead of 2000.  </p>\n\n<p>Better yet, check out <a href=\"http://www.mathworks.com/help/vision/ug/train-a-cascade-object-detector.html\" rel=\"nofollow noreferrer\">trainCascadeObjectDetector</a>, a function in the Computer Vision System Toolbox for Matlab, which lets you generate an xml file compatible with OpenCV.</p>\n",
        "system": ""
    },
    {
        "instruction": "Calculate epipolar line from essential matrix",
        "input": "",
        "output": "<p>there is a point line correspondence, so for any given point in image 1 your can compute the epipolar line corresponding to that point by the equation you have mentioned. I am not sure what do you mean by transformation between two cameras, but if you know the fundamental matrix, you don't need any information about the intrinsic parameters of the cameras for a point line corerspondence.</p>\n\n<p>you cannot compute the camera matrix for the second camera with just the fundamental matrix </p>\n",
        "system": ""
    },
    {
        "instruction": "Best supervised learning algorithm for small data",
        "input": "",
        "output": "<p>Some possible attempts:</p>\n\n<ol>\n<li>Some simple clustering methods such as k-means;</li>\n<li>Logistic regression;</li>\n<li>Increase the feature number then use linear SVM (do not use other kernels).</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "What&#39;s the best way to fit a set of points in an image one or more good lines using RANSAC using OpenCV?",
        "input": "",
        "output": "<p>RANSAC is not the most efficient but it is better for a large number of outliers. Here is how to do it using opencv:</p>\n\n<p>A useful structure-</p>\n\n<pre><code>struct SLine\n{\n    SLine():\n        numOfValidPoints(0),\n        params(-1.f, -1.f, -1.f, -1.f)\n    {}\n    cv::Vec4f params;//(cos(t), sin(t), X0, Y0)\n    int numOfValidPoints;\n};\n</code></pre>\n\n<p>Total Least squares used to make a fit for a successful pair</p>\n\n<pre><code>cv::Vec4f TotalLeastSquares(\n    std::vector&lt;cv::Point&gt;&amp; nzPoints,\n    std::vector&lt;int&gt; ptOnLine)\n{\n    //if there are enough inliers calculate model\n    float x = 0, y = 0, x2 = 0, y2 = 0, xy = 0, w = 0;\n    float dx2, dy2, dxy;\n    float t;\n    for( size_t i = 0; i &lt; nzPoints.size(); ++i )\n    {\n        x += ptOnLine[i] * nzPoints[i].x;\n        y += ptOnLine[i] * nzPoints[i].y;\n        x2 += ptOnLine[i] * nzPoints[i].x * nzPoints[i].x;\n        y2 += ptOnLine[i] * nzPoints[i].y * nzPoints[i].y;\n        xy += ptOnLine[i] * nzPoints[i].x * nzPoints[i].y;\n        w += ptOnLine[i];\n    }\n\n    x /= w;\n    y /= w;\n    x2 /= w;\n    y2 /= w;\n    xy /= w;\n\n    //Covariance matrix\n    dx2 = x2 - x * x;\n    dy2 = y2 - y * y;\n    dxy = xy - x * y;\n\n    t = (float) atan2( 2 * dxy, dx2 - dy2 ) / 2;\n    cv::Vec4f line;\n    line[0] = (float) cos( t );\n    line[1] = (float) sin( t );\n\n    line[2] = (float) x;\n    line[3] = (float) y;\n\n    return line;\n}\n</code></pre>\n\n<p>The actual RANSAC</p>\n\n<pre><code>SLine LineFitRANSAC(\n    float t,//distance from main line\n    float p,//chance of hitting a valid pair\n    float e,//percentage of outliers\n    int T,//number of expected minimum inliers \n    std::vector&lt;cv::Point&gt;&amp; nzPoints)\n{\n    int s = 2;//number of points required by the model\n    int N = (int)ceilf(log(1-p)/log(1 - pow(1-e, s)));//number of independent trials\n\n    std::vector&lt;SLine&gt; lineCandidates;\n    std::vector&lt;int&gt; ptOnLine(nzPoints.size());//is inlier\n    RNG rng((uint64)-1);\n    SLine line;\n    for (int i = 0; i &lt; N; i++)\n    {\n        //pick two points\n        int idx1 = (int)rng.uniform(0, (int)nzPoints.size());\n        int idx2 = (int)rng.uniform(0, (int)nzPoints.size());\n        cv::Point p1 = nzPoints[idx1];\n        cv::Point p2 = nzPoints[idx2];\n\n        //points too close - discard\n        if (cv::norm(p1- p2) &lt; t)\n        {\n            continue;\n        }\n\n        //line equation -&gt;  (y1 - y2)X + (x2 - x1)Y + x1y2 - x2y1 = 0 \n        float a = static_cast&lt;float&gt;(p1.y - p2.y);\n        float b = static_cast&lt;float&gt;(p2.x - p1.x);\n        float c = static_cast&lt;float&gt;(p1.x*p2.y - p2.x*p1.y);\n        //normalize them\n        float scale = 1.f/sqrt(a*a + b*b);\n        a *= scale;\n        b *= scale;\n        c *= scale;\n\n        //count inliers\n        int numOfInliers = 0;\n        for (size_t i = 0; i &lt; nzPoints.size(); ++i)\n        {\n            cv::Point&amp; p0 = nzPoints[i];\n            float rho      = abs(a*p0.x + b*p0.y + c);\n            bool isInlier  = rho  &lt; t;\n            if ( isInlier ) numOfInliers++;\n            ptOnLine[i]    = isInlier;\n        }\n\n        if ( numOfInliers &lt; T)\n        {\n            continue;\n        }\n\n        line.params = TotalLeastSquares( nzPoints, ptOnLine);\n        line.numOfValidPoints = numOfInliers;\n        lineCandidates.push_back(line);\n    }\n\n    int bestLineIdx = 0;\n    int bestLineScore = 0;\n    for (size_t i = 0; i &lt; lineCandidates.size(); i++)\n    {\n        if (lineCandidates[i].numOfValidPoints &gt; bestLineScore)\n        {\n            bestLineIdx = i;\n            bestLineScore = lineCandidates[i].numOfValidPoints;\n        }\n    }\n\n    if ( lineCandidates.empty() )\n    {\n        return SLine();\n    }\n    else\n    {\n        return lineCandidates[bestLineIdx];\n    }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "How to change the homography with the scale of the image?",
        "input": "",
        "output": "<p>The two answers are wrong! Sorry!</p>\n\n<p>The right answer with proof is:</p>\n\n<p><a href=\"https://i.sstatic.net/N2aXW.gif\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/N2aXW.gif\" alt=\"enter image description here\"></a></p>\n",
        "system": ""
    },
    {
        "instruction": "Template Matching with tolerance in OpenCV",
        "input": "",
        "output": "<p>You've identified the major limitation with template matching. It's very fragile to any deformation of the image. Template-matching works by sliding a template-sized box around the image, and checking the similarity between the template and the region inside the box. It checks similarity using a pixel-by-pixel comparison method, such as normalized cross-correlation. If you want to allow different sizes and rotations, you'll need to write a loop that scales the original template up or down, or rotates it. It gets really inefficient.</p>\n\n<p>If you want to allow deformation, and also do a more efficient search at different scales and rotations, the standard method is SURF. It's very efficient, and quite accurate if your images have good resolution, which yours do. You can google tutorials and find sample code for finding objects using SURF. Basically SURF identifies keypoints (distinctive image regions) in the template and the image. Then, you find the region in the image with the largest number of keypoints which match the template. (If you're already doing this, and it's what you meant by \"feature matching,\" then I think you're on the right track.)</p>\n",
        "system": ""
    },
    {
        "instruction": "How to obtain part scores in Object Detection with Discriminatively Trained Part Based Models",
        "input": "",
        "output": "<p>I figured out how to do this, at the end of gdetect_parse.m, add the following lines, the bounding boxes (x1, x2, y1, y2) would correspond to the boxes in <strong>bs</strong> and the scores would be in the second last row in trees{i}</p>\n\n<p>node_id = 8; //ensure that it is a leaf node, i.e second row of trees{i} should be 1</p>\n\n<p>tree_id = 1; //one of the detection trees</p>\n\n<p>scale = (model.sbin/pyra.scales(trees{tree_id}(8, node_id)));</p>\n\n<p>x1 = (trees{tree_id}(6, node_id) - 1 - pyra.padx*(2^trees{tree_id}(9, node_id)))*scale + 1</p>\n\n<p>y1 = (trees{tree_id}(7, node_id) - 1 - pyra.pady*(2^trees{tree_id}(9, node_id)))*scale + 1</p>\n\n<p>filter_id = model.symbols(trees{1}(3, node_id)).filter;</p>\n\n<p>fx = model.filters(filter_id).size(2);</p>\n\n<p>fy = model.filters(filter_id).size(1);</p>\n\n<p>x2 = x1 + fx*scale - 1</p>\n\n<p>y2 = y1 + fy*scale - 1</p>\n",
        "system": ""
    },
    {
        "instruction": "How to use the onCameraFrame function efficiently by not processing every single frame to increase the fps",
        "input": "",
        "output": "<p>I know it's late, but if it can help I have some hypothesis:</p>\n\n<ul>\n<li>you might need to decrease your image size even further;</li>\n<li>you might need to limit the number of features you match to a lower threshold;</li>\n<li>you might need to convert some code to NEON to run in real-time, or to GPU;</li>\n<li>you might need to change the algorithms you are using, decreasing cyclomatic complexity;</li>\n<li>FREAK is not fast enough for real-time mobile, so I would go with something like DAISY;</li>\n<li>don't know if you activated compiler optimizations when you built the app, but they are definitely needed (if you use C++ at least).</li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "splitting R G B image and using blob detection to detect red shapes in Open cv and C++",
        "input": "",
        "output": "<p>There are multiple problems: </p>\n\n<p>your blob detector parameters don't fit. <code>.minArea</code> is much too big for the sample image you provided in the comment, since the blobs are very small. <code>.blobColor</code> is a problem since your red isn't perfect, see the image of the red channel I added at bottom. In addition, yellow is combination of red and green and might have a very high red value too. So <code>.filterByColor=false</code> might be better if you don't know the value of your red dots.</p>\n\n<p>Here's the red channel, you will see that the red dots have a lower red value than the yellow dots and values are about the same as the values of the blue dots! In the third image, only one of the red dots is clearly visible to me as a human. On the fourth image, the dot next to the eye isn't really visible. All in all it looks like red channel isn't distinct enough to locate the red dots on human skin.</p>\n\n<p>red channel:</p>\n\n<p><img src=\"https://i.sstatic.net/aq5qg.png\" alt=\"enter image description here\"></p>\n\n<p>I've tried HSV color space, but the hue value isn't really better, but in saturation channel at least all the dots are a bit visible:</p>\n\n<p>hue:</p>\n\n<p><img src=\"https://i.sstatic.net/OWWOK.png\" alt=\"enter image description here\"></p>\n\n<p>saturation:</p>\n\n<p><img src=\"https://i.sstatic.net/LjJXo.png\" alt=\"enter image description here\"></p>\n\n<p>value:</p>\n\n<p><img src=\"https://i.sstatic.net/v2uuC.png\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "How do I detect multiple people and track them in opencv/emgucv",
        "input": "",
        "output": "<p>that's a quite hard problem and there is no out-of-the-box solution, so you might have to try different methods.</p>\n\n<p>In the beginning you will want to make some assumptions like static camera position and everything that's not background is a person or part of a person, maybe multiple persons. Persons can't appear within the image but they will have to 'enter' it (and are detected on entering and tracked after detection).</p>\n\n<p>Detection and tracking can both be difficult problems so you might want to focus on one of them first. I would start with tracking and choose a <code>probabilisic tracking method</code>, since simple tracking methods like <code>tracking by detection</code> probably can't handle overlap and multiple targets very well.</p>\n\n<p><code>Tracking:</code>\nI would try a particle filter, like <a href=\"http://www.irisa.fr/vista/Papers/2002/perez_hue_eccv02.pdf\" rel=\"nofollow\">http://www.irisa.fr/vista/Papers/2002/perez_hue_eccv02.pdf</a>\nwhich is capable to track multiple targets.</p>\n\n<p><code>Detection:</code> There is a HoG Person Detector in OpenCV which works quite fine for upright persons </p>\n\n<pre><code>HOGDescriptor hog;\nhog.setSVMDetector(HOGDescriptor::getDefaultPeopleDetector());\n</code></pre>\n\n<p>but it's good to know the approximate size of a person in the image and scale the image accordingly. You can do this after background subtraction by scaling the blobs or combination of blobs, or you use a calibration of your camera and scale image parts of size 1.6m to 2.0m to your HoG detector size. Otherwise you might have many misses and many false alarms.</p>\n\n<p>In the end you will have to work and research for some time to get the things running, but don't expect early success or 100% hit rates ;)</p>\n\n<p>I would create a sample video and work on that, manually masking entering people as detection and implement the tracker with those detections.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Sobel Filter with ksize=1",
        "input": "",
        "output": "<p>The Sobel operator is separable:</p>\n\n<pre><code>[-1 0 1      [1\n -2 0 2  =    2  * [-1 0 1]\n -1 0 1]      1]\n</code></pre>\n\n<p>Given an image <code>A</code>, </p>\n\n<pre><code>Gx = [1 2 1]'*([-1 0 1] * A);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Converting iOS code to Android, OpenCV",
        "input": "",
        "output": "<p>I got solution to my own question. I have found the answer from question given at <a href=\"https://stackoverflow.com/questions/17898480/object-detection-with-opencv-feature-matching-with-a-threshold-similarity-score/20994113#20994113\">Object detection with OpenCV Feature Matching with a threshold/similarity score - Java/C++</a></p>\n\n<p>with required changes as i have mentioned in that post. check it out.</p>\n",
        "system": ""
    },
    {
        "instruction": "Camera suggestion for Machine Vision with OpenCV",
        "input": "",
        "output": "<p>It was a little late for you. But it will help others who want to develop a Machine Vision project. Also you are right about there are not a lot of detailed resource about OpenCV and Machine Vision.</p>\n\n<p>First of all a web-cam will be an amateur solution for commercial applications. If you want to make an industrial application for inspection, you need a special illumination, and an industrial camera. You can work with video or trigger mode on camera. Also think about strobe mode illumination can give better results on a production line. An example camera model which I bought, is PointGrey. Some models can be triggered outside(you can use a sensor for trigger):<br>\n<a href=\"http://ww2.ptgrey.com/camera-applications\" rel=\"noreferrer\">http://ww2.ptgrey.com/camera-applications</a><br>\n<a href=\"http://www.baslerweb.com/Cameras-42173.html\" rel=\"noreferrer\">http://www.baslerweb.com/Cameras-42173.html</a><br>\nThere are also other smart cameras which don't need any PC or illumination like:\n<a href=\"http://www.teledynedalsa.com/imaging/products/vision-systems/cameras/-boa-overview\" rel=\"noreferrer\">http://www.teledynedalsa.com/imaging/products/vision-systems/cameras/-boa-overview</a>\n<a href=\"http://www.datalogic.com/eng/products/industrial-automation/machine-vision/a30-series-pd-549.html\" rel=\"noreferrer\">http://www.datalogic.com/eng/products/industrial-automation/machine-vision/a30-series-pd-549.html</a>\n<a href=\"http://www.microscan.com/en-us/Products.aspx\" rel=\"noreferrer\">http://www.microscan.com/en-us/Products.aspx</a><br></p>\n\n<p>All camera manufacturers supplies their own SDKs. It shows how you can get frame raw data and convert it to any format like bitmap, OpenCV data format, save as jpg, etc.\nSome examples for PointGrey:<br>\n<a href=\"http://www.ptgrey.com/products/pgrflycapture/samples.asp\" rel=\"noreferrer\">http://www.ptgrey.com/products/pgrflycapture/samples.asp</a><br>\nIt is easy to convert frame raw data to OpenCV image format, you should copy raw data to OpenCV image data. You can use memcpy. Some examples here:<br><a href=\"http://www.technical-recipes.com/2011/integrating-the-flycapture-sdk-for-use-with-opencv\" rel=\"noreferrer\">http://www.technical-recipes.com/2011/integrating-the-flycapture-sdk-for-use-with-opencv</a> <br><a href=\"http://kevinhughes.ca/tutorials/point-grey-blackfly-and-opencv\" rel=\"noreferrer\">http://kevinhughes.ca/tutorials/point-grey-blackfly-and-opencv</a> <br>\nFor all other camera manifacturers you can use similar way.\nAlso I suggest to prefer GigE interface cameras, they are becoming standard.</p>\n\n<p>Good luck.</p>\n",
        "system": ""
    },
    {
        "instruction": "unable to read an AVI file in OpenCV",
        "input": "",
        "output": "<p>Problem must be with the ffmpeg codec. there must be a mismatch between the binaries that causes a variable called icvCreateFileCapture_FFMPEG_p to be null, and logical check on this variable terminates the function before trying to read the avi file</p>\n",
        "system": ""
    },
    {
        "instruction": "How to draw a transparent image over live camera feed in opencv",
        "input": "",
        "output": "<p>If your overlay image has an alpha channel (and assuming the images are of the same size) you can do something like that</p>\n\n<pre><code>cv::Mat display_img( src.size(), src.type() );\nfor (int y = 0; y &lt; src.rows; y++)\n{\n    const cv::Vec3b* src_pixel = src.ptr&lt;cv::Vec3b&gt;(y);\n    const cv::Vec4b* ovl_pixel = overlay.ptr&lt;cv::Vec4b&gt;(y);\n    cv::Vec3b* dst_pixel = display_img.ptr&lt;cv::Vec3b&gt;(y);\n    for (int x = 0; x &lt; src.cols; x++, ++src_pixel, ++ovl_pixel, ++dst_pixel)\n    {\n        double alpha = (*ovl_pixel).val[3] / 255.0;\n        for (int c = 0; c &lt; 3; c++)\n        {\n            (*dst_pixel).val[c] = (uchar) ((*ovl_pixel).val[c] * alpha + (*src_pixel).val[c] * (1.0 -alpha));\n        }\n    }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Detection of position/angle of landing platform to steer a drone",
        "input": "",
        "output": "<p>Here is a <strong>pseudo code</strong>, assuming you already have full access to the motor control API; i.e. you have successfully defined what is needed for changing altitude, rotating left etc. </p>\n\n<pre><code>loop\n{\n    if(landing board detected)\n    {\n        if(circle including the center point detected)\n        {\n            find orientation from corner circles' center\n            change device's orientation accordingly\n        }\n        else\n        {\n            lose altitude &amp; move towards the center point\n        }\n    }\n    else\n    {\n        move around\n    }\n}\n</code></pre>\n\n<p><strong>Landing board &amp; its center:</strong></p>\n\n<p>Assumption: It is the biggest &amp; nearly perfect square.</p>\n\n<p>1- Threshold</p>\n\n<p>2- Extract contours</p>\n\n<p>3- Apply shape (square) filter to contours</p>\n\n<p>4- Find the biggest contour</p>\n\n<p>5- Find its center</p>\n\n<p>6- Crop the image with the bounding rect of this contour</p>\n\n<pre><code>Mat image = imread(\"~\\\\image.jpg\");\n\n// scale down for faster processing\npyrDown(image, image);\npyrDown(image, image);\n// safe copy\nMat temp = image.clone();\n// noise reduction &amp; thresholding\nGaussianBlur(image, image, Size(5,5), 3);\ncvtColor(image, image, CV_BGR2GRAY);\nthreshold(image, image, 127, 255, CV_THRESH_OTSU);\n\n// extract all contours \nvector&lt;vector&lt;Point&gt; &gt; contours;\nfindContours(image, contours, CV_RETR_LIST, CV_CHAIN_APPROX_NONE);\n\n// define a perfect square\nvector&lt;Point&gt; square;\nsquare.push_back(Point(0,0));\nsquare.push_back(Point(0,10));\nsquare.push_back(Point(10,10));\nsquare.push_back(Point(10,0));\n\n// filter out contours that are not square\nbool erased;\nfor(unsigned int i = 0; i&lt;contours.size(); i++)\n{\n    erased = false;\n    double x = matchShapes(contours[i], square, CV_CONTOURS_MATCH_I2, 0);\n    if(x &gt; 0.005)\n    {\n        contours.erase(contours.begin() + i);\n        erased = true;\n    }\n    if(erased) i--;\n}\n\n// area filtering to find the biggest square contour\nvector&lt;double&gt; contourAreas(contours.size());\nfor(unsigned int i = 0; i&lt;contours.size(); i++)\n{\n    contourAreas[i] = contourArea(contours[i]);\n}\nint ID = max_element(contourAreas.begin(), contourAreas.end()) - contourAreas.begin();  \nfor(unsigned int i = 0; i&lt;contours.size(); i++)\n{\n    erased = false;\n    if(i != ID)\n    {\n        contours.erase(contours.begin() + i);\n        erased = true;\n        ID--;\n    }\n    if(erased) i--;\n}\n\n// find the bounding rect of this contour and crop the image within that rect\nvector&lt;Point&gt; total;\nfor(unsigned int j = 0; j&lt;contours[0].size(); j++)\n{\n    total.push_back(contours[0][j]);\n}\nRect rect = boundingRect(total);\n\nMat t = Mat(temp, rect);\n\n// find the center of the landing board - to move towards it when necessary\nMoments m = moments(contours[0], false);\nPoint center = Point(cvRound(m.m10/m.m00), cvRound(m.m01/m.m00));\n</code></pre>\n\n<p>Now that we have detected the board, we need to detect the <strong>corner circles</strong> for orientation.</p>\n\n<p>1- Threshold</p>\n\n<p>2- Extract contours</p>\n\n<p>3- Apply shape (circular) filter to contours</p>\n\n<p>4- Filter out circles close to the center of the board</p>\n\n<p>5- Resultant circles are the corner circles, find the center of their biggest</p>\n\n<pre><code>// threshold\nMat gray;\ncvtColor(t, gray, CV_BGR2GRAY); \nthreshold(gray, gray, 2187451321, 12186471, CV_THRESH_OTSU);\n\n// extract contours\nvector&lt;vector&lt;Point&gt; &gt; conts;\nfindContours(gray, conts, CV_RETR_LIST, CV_CHAIN_APPROX_NONE);\n\n// circularity check\nfor(unsigned int i = 0; i&lt;conts.size(); i++)\n{\n    erased = false;\n    if(4*3.14*contourArea(conts[i]) / ((arcLength(conts[i],true) * arcLength(conts[i],true))) &lt; 0.85)\n    {\n        conts.erase(conts.begin() + i);\n        erased = true;\n    }\n    if(erased) i--;\n}\n\n// position check - filtering out center circle\nvector&lt;Moments&gt; mu(conts.size());\nvector&lt;Point2f&gt; mc(conts.size());\nfor(unsigned int i = 0; i&lt;conts.size(); i++ )\n{ \n    mu[i] = moments(conts[i], false); \n}\nfor(unsigned int i = 0; i &lt;conts.size(); i++ )\n{\n    mc[i] = Point2f(mu[i].m10/mu[i].m00 , mu[i].m01/mu[i].m00); \n}\nfor(unsigned int i=0; i&lt;conts.size(); i++)\n{\n    erased = false;\n    if((((int)mc[i].x &gt; t.cols/3) &amp;&amp; ((int)mc[i].x &lt; 2*t.cols/3) &amp;&amp; ((int)mc[i].y &lt; 2*t.rows/3) &amp;&amp; ((int)mc[i].y &gt; t.rows/3)))\n    {\n        mc.erase(mc.begin() + i);\n        conts.erase(conts.begin() + i);\n        erased = true;\n    }\n    if(erased) i--;\n}\n\n// selecting the biggest circle \nvector&lt;double&gt; contAreas(conts.size());\nfor(unsigned int i = 0; i&lt;conts.size(); i++)\n{\n    contAreas[i] = contourArea(conts[i]);\n}\nID = max_element(contAreas.begin(), contAreas.end()) - contAreas.begin();   \nfor(unsigned int i = 0; i&lt;conts.size(); i++)\n{\n    erased = false;\n    if(i != ID)\n    {\n        conts.erase(conts.begin() + i);\n        erased = true;\n        ID--;\n    }\n    if(erased) i--;\n}\n\ndrawContours(t, conts, -1, Scalar(0,255,255));\n\n// finding its center - this is nothing but current orientation\nMoments m2 = moments(conts[0], false);\nPoint c = Point(cvRound(m2.m10/m2.m00), cvRound(m2.m01/m2.m00));\n</code></pre>\n\n<p><strong>input image</strong>\n<img src=\"https://i.sstatic.net/QE7FZ.jpg\" alt=\"input image\"></p>\n\n<p><strong>detected biggest-square</strong> (<code>Mat t</code>)</p>\n\n<p><img src=\"https://i.sstatic.net/mrAXq.png\" alt=\"detected biggest-square\"></p>\n\n<p><strong>detected biggest-not close to center-circle-inside that biggest square</strong> (<code>conts[0]</code>)\n<img src=\"https://i.sstatic.net/bNReC.png\" alt=\"detected biggest-not close to center-circle-inside that biggest square\"></p>\n\n<p><strong>circle center and board center respectively, for orientation purposes</strong>\n<img src=\"https://i.sstatic.net/4ix1t.png\" alt=\"circle center and board center respectively, for orientation purposes\"></p>\n\n<p>EDIT: Board center (<code>center</code>) is the position according to the <code>image</code> whereas circle center (<code>c</code>) is the position according to the board (<code>t</code>). Only thing left is to find the slope of line that passes through the board center and the circle center.</p>\n",
        "system": ""
    },
    {
        "instruction": "Converting OpenCV code from iOS to Android",
        "input": "",
        "output": "<p>I have found solution to my question, You can check it out the solution on this post:<a href=\"https://stackoverflow.com/questions/17898480/object-detection-with-opencv-feature-matching-with-a-threshold-similarity-score/20994113#20994113\">Object detection with OpenCV Feature Matching with a threshold/similarity score - Java/C++</a></p>\n",
        "system": ""
    },
    {
        "instruction": "difference between classification and detection",
        "input": "",
        "output": "<p>Classification is a process of putting items into different bins.</p>\n\n<p>Detection: Detection is a process of actually finding out about item features</p>\n\n<p>Example: If i asked you to detect people coming into a room, you may have a procedure to do this and that would be detection. If I then ask you to classify them into two groups of age below 25 and above 25 you will have to do that but here most of people get confused that they have to do detection as well. They might be given information about age groups already like a list of people coming in with age and you can view that and classify them or you can use a simple detection by asking them what age they are and then classifying them.</p>\n\n<p>Most of the time in image processing you will see that detection is based on looking at object and its features and detecting those features out for example detecting edges and detecting average colours. This does not classify them however you may have a classification on top of it for example classify an image into black and white and coloured picture (2 bins) or you classify image parts using edges.</p>\n\n<p>Just to reiterate in other words detector is going to do its work till detection and thats it nothing else. How you use that detection is upto you. There are loads of algorithms to help in detection alone and then if you want to classify you can use neural network or simple manual classification.</p>\n\n<p>When you are driving and you are looking out and detecting if there is a hazard out there. It can be a human, animal or any other vehicle etc. But If I then ask you to look out for a boy of average height then your brain would start a simple classification algorithm which would classify people into bins of male and females then reject females and apply average height classifier on males and if it finds a fit then it will trigger a Hurrah.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV- how to unify different contours to a single enclosing contour",
        "input": "",
        "output": "<p>First, do you want to achieve the result only on this image or any other image where different people will present in different pose and different dresses?</p>\n\n<p>If you want to segment only this image, then with some color thresholding or with some morphology operations you can achieve it. But to make it work for any image with different persons probably you may need to pursue a PhD in computer vision.</p>\n\n<p>But if your task is segmentation only then I would suggest a Semi-Automatic Segmentation technique like Grab Cut or graph cut. These are very popular segmentation algorithms which are readily available in opencv or matlab. They work very well on all kind of images. Here is the result of grab cut algorithm on your image.</p>\n\n<p><img src=\"https://i.sstatic.net/8Lqog.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Use the osusvm to recognize faces after the C2 layer in HMAX model",
        "input": "",
        "output": "<p>Your code looks good to me.</p>\n\n<p>Since you have only 2 test images, the possible successful rate will be limited to 0, 0.5, 1. And it is expected to achieve the 100% accuracy with a 25% probability ([0 1],[1 0],[1 1],[0 0]). You can shuffle the data, and re-select 2 from the 8 as the test for some times, then observe the accuracy.</p>\n\n<p>Also try to add <a href=\"http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\" rel=\"nofollow\">more images</a> to both training and test samples.</p>\n",
        "system": ""
    },
    {
        "instruction": "Training images and test images",
        "input": "",
        "output": "<p>The only difference between training and test images is the fact, that test images are not used for <strong>selecting your models parameters</strong>. Each model has some kind of paramters, variables, which it fits to the data. This is called a training process. The training/test set separation ensures, that your model (algorithm) can actually do something more that just memorizing images - so you <strong>test</strong> it on <strong>test</strong> images, which has not been used during the <strong>training</strong> phase.</p>\n\n<p>It has been already discussed in detail on SO: <a href=\"https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ\">whats is the difference between train, validation and test set, in neural networks?</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Best tracking algorithm for multiple colored objects (billiard balls)",
        "input": "",
        "output": "<p>In theory mean shift works well regardless of the dimensionality (in very high dimensions sparseness is a bit of an issues, but there are works that address that problem)</p>\n\n<p>If you are trying to use an off the self mean shift tracker that only takes a single channel input, you can create your own problem specific color channel. You need a single channel that maximizes the difference between the different colored billiard balls.</p>\n\n<p>The easiest way of doing that will be to take the mean colors of all 15 balls and, put them in a <code>15x3</code> matrix and decompose it with <code>SVD</code> (subtract the mean first) so you'll get the axis of maximal variance. This will give you the best linear transformation from RGB to a new one dimensional color space that maximizes difference between the billiard balls colors. (If it isn't good enough you can do better with local mapping, but might not be necessary)</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV haar training with images that have transparency",
        "input": "",
        "output": "<p>As mentioned in the comments above, the haar features are only computed on the grayscale image. This might pose a problem as you mentioned, when the default color of 0 might cause the \"wheels\" to lose contrast. You can probably \"standardize\" the transparent color rather than have it default to 0.</p>\n\n<p>The first thing is you can load in all 4 channels (including your alpha channel) and then use the alpha channel to set the transparent part to a certain value.</p>\n\n<h1>Python version</h1>\n\n<pre><code>I = cv2.imread(\"image.jpg\", cv2.CV_LOAD_IMAGE_UNCHANGED)\nalpha = I[:, :, 3]\nG = cv2.cvtColor(I, cv2.COLOR_BGRA2GRAY)\nG[alpha == 0] = 125  # Set transparent region to 125. Change to suit your needs.\n</code></pre>\n\n<h1>C++</h1>\n\n<pre><code>vector&lt;cv::Mat&gt; channels;\ncv::split(I, channels);\ncv::Mat alpha = channels[3];\nalpha = 255 - alpha;  // Invert mask so we select the transparent regions.\ncv::Mat G = cv::cvtColor(I, cv::COLOR_BGRA2GRAY);\nG.setTo(cv::Scalar(125), alpha);\n</code></pre>\n\n<p>As a note of caution, I think you might have to be careful about some of the operations above, e.g., loading image with alpha and \"alpha = 255 - alpha;\". I believe they are only available only in later versions of OpenCV. I'm using OpenCV 2.4.7 and it works (for the python version. I haven't tried the C++ but it should be the same). So if things don't work, check whether these operations are supported for your version of OpenCV. If not there are ways to get round them.</p>\n",
        "system": ""
    },
    {
        "instruction": "Easiest image processing on Mac",
        "input": "",
        "output": "<p>I would suggest to get openCV and connect it to Xcode. Numerous resources can be found on the web or in book stores.<br>\nSee for example:<br>\n<a href=\"http://georgepavlides.info/opencv-and-macos/\" rel=\"nofollow noreferrer\">OpenCV MacOS installation</a><br/>\n<a href=\"https://rads.stackoverflow.com/amzn/click/com/1782163840\" rel=\"nofollow noreferrer\" rel=\"nofollow noreferrer\">OpenCV and iOS</a><br/>\n<a href=\"http://opencvexperiments.wordpress.com/2012/11/15/opencvonxcode/\" rel=\"nofollow noreferrer\">OpenCV and Xcode</a><br/>\n<a href=\"https://sites.google.com/site/learningopencv1/installing-opencv\" rel=\"nofollow noreferrer\">OpenCV and Xcode</a></p>\n",
        "system": ""
    },
    {
        "instruction": "HSI and HSV color space",
        "input": "",
        "output": "<p>HSI, HSV, and HSL are all different color spaces. Hue computation is (as far as I can find) identical between the three models, and uses a 6-piece piece-wise function to determine it, or for a simpler model that is accurate to within 1.2 degrees, <code>atan((sqrt(3)\u22c5(G-B))/2(R-G-B))</code> can be used.  For the most part, these two are interchangeable, but generally HSV and HSL use the piece-wise model, where HSI usually uses the arctan model. Different equations may be used, but these usually sacrifice precision for either simplicity or faster computation.</p>\n<p>For lightness/value/intensity, the three spaces use slightly different representations.</p>\n<ul>\n<li>Intensity is computed by simply averaging the RGB values: <code>(1/3)\u22c5(R+G+B)</code>.</li>\n<li>Lightness averages the minimum and maximum values for RGB: <code>(1/2)\u22c5(max(R,G,B) + min(R,G,B))</code>.</li>\n<li>Value is the simplest, being the value of the maximum of RGB: <code>max(R,G,B)</code>.</li>\n</ul>\n<p>When used in subsequent calculations, L/V/I is scaled to a decimal between 0 and 1.</p>\n<p>Saturation is where the three models differ the most. For all 3, if I/V/L is 0, then saturation is 0 (this is for black, so that its representation is unambiguous), and HSL additionally sets saturation to 0 if lightness is maximum (because for HSL maximum lightness means white).</p>\n<ul>\n<li>HSL and HSV account for both the minimum and maximum of RGB, taking the difference between the two: <code>max(R,G,B) - min(R,G,B)</code>, this value is sometimes referred to as chroma (C).</li>\n<li>HSV then takes the chroma and divides it by the value to get the saturation: <code>C/V</code>.</li>\n<li>HSL divides chroma by an expression taking lightness into account: <code>C/(1-abs(2L-1))</code>.</li>\n<li>HSI doesn't use chroma explicitly, instead only taking <code>min(R,G,B)</code> into account: <code>1 - min(R,G,B)/I</code>.</li>\n</ul>\n<h2>Sources</h2>\n<ul>\n<li>Wikipedia: <a href=\"http://en.wikipedia.org/wiki/HSL_and_HSV\" rel=\"nofollow noreferrer\">HSL and HSV</a></li>\n<li>Wikipedia: <a href=\"http://en.wikipedia.org/wiki/Hue\" rel=\"nofollow noreferrer\">Hue</a></li>\n</ul>\n",
        "system": ""
    },
    {
        "instruction": "How to convert PixelList to mask?",
        "input": "",
        "output": "<p>Use <a href=\"http://www.mathworks.com/help/matlab/ref/sub2ind.html\" rel=\"nofollow\"><code>sub2ind</code></a> to get a the linear indexes of the non-zero locations and set them in one shot:</p>\n\n<pre><code>pixellist = PixelList{1}; % get the 1447x2 int32 array\nmask = false(dim); % e.g. dim = [256 256]\nmask(sub2ind(size(mask),pixellist(:,2), pixellist(:,1))) = true;\n</code></pre>\n\n<p>Or use <code>accumarray</code> for a more elegant solution:</p>\n\n<pre><code>mask = accumarray(fliplr(pixellist),true,dim,@any,false)\n</code></pre>\n\n<p>This could be reduced to just <code>accumarray(fliplr(pixellist),true,dim)</code> if there are no repeated elements, and the above syntax is more explicit about the output dimensions.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to compute fundamental matrix using a stereo pair?",
        "input": "",
        "output": "<p>The Computer Vision System Toolbox includes a function <a href=\"http://www.mathworks.com/help/vision/ref/estimatefundamentalmatrix.html\" rel=\"nofollow\">estimateFundamentalMatrix</a> that does what you need.  Check out this <a href=\"http://www.mathworks.com/help/vision/examples/stereo-image-rectification.html\" rel=\"nofollow\">example</a> of how to estimate the fundamental matrix, and then use it for stereo rectification.</p>\n",
        "system": ""
    },
    {
        "instruction": "Detect low-quality circles/rings in image with MATLAB",
        "input": "",
        "output": "<p><a href=\"http://en.wikipedia.org/wiki/Hough_transform#Circle_detection_process\" rel=\"nofollow\">Hough</a> transform can be used to detect the shape of objects. You can use <a href=\"http://www.mathworks.com/help/images/ref/imfindcircles.html\" rel=\"nofollow\">Matlab</a> or <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/hough_circle/hough_circle.html\" rel=\"nofollow\">OpenCv</a>. Consider using the relevant <a href=\"http://docs.opencv.org/modules/gpu/doc/image_processing.html\" rel=\"nofollow\">GPU OpenCV</a> if you are familiarized with gpu libs.</p>\n\n<p>Good luck.</p>\n",
        "system": ""
    },
    {
        "instruction": "Software chain to find duplicate images",
        "input": "",
        "output": "<p>You should take the local feature matching approach (SURF/ORB/BRISK...)\nYou can find a nice tutorial here:<a href=\"http://docs.opencv.org/doc/tutorials/features2d/feature_flann_matcher/feature_flann_matcher.html\" rel=\"nofollow\">http://docs.opencv.org/doc/tutorials/features2d/feature_flann_matcher/feature_flann_matcher.html</a>\nIf efficiency is very important, you can replace OpenCV's <code>findHomography</code> with a custom find-rigid-transform code, but if it is not a big issue <code>findHomography</code> will probably serve you well.</p>\n",
        "system": ""
    },
    {
        "instruction": "Problems in reading images in Open CV 2.4.7",
        "input": "",
        "output": "<p>Even though lots of things <em>could</em> be the problem, the most likely issue is that the image could not be found.</p>\n\n<p>You can make sure by giving the complete path to <code>img.jpeg</code> instead of the relative path you have now.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to get depth images from Kinect and SoftKinetic at the same time?",
        "input": "",
        "output": "<p>Check that the two devices aren't trying to run on the same USB hub. To try to resolve the problem, you might try one device on a USB2 port and the other device on a USB3 port.</p>\n\n<p>(Egad this is an old post. Anyway, it will still be helpful to someone.)</p>\n",
        "system": ""
    },
    {
        "instruction": "LPR with MATLAB: how to find only one rectangle?",
        "input": "",
        "output": "<p>Try this !!!</p>\n\n<pre><code>I = imread('http://8pic.ir/images/88146564605446812704.jpg');\nim=rgb2gray(I);\nsigma=1;\nf=zeros(128,128);\nf(32:96,32:96)=255;\n[g3, t3]=edge(im, 'canny', [0.04 0.10], sigma);\nse=strel('rectangle', [1 1]);\nBWimage=imerode(g3,se);\ngg = imclearborder(BWimage,8);\nbw = bwareaopen(gg,200);\ngg1 = imclearborder(bw,26);\nimshow(gg1);\n\n%Dilation\nId = imdilate(gg1, strel('diamond', 1));\nimshow(Id);\n\n%Fill\nIf = imfill(Id, 'holes');\nimshow(If);\n\n%Find Plate\n[lab, n] = bwlabel(If);\n\nregions = regionprops(lab, 'All');\nregionsCount = size(regions, 1) ;\n\nfor i = 1:regionsCount\n    region = regions(i);\n    RectangleOfChoice = region.BoundingBox;\n    PlateExtent = region.Extent;\n\n    PlateStartX = fix(RectangleOfChoice(1));\n    PlateStartY = fix(RectangleOfChoice(2));\n    PlateWidth  = fix(RectangleOfChoice(3));\n    PlateHeight = fix(RectangleOfChoice(4));\n\n   if PlateWidth &gt;= PlateHeight*1 &amp;&amp; PlateExtent &gt;= 0.7\n        im2 = imcrop(I, RectangleOfChoice);\n        %figure, imshow(I);\n        figure, imshow(im2);\n    end\nend\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "estimateGeometricTransform don&#39;t return the status",
        "input": "",
        "output": "<p>There are other optional outputs before <code>status</code>:</p>\n\n<pre><code>[tform,inlierpoints1,inlierpoints2]  = estimateGeometricTransform(...)\n</code></pre>\n\n<p>The way you are calling the function, <code>status</code> actually contains <code>inlierPoints1</code>.</p>\n\n<p>What you have to do to get the status and prevent the function from throwing an error is this:</p>\n\n<pre><code>[tform, ~, ~, status] = estimateGeometricTransform(matchedBoxPoints, matchedScenePoints, ...\n     'projective');\n</code></pre>\n\n<p>A better way is to check the number of points before calling <code>estimateGeometricTransform</code>.</p>\n\n<pre><code>if size(boxPairs, 1) &gt;= 3\n   tform = estimateGeometricTransform(..., 'affine')\nelse\n   % skip this frame\nend\n</code></pre>\n\n<p>For projective, use 4 instead of 3.</p>\n",
        "system": ""
    },
    {
        "instruction": "Page not loading at localhost:8080",
        "input": "",
        "output": "<p>Try calling <code>import webbrowser</code> from the shell. If it fails you need to install the <a href=\"http://docs.python.org/2/library/webbrowser.html\" rel=\"nofollow\">library</a> (<code>pip install</code> or <code>easy_install</code>).</p>\n",
        "system": ""
    },
    {
        "instruction": "How to detect a Christmas Tree?",
        "input": "",
        "output": "<p>I have an approach which I think is interesting and a bit different from the rest.  The main difference in my approach, compared to some of the others, is in how the image segmentation step is performed--I used the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN\">DBSCAN</a> clustering algorithm from Python's scikit-learn; it's optimized for finding somewhat amorphous shapes that may not necessarily have a single clear centroid.</p>\n\n<p>At the top level, my approach is fairly simple and can be broken down into about 3 steps.  First I apply a threshold (or actually, the logical \"or\" of two separate and distinct thresholds).  As with many of the other answers, I assumed that the Christmas tree would be one of the brighter objects in the scene, so the first threshold is just a simple monochrome brightness test; any pixels with values above 220 on a 0-255 scale (where black is 0 and white is 255) are saved to a binary black-and-white image.  The second threshold tries to look for red and yellow lights, which are particularly prominent in the trees in the upper left and lower right of the six images, and stand out well against the blue-green background which is prevalent in most of the photos.  I convert the rgb image to hsv space, and require that the hue is either less than 0.2 on a 0.0-1.0 scale (corresponding roughly to the border between yellow and green) or greater than 0.95 (corresponding to the border between purple and red) and additionally I require bright, saturated colors: saturation and value must both be above 0.7.  The results of the two threshold procedures are logically \"or\"-ed together, and the resulting matrix of black-and-white binary images is shown below:</p>\n\n<p><img src=\"https://i.sstatic.net/iIkWV.png\" alt=\"Christmas trees, after thresholding on HSV as well as monochrome brightness\"></p>\n\n<p>You can clearly see that each image has one large cluster of pixels roughly corresponding to the location of each tree, plus a few of the images also have some other small clusters corresponding either to lights in the windows of some of the buildings, or to a background scene on the horizon.  The next step is to get the computer to recognize that these are separate clusters, and label each pixel correctly with a cluster membership ID number.</p>\n\n<p>For this task I chose <a href=\"http://en.wikipedia.org/wiki/DBSCAN\">DBSCAN</a>.  There is a pretty good visual comparison of how DBSCAN typically behaves, relative to other clustering algorithms, available <a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\">here</a>.  As I said earlier, it does well with amorphous shapes.  The output of DBSCAN, with each cluster plotted in a different color, is shown here:</p>\n\n<p><img src=\"https://i.sstatic.net/rWSqn.png\" alt=\"DBSCAN clustering output\"></p>\n\n<p>There are a few things to be aware of when looking at this result.  First is that DBSCAN requires the user to set a \"proximity\" parameter in order to regulate its behavior, which effectively controls how separated a pair of points must be in order for the algorithm to declare a new separate cluster rather than agglomerating a test point onto an already pre-existing cluster.  I set this value to be 0.04 times the size along the diagonal of each image.  Since the images vary in size from roughly VGA up to about HD 1080, this type of scale-relative definition is critical.</p>\n\n<p>Another point worth noting is that the DBSCAN algorithm as it is implemented in scikit-learn has memory limits which are fairly challenging for some of the larger images in this sample.  Therefore, for a few of the larger images, I actually had to \"decimate\" (i.e., retain only every 3rd or 4th pixel and drop the others) each cluster in order to stay within this limit.  As a result of this culling process, the remaining individual sparse pixels are difficult to see on some of the larger images.  Therefore, for display purposes only, the color-coded pixels in the above images have been effectively \"dilated\" just slightly so that they stand out better.  It's purely a cosmetic operation for the sake of the narrative; although there are comments mentioning this dilation in my code, rest assured that it has nothing to do with any calculations that actually matter.</p>\n\n<p>Once the clusters are identified and labeled, the third and final step is easy: I simply take the largest cluster in each image (in this case, I chose to measure \"size\" in terms of the total number of member pixels, although one could have just as easily instead used some type of metric that gauges physical extent) and compute the convex hull for that cluster.  The convex hull then becomes the tree border.  The six convex hulls computed via this method are shown below in red:</p>\n\n<p><img src=\"https://i.sstatic.net/sl6Ar.jpg\" alt=\"Christmas trees with their calculated borders\"></p>\n\n<p>The source code is written for Python 2.7.6 and it depends on <a href=\"http://www.numpy.org/\">numpy</a>, <a href=\"http://www.scipy.org/\">scipy</a>, <a href=\"http://matplotlib.org/\">matplotlib</a> and <a href=\"http://scikit-learn.org/stable/\">scikit-learn</a>.  I've divided it into two parts.  The first part is responsible for the actual image processing:</p>\n\n<pre><code>from PIL import Image\nimport numpy as np\nimport scipy as sp\nimport matplotlib.colors as colors\nfrom sklearn.cluster import DBSCAN\nfrom math import ceil, sqrt\n\n\"\"\"\nInputs:\n\n    rgbimg:         [M,N,3] numpy array containing (uint, 0-255) color image\n\n    hueleftthr:     Scalar constant to select maximum allowed hue in the\n                    yellow-green region\n\n    huerightthr:    Scalar constant to select minimum allowed hue in the\n                    blue-purple region\n\n    satthr:         Scalar constant to select minimum allowed saturation\n\n    valthr:         Scalar constant to select minimum allowed value\n\n    monothr:        Scalar constant to select minimum allowed monochrome\n                    brightness\n\n    maxpoints:      Scalar constant maximum number of pixels to forward to\n                    the DBSCAN clustering algorithm\n\n    proxthresh:     Proximity threshold to use for DBSCAN, as a fraction of\n                    the diagonal size of the image\n\nOutputs:\n\n    borderseg:      [K,2,2] Nested list containing K pairs of x- and y- pixel\n                    values for drawing the tree border\n\n    X:              [P,2] List of pixels that passed the threshold step\n\n    labels:         [Q,2] List of cluster labels for points in Xslice (see\n                    below)\n\n    Xslice:         [Q,2] Reduced list of pixels to be passed to DBSCAN\n\n\"\"\"\n\ndef findtree(rgbimg, hueleftthr=0.2, huerightthr=0.95, satthr=0.7, \n             valthr=0.7, monothr=220, maxpoints=5000, proxthresh=0.04):\n\n    # Convert rgb image to monochrome for\n    gryimg = np.asarray(Image.fromarray(rgbimg).convert('L'))\n    # Convert rgb image (uint, 0-255) to hsv (float, 0.0-1.0)\n    hsvimg = colors.rgb_to_hsv(rgbimg.astype(float)/255)\n\n    # Initialize binary thresholded image\n    binimg = np.zeros((rgbimg.shape[0], rgbimg.shape[1]))\n    # Find pixels with hue&lt;0.2 or hue&gt;0.95 (red or yellow) and saturation/value\n    # both greater than 0.7 (saturated and bright)--tends to coincide with\n    # ornamental lights on trees in some of the images\n    boolidx = np.logical_and(\n                np.logical_and(\n                  np.logical_or((hsvimg[:,:,0] &lt; hueleftthr),\n                                (hsvimg[:,:,0] &gt; huerightthr)),\n                                (hsvimg[:,:,1] &gt; satthr)),\n                                (hsvimg[:,:,2] &gt; valthr))\n    # Find pixels that meet hsv criterion\n    binimg[np.where(boolidx)] = 255\n    # Add pixels that meet grayscale brightness criterion\n    binimg[np.where(gryimg &gt; monothr)] = 255\n\n    # Prepare thresholded points for DBSCAN clustering algorithm\n    X = np.transpose(np.where(binimg == 255))\n    Xslice = X\n    nsample = len(Xslice)\n    if nsample &gt; maxpoints:\n        # Make sure number of points does not exceed DBSCAN maximum capacity\n        Xslice = X[range(0,nsample,int(ceil(float(nsample)/maxpoints)))]\n\n    # Translate DBSCAN proximity threshold to units of pixels and run DBSCAN\n    pixproxthr = proxthresh * sqrt(binimg.shape[0]**2 + binimg.shape[1]**2)\n    db = DBSCAN(eps=pixproxthr, min_samples=10).fit(Xslice)\n    labels = db.labels_.astype(int)\n\n    # Find the largest cluster (i.e., with most points) and obtain convex hull   \n    unique_labels = set(labels)\n    maxclustpt = 0\n    for k in unique_labels:\n        class_members = [index[0] for index in np.argwhere(labels == k)]\n        if len(class_members) &gt; maxclustpt:\n            points = Xslice[class_members]\n            hull = sp.spatial.ConvexHull(points)\n            maxclustpt = len(class_members)\n            borderseg = [[points[simplex,0], points[simplex,1]] for simplex\n                          in hull.simplices]\n\n    return borderseg, X, labels, Xslice\n</code></pre>\n\n<p>and the second part is a user-level script which calls the first file and generates all of the plots above:</p>\n\n<pre><code>#!/usr/bin/env python\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom findtree import findtree\n\n# Image files to process\nfname = ['nmzwj.png', 'aVZhC.png', '2K9EF.png',\n         'YowlH.png', '2y4o5.png', 'FWhSP.png']\n\n# Initialize figures\nfgsz = (16,7)        \nfigthresh = plt.figure(figsize=fgsz, facecolor='w')\nfigclust  = plt.figure(figsize=fgsz, facecolor='w')\nfigcltwo  = plt.figure(figsize=fgsz, facecolor='w')\nfigborder = plt.figure(figsize=fgsz, facecolor='w')\nfigthresh.canvas.set_window_title('Thresholded HSV and Monochrome Brightness')\nfigclust.canvas.set_window_title('DBSCAN Clusters (Raw Pixel Output)')\nfigcltwo.canvas.set_window_title('DBSCAN Clusters (Slightly Dilated for Display)')\nfigborder.canvas.set_window_title('Trees with Borders')\n\nfor ii, name in zip(range(len(fname)), fname):\n    # Open the file and convert to rgb image\n    rgbimg = np.asarray(Image.open(name))\n\n    # Get the tree borders as well as a bunch of other intermediate values\n    # that will be used to illustrate how the algorithm works\n    borderseg, X, labels, Xslice = findtree(rgbimg)\n\n    # Display thresholded images\n    axthresh = figthresh.add_subplot(2,3,ii+1)\n    axthresh.set_xticks([])\n    axthresh.set_yticks([])\n    binimg = np.zeros((rgbimg.shape[0], rgbimg.shape[1]))\n    for v, h in X:\n        binimg[v,h] = 255\n    axthresh.imshow(binimg, interpolation='nearest', cmap='Greys')\n\n    # Display color-coded clusters\n    axclust = figclust.add_subplot(2,3,ii+1) # Raw version\n    axclust.set_xticks([])\n    axclust.set_yticks([])\n    axcltwo = figcltwo.add_subplot(2,3,ii+1) # Dilated slightly for display only\n    axcltwo.set_xticks([])\n    axcltwo.set_yticks([])\n    axcltwo.imshow(binimg, interpolation='nearest', cmap='Greys')\n    clustimg = np.ones(rgbimg.shape)    \n    unique_labels = set(labels)\n    # Generate a unique color for each cluster \n    plcol = cm.rainbow_r(np.linspace(0, 1, len(unique_labels)))\n    for lbl, pix in zip(labels, Xslice):\n        for col, unqlbl in zip(plcol, unique_labels):\n            if lbl == unqlbl:\n                # Cluster label of -1 indicates no cluster membership;\n                # override default color with black\n                if lbl == -1:\n                    col = [0.0, 0.0, 0.0, 1.0]\n                # Raw version\n                for ij in range(3):\n                    clustimg[pix[0],pix[1],ij] = col[ij]\n                # Dilated just for display\n                axcltwo.plot(pix[1], pix[0], 'o', markerfacecolor=col, \n                    markersize=1, markeredgecolor=col)\n    axclust.imshow(clustimg)\n    axcltwo.set_xlim(0, binimg.shape[1]-1)\n    axcltwo.set_ylim(binimg.shape[0], -1)\n\n    # Plot original images with read borders around the trees\n    axborder = figborder.add_subplot(2,3,ii+1)\n    axborder.set_axis_off()\n    axborder.imshow(rgbimg, interpolation='nearest')\n    for vseg, hseg in borderseg:\n        axborder.plot(hseg, vseg, 'r-', lw=3)\n    axborder.set_xlim(0, binimg.shape[1]-1)\n    axborder.set_ylim(binimg.shape[0], -1)\n\nplt.show()\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Taking mean of images for background subtraction - incorrect results",
        "input": "",
        "output": "<p>You're right that it's almost certainly the rounding errors you get by accumulating scaled greyscale values. There's no reason why it should crash using floating point pixels though, so you should try something like this:</p>\n\n<pre><code>Mat buildModel()\n{\n    if (history.size() == 0)\n        return Mat();\n\n    Mat image_avg = Mat::zeros(height, width, CV_32FC1);\n    double alpha = (1.0 / history.size());\n\n    list&lt;Mat&gt;::iterator it = history.begin();\n\n    while (it != history.end())\n    {\n        Mat image_temp;\n        (*it).convertTo(image_temp, CV_32FC1);\n        image_avg += image_temp;\n        it++;\n    }\n    image_avg *= alpha;\n\n    return image_avg;\n}\n</code></pre>\n\n<p>Depending on what you want to do with the result, you may need to normalize it or rescale it or convert back to greyscale before display etc.</p>\n",
        "system": ""
    },
    {
        "instruction": "Bag of words not correctly labeling responses",
        "input": "",
        "output": "<p>Your logic looks fine to me.</p>\n\n<p>Now I guess you'll have to tweak the various parameters if you want to improve the classification accuracy. This includes the <a href=\"http://docs.opencv.org/modules/features2d/doc/object_categorization.html#bowkmeanstrainer-bowkmeanstrainer\" rel=\"nofollow\">clustering algorithm</a> parameters (such as the vocabulary size, clusters initialization, termination criteria, etc..), the SVM parameters (kernel type, the <code>C</code> coefficient, ..), the local features algorithm used (SIFT, SURF, ..).</p>\n\n<p>Ideally, whenever you want to perform parameter selection, you ought to use <a href=\"http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29\" rel=\"nofollow\">cross-validation</a>. Some methods already have such mechanism embedded (<a href=\"http://docs.opencv.org/modules/ml/doc/support_vector_machines.html#cvsvm-train-auto\" rel=\"nofollow\"><code>CvSVM::train_auto</code></a> for instance), but for the most part you'll have to do this manually...</p>\n\n<p>Finally you should follow general machine learning guidelines; see the whole <a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\" rel=\"nofollow\"><em>bias-variance tradeoff</em></a> <a href=\"http://en.wikipedia.org/wiki/Bias-variance_dilemma\" rel=\"nofollow\">dilemma</a>. The online <a href=\"http://ml-class.org/\" rel=\"nofollow\">Coursera ML class</a> discusses this topic in detail in week 6, and explains how to perform error analysis and use learning curves to decide what to try next (do we need to add more instances, increase model complexity, and so on..).</p>\n\n<p>With that said, I wrote my own version of the code. You might wanna compare it with your code:</p>\n\n<pre><code>% dataset of images\n% I previously saved them as: chair1.jpg, ..., ball1.jpg, ball2.jpg, ...\nd = [\n    dir(fullfile('images','chair*.jpg')) ;\n    dir(fullfile('images','ball*.jpg'))\n];\n\n% local-features algorithm used\ndetector = cv.FeatureDetector('SURF');\nextractor = cv.DescriptorExtractor('SURF');\n\n% extract local features from images\nt = struct();\nfor i=1:numel(d)\n    % load image as grayscale\n    img = imread(fullfile('images', d(i).name));\n    if ~ismatrix(img), img = rgb2gray(img); end\n\n    % extract local features\n    pts = detector.detect(img);\n    feat = extractor.compute(img, pts);\n\n    % store along with class label\n    t(i).img = img;\n    t(i).class = find(strncmp(d(i).name,{'chair','ball'},4));\n    t(i).pts = pts;\n    t(i).feat = feat;\nend\n\n% split into training/testing sets\n% (a better way would be to use cvpartition from Statistics toolbox)\ndisp('Distribution of classes:')\ntabulate([t.class])\ntTrain = t([1:7 11:17]);\ntTest = t([8:10 18:20]);\nfprintf('Number of training instances = %d\\n', numel(tTrain));\nfprintf('Number of testing instances = %d\\n', numel(tTest));\n\n% build visual vocabulary (by clustering training descriptors)\nK = 100;\nbowTrainer = cv.BOWKMeansTrainer(K, 'Attempts',5, 'Initialization','PP');\nclust = bowTrainer.cluster(vertcat(tTrain.feat));\n\nfprintf('Number of keypoints detected = %d\\n', numel([tTrain.pts]));\nfprintf('Codebook size = %d\\n', K);\n\n% compute histograms of visual words for each training image\nbowExtractor = cv.BOWImgDescriptorExtractor('SURF', 'BruteForce');\nbowExtractor.setVocabulary(clust);\nM = zeros(numel(tTrain), K);\nfor i=1:numel(tTrain)\n    M(i,:) = bowExtractor.compute(tTrain(i).img, tTrain(i).pts);\nend\nlabels = vertcat(tTrain.class);\n\n% train an SVM model (perform paramter selection using cross-validation)\nsvm = cv.SVM();\nsvm.train_auto(M, labels, 'SvmType','C_SVC', 'KernelType','RBF');\ndisp('SVM model parameters:'); disp(svm.Params)\n\n% evaluate classifier using testing images\nactual = vertcat(tTest.class);\npred = zeros(size(actual));\nfor i=1:numel(tTest)\n    descs = bowExtractor.compute(tTest(i).img, tTest(i).pts);\n    pred(i) = svm.predict(descs);\nend\n\n% report performance\ndisp('Confusion matrix:')\nconfusionmat(actual, pred)\nfprintf('Accuracy = %.2f %%\\n', 100*nnz(pred==actual)./numel(pred));\n</code></pre>\n\n<p>Here are the output:</p>\n\n<pre><code>Distribution of classes:\n  Value    Count   Percent\n      1       10     50.00%\n      2       10     50.00%\nNumber of training instances = 14\nNumber of testing instances = 6\n\nNumber of keypoints detected = 6300\nCodebook size = 100\n\nSVM model parameters:\n         svm_type: 'C_SVC'\n      kernel_type: 'RBF'\n           degree: 0\n            gamma: 0.5063\n            coef0: 0\n                C: 312.5000\n               nu: 0\n                p: 0\n    class_weights: []\n        term_crit: [1x1 struct]\n\nConfusion matrix:\nans =\n     3     0\n     1     2\nAccuracy = 83.33 %\n</code></pre>\n\n<p>So the classifier correctly labels 5 out of 6 images from the test set, which is not bad for a start :) Obviously you'll get different results each time you run the code due to the inherent randomness of the clustering step.</p>\n",
        "system": ""
    },
    {
        "instruction": "crop the required area with point in hand",
        "input": "",
        "output": "<p>would that work if you connect blue 2 and blue 3, blue 3 and blue 4? That will segment the middle 2 fingers. Then draw a line passing blue 4 and parallel with the green line passing blue 5, and another line passing blue 3 parallel with the green line passing blue 3</p>\n",
        "system": ""
    },
    {
        "instruction": "3D reconstruction, matlab",
        "input": "",
        "output": "<p>Are the two images taken with the same camera?</p>\n\n<p>Question A: what you are looking for is point correspondences between the two images. One way to find corresponding points is to use local feature matching. There are many algorithms for detecting interest points and finding feature desriptors, such as SIFT, SURF, BRISK, FREAK, etc.</p>\n\n<p>Question B: You can get the 3D points using <a href=\"http://en.wikipedia.org/wiki/Triangulation_%28computer_vision%29\" rel=\"nofollow\">triangulation</a>.  Also see Direct Linear Transformation in <em>Multiple View Geometry in computer vision</em> by Hartley and Zisserman.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to count near circular objects in image",
        "input": "",
        "output": "<p>I can give you a starting point:</p>\n\n<pre><code>v=double(img)/255;\nmask = v(:,:,3)+v(:,:,2)-v(:,:,1)&lt;0;\nmask = imopen(mask,strel('square',3));\nimagesc(min(1,v+cat(3,mask*0,mask*1,mask*0)));axis image\n</code></pre>\n\n<p>The overlay image looks like:<img src=\"https://i.sstatic.net/O9lb8.jpg\" alt=\"enter image description here\"></p>\n\n<p>use RANSAC (as suggested by AdrienNK) on the mask and you'll get the location (and count) of your tomatoes ;-)</p>\n\n<p>-O-</p>\n",
        "system": ""
    },
    {
        "instruction": "How to &quot;translate&quot; the movement of the camera to the image?",
        "input": "",
        "output": "<p>Knowing how much the camera has moved is not enough for creating a synthesized frame. For that you'll need the 3D model of the world as well, which I assume you don't have.</p>\n\n<p>To demonstrate that assume the camera movement is pure translation and you are looking at two objects, one is very far - a few kilometers away and the other is very close - a few centimeters away. The very far object will hardly move in the new frame, while the very close one can move dramatically or even disappear from the field of view of the second frame, you need to know how much the viewing angle has changed for each point and for that you need the 3D model.</p>\n\n<p>Having sensor information may help in the case of rotation but it is not as useful for translations.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to select one specific frame",
        "input": "",
        "output": "<p>Assuming that you not only get a binary detection result (<em>\"there is a car\"</em>) but also some kind of spatial information (<em>\"there is a car, and its bounding box is ...\"</em>) then you can simply keep the frame that shows the most.</p>\n\n<p>Something like this</p>\n\n<pre><code>best_frame = None\nbest_frame_score = 0.0\nfor frame in video:\n    has_car, score = detect_car(frame)\n    if has_car and score &gt; best_frame_score:\n        best_frame = frame\n        best_frame_score = score\n</code></pre>\n\n<p>This assumes that the function <code>detect_car</code> returns a binary detection result and some score. The score could for example be the size of the bounding box.</p>\n",
        "system": ""
    },
    {
        "instruction": "Rotation around an arbitrary point using transformation matrix",
        "input": "",
        "output": "<p>You shouldn't swap your 1st and 3rd matrix, as you are right-multiplying the point which is a column vector, so the sequence is not left to right, instead it is right to left. </p>\n\n<p>You can easily verify the result by extending the right hand matrix, which will be exact the equations representing p'x and p'y.</p>\n",
        "system": ""
    },
    {
        "instruction": "which algorithm to choose for object detection?",
        "input": "",
        "output": "<p>I would suggest using color as the main feature to look for, and only try other features as needed. The fire extinguisher red is very distinctive, and should not occur too often elsewhere in an office environment. Other, more computationally expensive tests can then be performed only in regions of the right color.</p>\n\n<p>Here is a good <a href=\"http://opencv-srf.blogspot.de/2010/09/object-detection-using-color-seperation.html\" rel=\"nofollow\">tutorial for color detection</a> that also explains how to find good thresholds for your desired color.</p>\n\n<p>I would suggest the following approach:</p>\n\n<ul>\n<li>denoise your image with a median filter</li>\n<li>convert the image to HSV format (Hue, Saturation, Value)</li>\n<li>select pixels close to that particular shade of red with InRange()<br>\nNow you have a binary image image that contains only the pixels that are red.</li>\n<li>count the number of red pixels with CountNonZero()<br>\n<ul>\n<li>If that number is too small, abort </li>\n</ul></li>\n<li>remove noise from the binary image by morphological opening / closing</li>\n<li>find contours of all blobs in your picture with findContours or the CvBlob library</li>\n<li>check if there are blobs of the correct width, correct height and correct width/height ratio<br>\nsince your fire extinguishers are vertical cylinders, the width/height ratio will be constant from every angle. The width and height will of course vary somewhat with distance to the camera.\n<ul>\n<li>if the width and height do not match, abort</li>\n</ul></li>\n<li>repeat these steps to find the black-colored part on the bottom of the extinguisher, \n<ul>\n<li>abort if there is no black region with correct width/height below the red region</li>\n</ul></li>\n<li>(perhaps also repeat these steps for the metallic top and the yellow rectangle)</li>\n</ul>\n\n<p>These tests should all be very fast. If they are too slow, you could reduce the resolution of your input images.</p>\n\n<p>Depending on your environment, it is possible that this is already a robust enough test. If not, you can proceed with sift/surf feature matching, but only in a small region around the blobs with the correct color. You also do not necessarily have to do that for each frame, each n-th frame should be be enough for confirmation.</p>\n",
        "system": ""
    },
    {
        "instruction": "How do I get rid of the outliers in this Point array?",
        "input": "",
        "output": "<p>Here is a method that should work for all grid sizes:</p>\n\n<p>Count the number of dots in each row, and the number of dots in each column.<br>\nThen remove all dots in rows and columns that have less than the average number of dots.</p>\n\n<p>You can construct such a histogram efficiently with a dictionary:</p>\n\n<pre><code>var numDotsPerRow = new Dictionary&lt;int,int&gt;();\nvar numDotsPerColumn = new Dictionary&lt;int,int&gt;();\n\nforeach (var point in pointArray)\n{\n    int count;\n    numDotsPerRow.TryGetValue(point.X, out count);\n    numDotsPerRow[point.X] = count+1;\n    numDotsPerColumn.TryGetValue(point.Y, out count);\n    numDotsPerColumn[point.Y] = count+1;\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Approximating convex in image processing",
        "input": "",
        "output": "<p>I am afraid that I cannot get good results from the image after your processing (the 2nd image link). I tried to use a slightly different method to extract the red light in your original image. Hopefully it can help you a little bit.\nFirst of all, you may need to convert the RGB to the <a href=\"http://en.wikipedia.org/wiki/Lab_color_space\" rel=\"nofollow noreferrer\">Lab space</a>. Actually it is very suitable to handle the image in that space, and some operations can maximize the difference between red and green light. Since there is only a red light in your image, I won't go that complicated. I only used b-channel for further processing.</p>\n\n<pre><code>file='https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-prn1/1524779_10153642995125182_414999862_n.jpg';\nI=imread(file);Irgb=I;\n\ncolorTransform = makecform('srgb2lab');\nI = applycform(I, colorTransform);\nb = I(:,:,3);\nI=double(b);\nImin=min(I(:));\nImax=max(I(:));\nI=(I-Imin)/(Imax-Imin);\nimshow(I)\n</code></pre>\n\n<p>Will get you:</p>\n\n<p><img src=\"https://i.sstatic.net/qSlcj.jpg\" alt=\"enter image description here\"></p>\n\n<p>Next, we remove the line/square structure, indeed they have very strong intensity that may impact our further result:</p>\n\n<pre><code>se = strel('line',20,0);\nI = imtophat(I,se);\nfigure,imshow(I)\n</code></pre>\n\n<p>You will have:</p>\n\n<p><img src=\"https://i.sstatic.net/9ywUS.jpg\" alt=\"enter image description here\"></p>\n\n<p>In the next step we are trying to find out the local maximum value in the image:</p>\n\n<pre><code>roi=20;thresh=0.5;\nlocal_extr = ordfilt2(I, roi^2, ones(roi)); \n% Get local maxima and reject candidates below a threshold\nresult = (I == local_extr) &amp; (I &gt; thresh);\n% Get indices of extrema\n[r, c] = find(result);\n% Show them\nfigure;imshow(I, []); hold on;\nplot(c, r, 'r.');hold off\n</code></pre>\n\n<p>You will see:</p>\n\n<p><img src=\"https://i.sstatic.net/fIDvZ.jpg\" alt=\"enter image description here\"></p>\n\n<p>In fact the red light has already been tagged (I saw another red point in your original image, but I assume that is a walking sign with the red hand shape instead of a circle red light, so herein I won't consider that part.)</p>\n\n<p>Finally we use the <code>regionprops</code> with the <code>eccentricity</code> property, and find out the region with the least eccentricity, that will look more alike a circle shape:</p>\n\n<pre><code>for i=1:length(c)\n\n    I1=I(r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi);\n    I1=im2bw(I1);\n\n    labeledImage = bwlabel(I1);\n    blobMeasurements = regionprops(labeledImage,'eccentricity'); \n    if isempty(blobMeasurements)\n        e(i)=Inf;\n    else\n        e(i)=blobMeasurements.Eccentricity;\n    end\n\nend\n[a,idx]=min(e);\n\nres = zeros(size(I,1),size(I,2),3); \ni=idx;\nres( r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi ,1) = Irgb(r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi,1);\nres( r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi ,2) = Irgb(r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi,2);\nres( r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi ,3) = Irgb(r(i)-roi:r(i)+roi,c(i)-roi:c(i)+roi,3);\nfigure,imshow(uint8(res))\n</code></pre>\n\n<p>The result:</p>\n\n<p><img src=\"https://i.sstatic.net/447Jg.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "Cross validation is very slow in Grid search (libsvm)",
        "input": "",
        "output": "<p>This is simply expensive task to find a good model. Lets do some calculations:</p>\n\n<p><code>62 classes x 5 folds x 4 values of C x 4 values of Gamma = 4960 SVMs</code></p>\n\n<p>You can always reduce the number of folds, which will decrease the quality of the search, but will reduce the whole amount of trained SVMs of about <code>40%</code>. </p>\n\n<p>The most expensive part is the fact, that SVM is not well suited for multi label classification. It needs to train at least <code>O(log n)</code> models (in the error correcting code scenario), <code>O(n)</code> (in libsvm one-vs-all) to even <code>O(n^2)</code> (in one-vs-one scenario, which achieves the best results).</p>\n\n<p>Maybe it would be more valuable to switch to some fast multilabel model? Like for example some ELM (Extreme Learning Machine)?</p>\n",
        "system": ""
    },
    {
        "instruction": "Issues with imgIdx in DescriptorMatcher mexopencv",
        "input": "",
        "output": "<p>I think it is easier to explain with code, so here it goes :)</p>\n\n<pre><code>%% init\ndetector = cv.FeatureDetector('ORB');\nextractor = cv.DescriptorExtractor('ORB');\nmatcher = cv.DescriptorMatcher('BruteForce-Hamming');\n\nurls = {\n    'http://i.imgur.com/8Pz4M9q.jpg?1'\n    'http://i.imgur.com/1aZj0MI.png?1'\n    'http://i.imgur.com/pYepuzd.jpg?1'\n};\n\nN = numel(urls);\ntrain = struct('img',cell(N,1), 'pts',cell(N,1), 'feat',cell(N,1));\n\n%% training\nfor i=1:N\n    % read image\n    train(i).img = imread(urls{i});\n    if ~ismatrix(train(i).img)\n        train(i).img = rgb2gray(train(i).img);\n    end\n\n    % extract keypoints and compute features\n    train(i).pts = detector.detect(train(i).img);\n    train(i).feat = extractor.compute(train(i).img, train(i).pts);\n\n    % add to training set to match against\n    matcher.add(train(i).feat);\nend\n% build index\nmatcher.train();\n\n%% testing\n% lets create a distorted query image from one of the training images\n% (rotation+shear transformations)\nt = -pi/3;    % -60 degrees angle\ntform = [cos(t) -sin(t) 0; 0.5*sin(t) cos(t) 0; 0 0 1];\nimg = imwarp(train(3).img, affine2d(tform));    % try all three images here!\n\n% detect fetures in query image\npts = detector.detect(img);\nfeat = extractor.compute(img, pts);\n\n% match against training images\nm = matcher.match(feat);\n\n% keep only good matches\n%hist([m.distance])\nm = m([m.distance] &lt; 3.6*min([m.distance]));\n\n% sort by distances, and keep at most the first/best 200 matches\n[~,ord] = sort([m.distance]);\nm = m(ord);\nm = m(1:min(200,numel(m)));\n\n% naive classification (majority vote)\ntabulate([m.imgIdx])    % how many matches each training image received\nidx = mode([m.imgIdx]);\n\n% matches with keypoints belonging to chosen training image\nmm = m([m.imgIdx] == idx);\n\n% estimate homography (used to locate object in query image)\nptsQuery = num2cell(cat(1, pts([mm.queryIdx]+1).pt), 2);\nptsTrain = num2cell(cat(1, train(idx+1).pts([mm.trainIdx]+1).pt), 2);\n[H,inliers] = cv.findHomography(ptsTrain, ptsQuery, 'Method','Ransac');\n\n% show final matches\nimgMatches = cv.drawMatches(img, pts, ...\n    train(idx+1).img, train(idx+1).pts, ...\n    mm(logical(inliers)), 'NotDrawSinglePoints',true);\n\n% apply the homography to the corner points of the training image\n[h,w] = size(train(idx+1).img);\ncorners = permute([0 0; w 0; w h; 0 h], [3 1 2]);\np = cv.perspectiveTransform(corners, H);\np = permute(p, [2 3 1]);\n\n% show where the training object is located in the query image\nopts = {'Color',[0 255 0], 'Thickness',4};\nimgMatches = cv.line(imgMatches, p(1,:), p(2,:), opts{:});\nimgMatches = cv.line(imgMatches, p(2,:), p(3,:), opts{:});\nimgMatches = cv.line(imgMatches, p(3,:), p(4,:), opts{:});\nimgMatches = cv.line(imgMatches, p(4,:), p(1,:), opts{:});\nimshow(imgMatches)\n</code></pre>\n\n<p>The result:</p>\n\n<p><img src=\"https://i.sstatic.net/MW76A.png\" alt=\"object_detection\"></p>\n\n<p>Note that since you did not post any testing images (in your code you are taking input from the webcam), I created one by distorting one the training images, and using it as a query image. I am using functions from certain MATLAB toolboxes (<code>imwarp</code> and such), but those are non-essential to the demo and you could replace them with equivalent OpenCV ones...</p>\n\n<p>I must say that this approach is not the most robust one.. Consider using other techniques such as the <a href=\"http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision\" rel=\"nofollow noreferrer\">bag-of-word model</a>, which OpenCV already <a href=\"http://docs.opencv.org/modules/features2d/doc/object_categorization.html\" rel=\"nofollow noreferrer\">implements</a>. </p>\n",
        "system": ""
    },
    {
        "instruction": "Improve gaussian mixture model",
        "input": "",
        "output": "<p>looking from the \"press Esc to exit application\" words, I am going to make an assumption that your application is real time? If so, have you considered real time background substraction?</p>\n\n<p>More details about background subtraction can be found here: <a href=\"http://web.bii.a-star.edu.sg/~chengli/BkgSbt.htm\" rel=\"nofollow\">http://web.bii.a-star.edu.sg/~chengli/BkgSbt.htm</a>.</p>\n\n<p>To my knowledge, this should be sufficient to track and extract the hand. But it's not that robust, for instance, considering the user keep bobbing his head, or shifting it left and right. (the user head will be then considered a foreground object instead of background)</p>\n\n<p>But if you are not going to do something so complicated like your question posed, then if the user just gonna sit there not moving while the hand comes into frame, then I believe you will find the background subtraction method extremely useful. Cheers.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV Template Matching: Restrict search area",
        "input": "",
        "output": "<pre><code>roi = image[y:y+h , x:x+w]\n\ncv2.matchTemplate(roi, templ, ...\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Letter inside letter, pattern recognition",
        "input": "",
        "output": "<p><strong>And here we go!</strong> A <strong>high level overview</strong> of this approach can be described as the sequential execution of the following steps:</p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/a/16109992/176769\">Load the input image</a>;</li>\n<li><a href=\"https://stackoverflow.com/a/8408961/176769\">Convert it to grayscale</a>;</li>\n<li><a href=\"https://stackoverflow.com/a/9471551/176769\">Threshold</a> it to generate a binary image;</li>\n<li>Use the binary image to <a href=\"https://stackoverflow.com/a/9334267/176769\">find contours</a>;</li>\n<li><a href=\"https://stackoverflow.com/questions/19966423/filling-in-a-single-colour-background-in-opencv/19989729#19989729\">Fill each area of contours</a> with a different color (so we can extract each letter separately);</li>\n<li><a href=\"https://stackoverflow.com/a/20288616/176769\">Create a mask for each letter found to isolate them in separate images</a>;</li>\n<li><a href=\"https://stackoverflow.com/a/10317919/176769\">Crop the images to the smallest possible size</a>;</li>\n<li>Figure out the center of the image;</li>\n<li>Figure out the width of the letter's border to identify the exact center of the border;</li>\n<li>Scan along the border (in a circular fashion) for discontinuity;   </li>\n<li>Figure out an approximate angle for the discontinuity, thus identifying the amount of rotation of the letter.</li>\n</ul>\n\n<p>I don't want to get into too much detail since I'm sharing the source code, so feel free to test and change it in any way you like. \nLet's start, <em>Winter Is Coming</em>:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;cmath&gt;\n\n#include &lt;opencv2/highgui/highgui.hpp&gt;\n#include &lt;opencv2/imgproc/imgproc.hpp&gt;\n\ncv::RNG rng(12345);\nfloat PI = std::atan(1) * 4;\n\nvoid isolate_object(const cv::Mat&amp; input, cv::Mat&amp; output)\n{    \n    if (input.channels() != 1)\n    {\n        std::cout &lt;&lt; \"isolate_object: !!! input must be grayscale\" &lt;&lt; std::endl;\n        return;\n    }\n\n    // Store the set of points in the image before assembling the bounding box\n    std::vector&lt;cv::Point&gt; points;\n    cv::Mat_&lt;uchar&gt;::const_iterator it = input.begin&lt;uchar&gt;();\n    cv::Mat_&lt;uchar&gt;::const_iterator end = input.end&lt;uchar&gt;();\n    for (; it != end; ++it)\n    {\n        if (*it) points.push_back(it.pos());\n    }\n\n    // Compute minimal bounding box\n    cv::RotatedRect box = cv::minAreaRect(cv::Mat(points));\n\n    // Set Region of Interest to the area defined by the box\n    cv::Rect roi;\n    roi.x = box.center.x - (box.size.width / 2);\n    roi.y = box.center.y - (box.size.height / 2);\n    roi.width = box.size.width;\n    roi.height = box.size.height;\n\n    // Crop the original image to the defined ROI\n    output = input(roi);\n}\n</code></pre>\n\n<p>For more details on the implementation of <code>isolate_object()</code> please <a href=\"https://stackoverflow.com/a/10317919/176769\">check this thread</a>. <code>cv::RNG</code> is used later on to <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/find_contours/find_contours.html\" rel=\"nofollow noreferrer\">fill each contour with a different color</a>, and <code>PI</code>, well... you know <strong>PI</strong>.</p>\n\n<pre><code>int main(int argc, char* argv[])\n{   \n    // Load input (colored, 3-channel, BGR)\n    cv::Mat input = cv::imread(\"test.jpg\");\n    if (input.empty())\n    {\n        std::cout &lt;&lt; \"!!! Failed imread() #1\" &lt;&lt; std::endl;\n        return -1;\n    }\n\n    // Convert colored image to grayscale\n    cv::Mat gray;\n    cv::cvtColor(input, gray, CV_BGR2GRAY);\n\n    // Execute a threshold operation to get a binary image from the grayscale\n    cv::Mat binary;\n    cv::threshold(gray, binary, 128, 255, cv::THRESH_BINARY); \n</code></pre>\n\n<p>The <strong>binary</strong> image looks exactly like the input because it only had 2 colors (B&amp;W):</p>\n\n<p><img src=\"https://i.sstatic.net/kLosJ.png\" width=\"300\" height=\"200\"></p>\n\n<pre><code>    // Find the contours of the C's in the thresholded image\n    std::vector&lt;std::vector&lt;cv::Point&gt; &gt; contours;\n    cv::findContours(binary, contours, cv::RETR_LIST, cv::CHAIN_APPROX_SIMPLE);\n\n    // Fill the contours found with unique colors to isolate them later\n    cv::Mat colored_contours = input.clone();  \n    std::vector&lt;cv::Scalar&gt; fill_colors;   \n    for (size_t i = 0; i &lt; contours.size(); i++)\n    {\n        std::vector&lt;cv::Point&gt; cnt = contours[i];\n        double area = cv::contourArea(cv::Mat(cnt));        \n        //std::cout &lt;&lt; \"* Area: \" &lt;&lt; area &lt;&lt; std::endl;\n\n        // Fill each C found with a different color. \n        // If the area is larger than 100k it's probably the white background, so we ignore it.\n        if (area &gt; 10000 &amp;&amp; area &lt; 100000)\n        {\n            cv::Scalar color = cv::Scalar(rng.uniform(0, 255), rng.uniform(0,255), rng.uniform(0,255));            \n            cv::drawContours(colored_contours, contours, i, color, \n                             CV_FILLED, 8, std::vector&lt;cv::Vec4i&gt;(), 0, cv::Point());\n            fill_colors.push_back(color);\n            //cv::imwrite(\"test_contours.jpg\", colored_contours);\n        }           \n    }\n</code></pre>\n\n<p>What <strong>colored_contours</strong> looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/76Jqm.png\" width=\"300\" height=\"200\"></p>\n\n<pre><code>    // Create a mask for each C found to isolate them from each other\n    for (int i = 0; i &lt; fill_colors.size(); i++)\n    {\n        // After inRange() single_color_mask stores a single C letter\n        cv::Mat single_color_mask = cv::Mat::zeros(input.size(), CV_8UC1);\n        cv::inRange(colored_contours, fill_colors[i], fill_colors[i], single_color_mask);\n        //cv::imwrite(\"test_mask.jpg\", single_color_mask);\n</code></pre>\n\n<p>Since this <code>for</code> loop is executed twice, one for each color that was used to fill the contours, I want you to see all images that were generated by this stage. So the following images are the ones that were stored by <strong>single_color_mask</strong> (one for each iteration of the loop):</p>\n\n<p><img src=\"https://i.sstatic.net/VPmxs.png\" width=\"300\" height=\"200\"> <img src=\"https://i.sstatic.net/YW1nr.png\" width=\"300\" height=\"200\"></p>\n\n<pre><code>        // Crop image to the area of the object\n        cv::Mat cropped;\n        isolate_object(single_color_mask, cropped);        \n        //cv::imwrite(\"test_cropped.jpg\", cropped);\n        cv::Mat orig_cropped = cropped.clone();\n</code></pre>\n\n<p>These are the ones that were stored by <strong>cropped</strong> (by the way, the smaller C looks fat because the image is rescaled by this page to have the same size of the larger C, don't worry):</p>\n\n<p><img src=\"https://i.sstatic.net/rvYTM.png\" width=\"200\" height=\"200\"> <img src=\"https://i.sstatic.net/p0alB.png\" width=\"200\" height=\"200\"></p>\n\n<pre><code>        // Figure out the center of the image\n        cv::Point obj_center(cropped.cols/2, cropped.rows/2);\n        //cv::circle(cropped, obj_center, 3, cv::Scalar(128, 128, 128));\n        //cv::imwrite(\"test_cropped_center.jpg\", cropped);\n</code></pre>\n\n<p>To make it clearer to understand for what <strong>obj_center</strong> is for, I painted a little gray circle for educational purposes on that location:</p>\n\n<p><img src=\"https://i.sstatic.net/WUgld.png\" width=\"200\" height=\"200\"> <img src=\"https://i.sstatic.net/Qy765.png\" width=\"200\" height=\"200\"> </p>\n\n<pre><code>        // Figure out the exact center location of the border\n        std::vector&lt;cv::Point&gt; border_points;\n        for (int y = 0; y &lt; cropped.cols; y++) \n        {\n            if (cropped.at&lt;uchar&gt;(obj_center.x, y) != 0)\n                border_points.push_back(cv::Point(obj_center.x, y));\n\n            if (border_points.size() &gt; 0 &amp;&amp; cropped.at&lt;uchar&gt;(obj_center.x, y) == 0)\n                break;\n        }\n\n        if (border_points.size() == 0)\n        {\n            std::cout &lt;&lt; \"!!! Oops! No border detected.\" &lt;&lt; std::endl;\n            return 0;\n        }\n\n        // Figure out the exact center location of the border\n        cv::Point border_center = border_points[border_points.size() / 2];\n        //cv::circle(cropped, border_center, 3, cv::Scalar(128, 128, 128));\n        //cv::imwrite(\"test_border_center.jpg\", cropped);\n</code></pre>\n\n<p>The procedure above <em>scans a single vertical line</em> from top/middle of the image to find the borders of the circle to be able to calculate it's width. Again, for education purposes I painted a small gray circle in the middle of the border. This is what <strong>cropped</strong> looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/c5IMQ.png\" width=\"200\" height=\"200\"> <img src=\"https://i.sstatic.net/ZHz8N.png\" width=\"200\" height=\"200\"> </p>\n\n<pre><code>        // Scan the border of the circle for discontinuities \n        int radius = obj_center.y - border_center.y;\n        if (radius &lt; 0) \n            radius *= -1;  \n        std::vector&lt;cv::Point&gt; discontinuity_points;   \n        std::vector&lt;int&gt; discontinuity_angles;\n        for (int angle = 0; angle &lt;= 360; angle++)\n        {\n            int x = obj_center.x + (radius * cos((angle+90) * (PI / 180.f))); \n            int y = obj_center.y + (radius * sin((angle+90) * (PI / 180.f)));                \n\n            if (cropped.at&lt;uchar&gt;(x, y) &lt; 128)\n            {\n                discontinuity_points.push_back(cv::Point(y, x));\n                discontinuity_angles.push_back(angle); \n                //cv::circle(cropped, cv::Point(y, x), 1, cv::Scalar(128, 128, 128));                           \n            }\n        }\n\n        //std::cout &lt;&lt; \"Discontinuity size: \" &lt;&lt; discontinuity_points.size() &lt;&lt; std::endl;\n        if (discontinuity_points.size() == 0 &amp;&amp; discontinuity_angles.size() == 0)\n        {\n            std::cout &lt;&lt; \"!!! Oops! No discontinuity detected. It's a perfect circle, dang!\" &lt;&lt; std::endl;\n            return 0;\n        }\n</code></pre>\n\n<p>Great, so the piece of code above scans along the middle of the circle's border looking for discontinuity. I'm sharing a sample image to illustrate what I mean. Every gray dot on the image represents a pixel that is tested. When the pixel is black it means we found a discontinuity:</p>\n\n<p><img src=\"https://i.sstatic.net/a9tSp.jpg\" alt=\"enter image description here\"></p>\n\n<pre><code>        // Figure out the approximate angle of the discontinuity: \n        // the first angle found will suffice for this demo.\n        int approx_angle = discontinuity_angles[0];        \n        std::cout &lt;&lt; \"#\" &lt;&lt; i &lt;&lt; \" letter C is rotated approximately at: \" &lt;&lt; approx_angle &lt;&lt; \" degrees\" &lt;&lt; std::endl;    \n\n        // Figure out the central point of the discontinuity\n        cv::Point discontinuity_center;\n        for (int a = 0; a &lt; discontinuity_points.size(); a++)\n            discontinuity_center += discontinuity_points[a];\n        discontinuity_center.x /= discontinuity_points.size(); \n        discontinuity_center.y /= discontinuity_points.size(); \n        cv::circle(orig_cropped, discontinuity_center, 2, cv::Scalar(128, 128, 128));\n\n        cv::imshow(\"Original crop\", orig_cropped);\n        cv::waitKey(0);\n    }\n\n    return 0;\n}\n</code></pre>\n\n<p>Very well... This last piece of code is responsible for figuring out the approximate angle of the discontinuity as well as indicate the central point of discontinuity. The following images are stored by <strong>orig_cropped</strong>. Once again I added a gray dot to show the exact positions detected as the center of the gaps:</p>\n\n<p><img src=\"https://i.sstatic.net/hthex.png\" width=\"280\" height=\"260\"> <img src=\"https://i.sstatic.net/KLtg0.png\" width=\"280\" height=\"260\"> </p>\n\n<p>When executed, this application prints the following information to the screen:</p>\n\n<pre><code>#0 letter C is rotated approximately at: 49 degrees\n#1 letter C is rotated approximately at: 0 degrees \n</code></pre>\n\n<p>I hope it helps.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV svm in the primal form(absolute of alpha )",
        "input": "",
        "output": "<p>The OpenCV form is in the dual. The primal does not have the alphas. They simply rolled the sign of the label into the sign of the alphas since the alphas must be positive - saves on space and ups. </p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV understanding Greyscale",
        "input": "",
        "output": "<p>I don't think there is a single function, but you can just split the channels, multiply each channel with its coefficient and sum up all channels in a single image.</p>\n\n<p>c++ syntax would be:</p>\n\n<pre><code>int main()\n{\n  cv::Mat color = cv::imread(\"lena.jpg\");\n  cv::Mat red(color.rows, color.cols, CV_8UC1);\n  cv::Mat green(color.rows, color.cols, CV_8UC1);\n  cv::Mat blue(color.rows, color.cols, CV_8UC1);\n\n  std::vector&lt;cv::Mat&gt; output;\n  output.push_back(blue);\n  output.push_back(green);\n  output.push_back(red);\n\n  cv::split(color,output);\n\n  float r = 0.0f;\n  float g = 0.2f;\n  float b = 1.0f - (r + g);\n  cv::Mat graySelf = r*red + g*green + b*blue;\n  // could be computed with two calls of cv function addWeighted() in the same way!\n\n\n  cv::Mat grayCV;\n  cv::cvtColor(color,grayCV, CV_RGB2GRAY);\n\n  cv::namedWindow(\"cv\"); cv::imshow(\"cv\", grayCV);\n  cv::namedWindow(\"self\"); cv::imshow(\"self\", graySelf);\n  cv::namedWindow(\"color\"); cv::imshow(\"color\", color);\n\n  cv::imwrite(\"cv.png\", grayCV);\n  cv::imwrite(\"self.png\", graySelf);\n  cv::waitKey(-1);\n\n\n  return 0;\n}\n</code></pre>\n\n<p>giving images:</p>\n\n<p>color:</p>\n\n<p><img src=\"https://i.sstatic.net/LxUAt.jpg\" alt=\"enter image description here\"></p>\n\n<p>openCV:</p>\n\n<p><img src=\"https://i.sstatic.net/5RJ3N.png\" alt=\"enter image description here\"></p>\n\n<p>self:</p>\n\n<p><img src=\"https://i.sstatic.net/VTnIK.png\" alt=\"enter image description here\"></p>\n\n<p>in cvtColor, opencv 2.4.4 c++ internally uses this function snippet in imgproc/src/color.cpp I guess:</p>\n\n<pre><code>int scn = srccn;\nfloat cb = coeffs[0], cg = coeffs[1], cr = coeffs[2];\nfor(int i = 0; i &lt; n; i++, src += scn)\n    dst[i] = saturate_cast&lt;_Tp&gt;(src[0]*cb + src[1]*cg + src[2]*cr);\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Matching template imge(scaled) to Main/larger image",
        "input": "",
        "output": "<p>Use OpenCV Feature Detection. it is more accurate than template matching..</p>\n\n<p>Please try with this code..</p>\n\n<pre><code>-(void)featureDetection:(UIImage*)largerImage withImage:(UIImage*)subImage\n{\n    cv::Mat tempMat1 = [largerImage CVMat];\n    cv::Mat tempMat2 = [subImage CVMat];\n\n    cv::cvtColor(tempMat1, tempMat1, CV_RGB2GRAY);\n    cv::cvtColor(tempMat2, tempMat2, CV_RGB2GRAY);\n\n    if( !tempMat1.data || !tempMat2.data ) {\n        return;\n    }\n\n    //-- Step 1: Detect the keypoints using SURF Detector\n    int minHessian = 25;\n\n    cv::SurfFeatureDetector detector( minHessian ); // More Accurate bt take more time..\n    //cv::FastFeatureDetector detector( minHessian ); //Less Accurate bt take less time..\n\n    std::vector&lt;cv::KeyPoint&gt; keypoints_1, keypoints_2;\n\n    detector.detect( tempMat1, keypoints_1 );\n    detector.detect( tempMat2, keypoints_2 );\n\n    //-- Step 2: Calculate descriptors (feature vectors)\n    cv::SurfDescriptorExtractor extractor;\n\n    cv::Mat descriptors_1, descriptors_2;\n\n    extractor.compute( tempMat1, keypoints_1, descriptors_1 );\n    extractor.compute( tempMat2, keypoints_2, descriptors_2 );\n\n    std::vector&lt;cv::Point2f&gt; obj_corners(4);\n\n    //Get the corners from the object\n    obj_corners[0] = (cvPoint(0,0));\n    obj_corners[1] = (cvPoint(tempMat2.cols,0));\n    obj_corners[2] = (cvPoint(tempMat2.cols,tempMat2.rows));\n    obj_corners[3] = (cvPoint(0, tempMat2.rows));\n\n    //-- Step 3: Matching descriptor vectors with a brute force matcher\n    //cv::BruteForceMatcher &lt; cv::L2&lt;float&gt; &gt; matcher;\n    cv::FlannBasedMatcher matcher;\n    //std::vector&lt; cv::DMatch &gt; matches;\n    std::vector&lt;cv::vector&lt;cv::DMatch &gt; &gt; matches;\n\n    std::vector&lt;cv::DMatch &gt; good_matches;\n    std::vector&lt;cv::Point2f&gt; obj;\n    std::vector&lt;cv::Point2f&gt; scene;\n    std::vector&lt;cv::Point2f&gt; scene_corners(4);\n    cv::Mat H;\n\n    matcher.knnMatch( descriptors_2, descriptors_1, matches,2);\n\n    for(int i = 0; i &lt; cv::min(tempMat1.rows-1,(int) matches.size()); i++)  {\n\n        if((matches[i][0].distance &lt; 0.6*(matches[i][1].distance)) &amp;&amp; ((int) matches[i].size()&lt;=2 &amp;&amp; (int) matches[i].size()&gt;0))  {\n            good_matches.push_back(matches[i][0]);\n        }\n    }\n    cv::Mat img_matches;\n    drawMatches( tempMat2, keypoints_2, tempMat1, keypoints_1, good_matches, img_matches );\n\n    NSLog(@\"good matches %lu\",good_matches.size());\n\n    if (good_matches.size() &gt;= 4)  {\n\n         for( int i = 0; i &lt; good_matches.size(); i++ ) {\n             //Get the keypoints from the good matches\n             obj.push_back( keypoints_2[ good_matches[i].queryIdx ].pt );\n             scene.push_back( keypoints_1[ good_matches[i].trainIdx ].pt );\n         }\n\n         H = findHomography( obj, scene, CV_RANSAC );\n\n         perspectiveTransform( obj_corners, scene_corners, H);\n\n         NSLog(@\"%f %f\",scene_corners[0].x,scene_corners[0].y);\n         NSLog(@\"%f %f\",scene_corners[1].x,scene_corners[1].y);\n         NSLog(@\"%f %f\",scene_corners[2].x,scene_corners[2].y);\n         NSLog(@\"%f %f\",scene_corners[3].x,scene_corners[3].y);\n\n\n         //Draw lines between the corners (the mapped object in the scene image )\n         line( tempMat1, scene_corners[0], scene_corners[1], cvScalar(0, 255, 0), 4 );\n\n         line( tempMat1, scene_corners[1], scene_corners[2], cvScalar( 0, 255, 0), 4 );\n\n         line( tempMat1, scene_corners[2], scene_corners[3], cvScalar( 0, 255, 0), 4 );\n\n         line( tempMat1, scene_corners[3], scene_corners[0], cvScalar( 0, 255, 0), 4 );\n     }\n\n     // View matching..\n\n     UIImage *resultimage = [UIImage imageWithCVMat:img_matches];\n     UIImageView *imageview = [[UIImageView alloc] initWithImage:resultimage];\n     imageview.frame = CGRectMake(0, 0, 320, 240);\n     [self.view addSubview:imageview];\n\n     // View Result\n\n     UIImage *resultimage2 = [UIImage imageWithCVMat:tempMat1];\n     UIImageView *imageview2 = [[UIImageView alloc] initWithImage:resultimage2];\n     imageview2.frame = CGRectMake(0, 240, 320, 240);\n     [self.view addSubview:imageview2];\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Computer Vision - Figuring out what angle the camera is pointing to as it moves around?",
        "input": "",
        "output": "<p>If you don't want to implement a full structure from motion algorithm (which is a big task), here's an idea I would try:</p>\n\n<p>First track interest points from one frame to the next. SIFT or SURF is probably a good start. Most of the interest points don't move very far, and once you have processed the first two frames, you have a good estimate where each interest point will be in the next frame, so this should be possible.</p>\n\n<p>Once you have feature correspondences between two frames, the movement of each feature point is (approximately) a sum of two components: One component away from the vanishing point (that is, the direction of motion of the bike) and one component that's identical for each point (the camera's orientation change).</p>\n\n<p><img src=\"https://i.sstatic.net/z258m.png\" alt=\"enter image description here\"></p>\n\n<p>Mathematically speaking, that means point p's location at time t1 is:</p>\n\n<p><img src=\"https://i.sstatic.net/ZQm3j.png\" alt=\"enter image description here\"></p>\n\n<p>where v is the vanishing point, d is the speed of movement away from the vanishing point in the image (depends on the distance of the object from the observer and the speed of the bike) and m is the camera's motion. That's 2*n equations and 4+n unknowns for n points, meaning if you have 4 or more points, you can find an (approximate) solution.</p>\n",
        "system": ""
    },
    {
        "instruction": "Math: How to convert a left-handed transformation matrix to a right handed one",
        "input": "",
        "output": "<blockquote>\n  <p>a right-handed coordinate system where X is right, Y is up and Z is into the screen</p>\n</blockquote>\n\n<p>Then it's a left-handed coordinate system: X is the thumb, Y is the forefinger, Z is the middle-finger. Your right hand should not be able to do what you describe without breaking a few bones :-p</p>\n\n<blockquote>\n  <p>a left-handed coordinate system where X is right, Y is down and Z is out of the screen</p>\n</blockquote>\n\n<p>This one is indeed a left-handed coordinate system too.</p>\n\n<p>As for the transformation linking these two, this is a simple 180\u00b0 rotation around the X axis (beware of the wrist sprain while trying this one), which expresses as the following 3x3 matrix:</p>\n\n<pre><code>(1  0  0)\n(0 -1  0)\n(0  0 -1) \n</code></pre>\n\n<p>You can pre-multiply your transformation with this one!</p>\n\n<p>Hope this helps!</p>\n",
        "system": ""
    },
    {
        "instruction": "openCV recognition throws exception (LBPH algorithm)",
        "input": "",
        "output": "<p>How you define yout algorithm? </p>\n\n<p>If you want to detect face, you have to define model in this way:</p>\n\n<pre><code>const char* recAlgorithmEigenfaces = \"FaceRecognizer.Eigenfaces\";\nPtr&lt;FaceRecognizer&gt; model;\nmodel = Algorithm::create&lt;FaceRecognizer&gt;(recAlgorithmEigenfaces);\n</code></pre>\n\n<p>Optionally you have to check the model:</p>\n\n<pre><code>if (model.empty()) {\n /* throw exception */ \n}\n</code></pre>\n\n<p>And then train your model..</p>\n\n<h1>edit</h1>\n\n<p>another explanation, reading <a href=\"http://books.google.it/books?id=UjWoIFHcr58C&amp;pg=PT434&amp;lpg=PT434&amp;dq=%22FaceRecognizer.Eigenfaces%22&amp;source=bl&amp;ots=S8k6AArx-A&amp;sig=Q4qttbQVYDQNi_VTSSSXzdbl5B0&amp;hl=it&amp;sa=X&amp;ei=fnywUrXRE8jNygOR8oGoDg&amp;ved=0CEwQ6AEwAw#v=onepage&amp;q=%22FaceRecognizer.Eigenfaces%22FaceRecognizer.Eigenfaces&amp;f=false\" rel=\"nofollow\">here</a>, is that:</p>\n\n<blockquote>\n  <p>[..]face recognition algorithms are available through the\n  <strong>FaceRecognizer</strong> class in OpenCV's <strong>contrib</strong> module. Due to dynamic linking, it is possible that your program is linked to the\n  contrib module but it is not actually loaded at runtinme (if it was\n  deemed as not required). So it's recommended to call the</p>\n</blockquote>\n\n<pre><code>cv::initModule_contrib()\n</code></pre>\n\n<blockquote>\n  <p>function before trying to access the FaceRecognizer algorithms.    The\n  function is only available from OpenCV v2.4.1, so it also ensures that\n  the face recognition algorithms are at least available to you at\n  compile time [..]</p>\n</blockquote>\n",
        "system": ""
    },
    {
        "instruction": "How can I capture and recognise user signatures using HTML5 and JS?",
        "input": "",
        "output": "<p>Somewhat off-topic, but still related since you mention multiple biometric identification methods.</p>\n\n<p>Our problem: </p>\n\n<p>We had multiple retail stores with managers.  Only those managers had the authority to change prices and that authority required their PIN code to execute.  Problem was that the managers would get busy and when they were approached by salespeople, they would give their PINs to the salespeople.  Security violations.</p>\n\n<p>Our solution: </p>\n\n<p>We installed fingerprint readers so the managers were required to physically do those tasks that they alone were authorized to do.  No more PINs to cause security violations.</p>\n\n<p>Fingerprint verification has fewer false positives and fewer false negatives than other biometrics. It is inexpensive to implement and properly matches actions with authority.  It was a good solution for us.</p>\n",
        "system": ""
    },
    {
        "instruction": "Pose from Fundamental matrix and vice versa",
        "input": "",
        "output": "<p>The first F is defined up to scale, hence if you're going to compare the returned F and with the F matrix computed from E you need to normalize them to make sure both are at the same scale. Hence you need to normalize the second computed F.</p>\n",
        "system": ""
    },
    {
        "instruction": "how to use matlab vision.ForegroundDetector with webcam",
        "input": "",
        "output": "<p>If you have access to the Image Acquisition Toolbox, you could replace the first line with:</p>\n\n<pre><code>hsrc = imaq.VideoDevice('winvideo', 1, 'MJPG_640x480', ...\n    'ReturnedColorSpace','grayscale', 'ReturnedDataType','uint8');\n</code></pre>\n\n<p>You will need to adjust the parameters according to the supported formats by your webcam. Just consult the documentation of the <a href=\"http://www.mathworks.com/help/imaq/imaq.videodevice.html\" rel=\"nofollow\"><code>imaq.VideoDevice</code></a> function.</p>\n\n<p>Also replace the loop test by just <code>while true</code> since the video feed is always not done :)</p>\n",
        "system": ""
    },
    {
        "instruction": "Why won&#39;t cv.findChessboardCorners work for me?",
        "input": "",
        "output": "<p>I hope that you don't mind an answer in Python, as opposed to matlab.  (They use the same openCV library and I hope the correspondence between commands is clear.)</p>\n\n<p>Your image unchanged works fine for me with the code below which finds the corners and displays them with colored points in a window:</p>\n\n<pre><code>#!/usr/bin/python\nimport cv2.cv as cv\nimport sys\n\nfilename = sys.argv[1]\nim = cv.LoadImage(filename, cv.CV_LOAD_IMAGE_GRAYSCALE)\nim3 = cv.LoadImage(filename, cv.CV_LOAD_IMAGE_COLOR)\nchessboard_dim = (15, 11)\nfound_all, corners = cv.FindChessboardCorners( im, chessboard_dim )\ncv.DrawChessboardCorners( im3, chessboard_dim, corners, found_all )\ncv.ShowImage(\"Chessboard with corners\", im3)\ncv.WaitKey()\n</code></pre>\n\n<p>The output image (slightly cropped) looks like:</p>\n\n<p><img src=\"https://i.sstatic.net/29J2E.jpg\" alt=\"Checkerboard with corners highlighted\"></p>\n",
        "system": ""
    },
    {
        "instruction": "VLFeat HOG feature extraction",
        "input": "",
        "output": "<p>The entries in that matrix <em>are</em> features! Depending on what you're trying to achieve you might do some dimensionality reduction or augmentation or post processing, but none of that is strictly necessary. Check out <a href=\"http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf\" rel=\"nofollow\">the original HoG paper</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Vehicle Counting Based on BackgroundSubtractorMOG",
        "input": "",
        "output": "<p>How do you update your background? Because of changes in the lighting condition (clouds, day, night, dusk, weather) you cannot keep it statistic, however the presence of a stopped car could be still detectable if you still know the appearance of the background, that is the appearance of the road if the car is not there.\nIf you have an area in the image where car do not pass, you can use that to understand if the lighting conditions are changing.</p>\n\n<p>Which is your view angle for the vehicles? There is chance that combining a Viola Jones detector with a KLT tracker you obtain better and more general results.</p>\n",
        "system": ""
    },
    {
        "instruction": "License Plate Image Matching",
        "input": "",
        "output": "<p>Frankly this is a non-trivial question.</p>\n\n<p>Just to list some obvious options:</p>\n\n<ul>\n<li>Implement one of the numerous character recognition softwares, and\nget the string of characters, and then do a search for the substring\nin another string.</li>\n<li>For images with almost no difference in zoom\nlevel, Use edge detection filters, like canny edge detection, to\nenhance the image, then use ICP (Iterative Closest Point), letting\neach edge pixel provide a vector to the closest edge pixel in the\nother image, with a similar value. this typically aligns images if\nthey are similar enough. The final score tells you how similar they\nare.</li>\n<li>For very large zoom levels, use multiple rotation and zoom\nhypothesis, and for each, scale the images and do cross correlation\nof the two images. select the hypothesis, that provides the\ncoordinates with the best correlation, and use the point of\ncorrelation, as the x and y offset. The value of the correlation\ntells you how good a fit you have..</li>\n</ul>\n\n<p>many other smarter algorithms have been produced for image fitting. However, you have much larger problems.\nThe two example images you provide does not show the entire licenseplate, so you will not be able to say anything better than, \"the probabillity of a match is larger than zero\", as the number of visible characters increase, so does the probabillity of a match.</p>\n\n<p>you could argue that small damages to a license plate also increases the probabillity, in that case cross correlation or similar method is needed to evaluate the probabillity of a match.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to convert the RGB image to the gray scale image?",
        "input": "",
        "output": "<p>With \"the\" toolbox: <a href=\"http://www.mathworks.nl/help/images/ref/rgb2gray.html\" rel=\"nofollow noreferrer\">rgb2gray</a>.</p>\n\n<p>Without \"the\" toolbox: look <a href=\"http://www.mathworks.com/matlabcentral/answers/99136\" rel=\"nofollow noreferrer\">here</a>, or <a href=\"https://stackoverflow.com/questions/4063965/how-can-i-convert-an-rgb-image-to-grayscale-but-keep-one-color\">here</a> or <a href=\"https://stackoverflow.com/questions/2348037/converting-rgb-images-to-grayscale-and-renaming-them-using-matlab\">here</a> or <a href=\"http://core.kmi.open.ac.uk/download/pdf/823688.pdf\" rel=\"nofollow noreferrer\">this paper</a> or <a href=\"https://stackoverflow.com/questions/687261/converting-rgb-to-grayscale-intensity\">here</a> or <a href=\"http://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/\" rel=\"nofollow noreferrer\">here</a> or ...</p>\n\n<p>That's just the first couple of Google hits for \"convert RGB to grayscale\" and \"convert RGB to grayscale MATLAB\".</p>\n\n<p>Summary: </p>\n\n<pre><code>gray = 0.2989*img(:,:,1) + 0.5870*img(:,:,2) + 0.1140*img(:,:,3); \n</code></pre>\n\n<p>where <code>img</code> is your RGB image.</p>\n",
        "system": ""
    },
    {
        "instruction": "Check presence of subimage in image in iOS",
        "input": "",
        "output": "<p>You can use OpennCV Template Matching algorithm..</p>\n\n<p>please try it..</p>\n\n<p>download opencv2.framework from <a href=\"https://github.com/Fl0p/OpenCV-iOS\" rel=\"noreferrer\">https://github.com/Fl0p/OpenCV-iOS</a></p>\n\n<p>download UIImage+OpenCV from <a href=\"https://github.com/aptogo/OpenCVForiPhone/tree/master/OpenCVClient\" rel=\"noreferrer\">https://github.com/aptogo/OpenCVForiPhone/tree/master/OpenCVClient</a></p>\n\n<p>import files opencv2/nonfree/nonfree.hpp, opencv2/highgui/highgui.hpp, opencv2/calib3d/calib3d.hpp, UIImage+OpenCV.h</p>\n\n<p>use this function for matching images.</p>\n\n<pre><code>-(BOOL) matchImages:(UIImage*)largerImage Image2:(UIImage*)subImage\n{\n\n cv::Mat tempMat1 = [largerImage CVMat];\n cv::Mat tempMat2 = [subImage CVMat];\n\n cv::Mat result;\n\n\n int match_method = CV_TM_SQDIFF_NORMED;\n //cv::cvtColor(tempMat1, debug, CV_GRAY2BGR);\n //cv::cvtColor(tempMat1, tempMat1, cv::COLOR_RGB2GRAY);\n //cv::cvtColor(tempMat2, tempMat2, cv::COLOR_RGB2GRAY);\n\n int result_cols =  tempMat1.cols - tempMat2.cols + 1;\n int result_rows = tempMat1.rows - tempMat2.rows + 1;\n\n result.create( result_cols, result_rows, CV_32FC1 );\n\n\n /// Do the Matching and Normalize\n cv::matchTemplate( tempMat1,tempMat2 ,result,match_method);\n\n double minVal; double maxVal;\n cv::Point minLoc, maxLoc, matchLoc;\n cv::minMaxLoc(result, &amp;minVal, &amp;maxVal, &amp;minLoc, &amp;maxLoc, cv::Mat() );\n if( match_method == CV_TM_SQDIFF || match_method == CV_TM_SQDIFF_NORMED ) matchLoc = minLoc;\n else matchLoc = maxLoc;\n\n//NSLog(@\"%d %d\",tempMat1.cols,tempMat1.rows);\nNSLog(@\"%f %f\",minVal,maxVal);\n\n if (minVal &lt; 0.25) {\n\n     NSLog(@\"success\");\n\n     //cv::rectangle(tempMat1,matchLoc,cv::Point(matchLoc.x + tempMat2.cols , matchLoc.y + tempMat2.rows),CV_RGB(255,0,0),3);\n\n     //UIImage *resImage = [[UIImage alloc] initWithCVMat:tempMat1];\n     //UIImageView * imageview = [[UIImageView alloc] initWithImage:resImage];\n     //[imageview setFrame:CGRectMake(0, 0, resImage.size.width, resImage.size.height)];\n     //[self.view addSubview:imageview];\n\n     return YES;\n }\n else {\n\n    return NO;\n }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Troubles in running STIP (Space-Time Interest Points) in windows",
        "input": "",
        "output": "<p>Install the Xvid codec, not the DivX codec</p>\n",
        "system": ""
    },
    {
        "instruction": "Extracting the fingers from the hand",
        "input": "",
        "output": "<p>The problem (or at least the main one I can see), is the way you construct your contours. You should use: </p>\n\n<pre><code>vector&lt;vector&lt;int&gt; &gt;hull( contours.size() );\n</code></pre>\n\n<p>Instead of:</p>\n\n<pre><code>vector&lt;vector&lt;Point&gt; &gt;hull( contours.size() );\n</code></pre>\n\n<p>This is because the <code>convexityDefects</code> function only works on convex hulls represented by a series of indices rather than a series of points.</p>\n",
        "system": ""
    },
    {
        "instruction": "Object with text and images detection",
        "input": "",
        "output": "<p>SURF would probably not be the best descriptor for this purpose.<br>\nI would approach this problem by running connected component analysis, and then use the distribution pattern of the letter components to match the shape of the text blocks between pages.<br>\nIf you take the centroid of each component as a single point, the list of points create a sort of shape.\n<p>This would probably have problems on pages that only have long monotonous text without breaks or titles, but then any descriptor short of performing optical recognition and comparing text would fail.\n<p>On the example you've given, it has a potential of working due to the fact that the lines width are not constant, and there are markers, like the paragraph headers.</p>\n",
        "system": ""
    },
    {
        "instruction": "OpenCV: Fundamental matrix accuracy",
        "input": "",
        "output": "<p>I believe the problem is because you calculated the Fundamental matrix based on brute force matcher only, you should make some more optimization for these corresponding point, like ration test and symmetric test.\nI recommend you to ready page 233, from book \"OpenCV2 Computer Vision Application Programming Cookbook\" Chapter 9.\nIts explained very well! </p>\n",
        "system": ""
    },
    {
        "instruction": "Porting OpenCV on WinCE07 platform",
        "input": "",
        "output": "<p>OpenCV site has instruction to build OpenCV from source, see <a href=\"http://docs.opencv.org/doc/tutorials/introduction/windows_install/windows_install.html#installation-by-making-your-own-libraries-from-the-source-files\" rel=\"nofollow\">\"Installation in Windows - Installation by Making Your Own Libraries from the Source Files\"</a>.</p>\n\n<p>Basically you need:</p>\n\n<ol>\n<li>A suitable development environment for Windows CE (some years ago the environment was eMbedded Visual C++ 4.0, I do not know what Microsoft has now).</li>\n<li>The source code of OpenCV.</li>\n<li>CMake, it is the program for managing the build. CMake will create the <a href=\"http://msdn.microsoft.com/en-us/library/bb384842.aspx\" rel=\"nofollow\">projects and solutions</a> files. Usually CMake automatically recognize your development environment.</li>\n</ol>\n",
        "system": ""
    },
    {
        "instruction": "How to output result of rgb2hsv (causes colour values to be between 0-1) as a uint8 image?",
        "input": "",
        "output": "<p>Use <code>sDiff = uint8(sDiff.*256);</code> to convert it to uint8 format</p>\n",
        "system": ""
    },
    {
        "instruction": "Computing the 3D Transformation between Two Sets of Points",
        "input": "",
        "output": "<p>I am currently struggling with ICP myself. Here is what I have gathered so far:</p>\n\n<p>ICP consists of three steps:</p>\n\n<ul>\n<li>Given two point clouds A and B, find pairs of points between A and B that <em>probably</em> represent the same point in space. Often this is done simply by matching each point with its closest neighbor in the other cloud, but you can use additional features such as color, texture or surface normal to improve the matching. Optionally you can then discard the worst matches.</li>\n<li>Given this list of correspondence pairs, find the optimal transformation from A to B</li>\n<li>Apply this transformation to all points in A</li>\n<li>repeat these three steps until you converge on an acceptable solution.</li>\n</ul>\n\n<p>Step one is easy, although there are lots of ways to optimize its speed, since this is the major performance bottleneck of ICP; and to improve the accuracy, since this is the main source of errors. OpenCV can help you there with the <a href=\"http://docs.opencv.org/trunk/modules/flann/doc/flann_fast_approximate_nearest_neighbor_search.html\">FLANN library</a>.</p>\n\n<p>I assume your troubles are with step two, finding the best transformation given a list of correspondences.</p>\n\n<p>One common approach works with <a href=\"http://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular Value Decomposition (SVD)</a>. Here is a rough sketch of the algorithm. Searching for ICP &amp; SVD will give a lot of further references.</p>\n\n<ul>\n<li>Take the list of corresponding points A<sub>1</sub>..A<sub>n</sub> and B<sub>1</sub>..B<sub>n</sub> from step 1</li>\n<li>calculate the centroid C<sub>a</sub> of all points in A and the centroid C<sub>b</sub> of all points in B</li>\n<li>Calculate the 3x3 covariance matrix M<br>\nM = (A<sub>1</sub> - C<sub>a</sub>)* (B<sub>1</sub> - C<sub>b</sub>)<sup>T</sup> + ... + (A<sub>n</sub> - C<sub>a</sub>)* (B<sub>n</sub> - C<sub>b</sub>)<sup>T</sup>  </li>\n<li>Use SVD to calculate the 3x3 Matrices U and V for M<br>\n<em>(OpenCV has a <a href=\"http://docs.opencv.org/modules/core/doc/operations_on_arrays.html#SVD\">function to perform SVD</a>)</em></li>\n<li>Calculate R = U * V<sup>T</sup>.<br>\nThis is your desired optimal rotation matrix.</li>\n<li>Calculate the optimal translation as C<sub>b</sub> - R*C<sub>a</sub></li>\n<li>The optimal transformation is the combination of R and this translation</li>\n</ul>\n\n<p>Please note that I have not yet implemented this algorithm myself, so I am only paraphrasing what I read.</p>\n",
        "system": ""
    },
    {
        "instruction": "Can I detect shapes in an image using gd php",
        "input": "",
        "output": "<p>Possibly not memory or super fast, but perhaps taking / making a hashmap of the pixel colors at certain points within your rectangle</p>\n<p>Example:</p>\n<p>Rectangle size: 200px by 200px\nPlots:      0x0, 0x50, 0x100,0x150, 0x200, 50x0, 50x50, 50x100, etx.</p>\n<p>At each plot record the color average within X points around your plot ( this should help prevent noise causing false positives.</p>\n<p>then if at anypoint the map of colors you have is equal to empty plot, then your car is probably gone.</p>\n<p>I understand that this question is old, but it was semi-close to a project I'm working on, as such I decided to leave an answer for someone in the future.</p>\n",
        "system": ""
    },
    {
        "instruction": "cv:Mat, every second pixel is set",
        "input": "",
        "output": "<p>If your matrix is of type CV_8UC1, then each element is one byte in size and you should be using <code>.at&lt;uchar&gt;</code> or similar, rather than <code>.at&lt;int&gt;</code>.</p>\n\n<p>Although this isn't your problem, you might also end-up confused about rows and columns, as your Mat constructor takes <code>nRows,nCols</code>, which is the opposite way around to <code>x,y</code></p>\n",
        "system": ""
    },
    {
        "instruction": "what this function does videooptflowlines() matlab?",
        "input": "",
        "output": "<p>The function <code>videooptflowlines</code> is a helper function used by the demos (<a href=\"http://www.mathworks.com/products/computer-vision/examples.html\" rel=\"nofollow\"><code>visiondemos</code></a>) in the Computer Vision System toolbox. You can see the code for this function by typing <code>edit videooptflowlines</code> in the Matlab command window. A comment in the code states that, as its name indicates, the function is used in a help example for <a href=\"http://www.mathworks.com/help/vision/ref/vision.opticalflowclass.html#examples\" rel=\"nofollow\"><code>vision.OpticalFlow</code></a>.</p>\n\n<p>Essentially the function does the basic math to create vector lines that indicate optic flow direction. There are several parameters in the code that will probably depend on the resolution of the image used. If you're creating your own code that uses this function, you should probably create a copy of it and edit the new version to suit your needs.</p>\n",
        "system": ""
    },
    {
        "instruction": "Opencv Face detection slower",
        "input": "",
        "output": "<p>If the location of the face is stored in cvRect faceRect, and the original image is stored in cvMat OriginalImage, try this:</p>\n\n<pre><code>cvRect enlargedFrame;\nenlargedFrame.x = faceRect.x*originalImage.cols/720;\nenlargedFrame.y = faceRect.y*originalImage.rows/480;\nenlargedFrame.width = faceRect.width*originalImage.cols/720;\nenlargedFrame.height = faceRect.height*originalImage.rows/480;\n</code></pre>\n\n<p>Now draw a rectangle using the coordinates of enlargedFrame.</p>\n\n<p>If you decide to crop the image to make it even smaller, this gets more complicated. In that case, you could use affine transforms.</p>\n",
        "system": ""
    },
    {
        "instruction": "Simulink: draw only the graphic marker without image/video",
        "input": "",
        "output": "<p>You have to define your background and what value of background you can use to indicate transparency. In the case of \"(Image + Marker) - Image\" you are basically defining your background as zero. You can simply start with an image of all zeros and use \"Draw Markers\" on it. That should give you the same results.</p>\n",
        "system": ""
    },
    {
        "instruction": "Query regarding next layer in a Stacked Denoising AutoEncoder",
        "input": "",
        "output": "<p>My interpretation of the stacked denoising autoencoder is you train the first autoencoder (i.e. 64 -> 32 -> 64) with backprogogation and your noise-free input as the output as you would a typical neural network then push your data through the first layer into 32 dimensional space and run the same process (i.e. 32 -> 16 -> 32) and go forwards from there.  You could then add another layer on.  You theoretical could also do some sort of fine tuning on the network as you could also form a 64 -> 32 -> 16 -> 32 -> 64 network and fine tune the parameters, but that probably isn't necessary.</p>\n\n<p>After those steps, you then take you input data in 64 dimensional space and push it through 64 -> 32 -> 16 -> your classifier.  If you want to use a neural network as you classifier then you could continue with more layers after that and then run backprop on that all the way to the start thus achieving better results.  The original work on stacked denoising autoencoders is <a href=\"http://www.iro.umontreal.ca/~lisa/pointeurs/BengioNips2006All.pdf\" rel=\"nofollow\">here</a> (PDF).</p>\n\n<p>On a side note, if you are thinking of using an SVM, I believe that is referred to as learning the kernel, but I do not have the reference for that.</p>\n",
        "system": ""
    },
    {
        "instruction": "How to change invert frames in EmguCV?",
        "input": "",
        "output": "<p>From the images above it feels that the second image is the negative of the first image(correct me if I am wrong),</p>\n\n<p>The function you are using is a threshold function,i.e.  will render everything as white if it falls between the specified color range and other wise it will render it as black.</p>\n\n<p>To find the negative of an image you can use one of the following methods.</p>\n\n<p>Taking the NOT of an image.</p>\n\n<pre><code> Image&lt;Bgr, Byte&gt; img2 = img1.Not();// imag1 can be a static image or your current captured frame\n</code></pre>\n\n<p>for more details you can refer the documentation <a href=\"http://www.emgu.com/wiki/index.php/Working_with_Images\" rel=\"noreferrer\">here</a>.</p>\n",
        "system": ""
    },
    {
        "instruction": "Segmentation in Bag of Word Model",
        "input": "",
        "output": "<p>I hope these links will help you...\nYes, you can perform segmentation with bag of features or words, here is a paper.\n<a href=\"http://www.iiia.csic.es/~mantaras/CCIA2010.pdf\" rel=\"nofollow\">Link1</a></p>\n\n<p>This paper \u201cCombined Object Categorization and Segmentation with an Implicit Shape Model\u201d is one of the most famous paper. Even though it is not directly related to your question but go through the related work section of this paper, where they presented many algorithms which uses bag of words sort of techniques for segmentation.  It is one of the finest CVPR papers I ever come across. \n<a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/leibe04.pdf\" rel=\"nofollow\">Link2</a></p>\n\n<p>But bag of words or features are mostly popular in Object Recognition filed only. It is up to you to decide Whether to use it directly or combined it with other algorithms. But i am sure that visual vocabulary formed from local texture descriptors can definitely segment the object of interest.  </p>\n",
        "system": ""
    },
    {
        "instruction": "How to get covariance matrix from point cloud data using PCL library?",
        "input": "",
        "output": "<p>That is straight forward, but I guess you need more reading the documentations/tutorials :) </p>\n\n<p>1- load the PCD file, for example:</p>\n\n<pre><code>pcl::PointCloud&lt;pcl::PointXYZ&gt;::Ptr cloud (new pcl::PointCloud&lt;pcl::PointXYZ&gt; ());\npcl::io::loadPCDFile(\"c:\\path\\pcdfile.pcd\",*cloud)\n</code></pre>\n\n<p>2- calculate the centroid:</p>\n\n<pre><code>Eigen::Vector4f xyz_centroid;\ncompute3DCentroid (cloud, xyz_centroid);\n</code></pre>\n\n<p>3- calculate the covariance</p>\n\n<pre><code>Eigen::Matrix3f covariance_matrix;\ncomputeCovarianceMatrix (cloud, xyz_centroid, covariance_matrix); \n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Getting rid of certain spots on an image",
        "input": "",
        "output": "<p>I assume that the image is binary. In that case:</p>\n\n<p>If the \"dots\" (small circles) that you want to detect are larger than the spots you want to remove, you can apply a median filter on the image. The size of the median filter can be determined according to the size of the noise spots. Another possibility is to use morphological operations (erosion and dilation). All these operations are supported by OpenCV.</p>\n\n<p>If the image is not binary, you can start by converting it to binary using a threshold value.</p>\n",
        "system": ""
    },
    {
        "instruction": "Opencv: Computing fundamental matrix from R and T",
        "input": "",
        "output": "<p>Hum, your F matrix seems wrong - to begin with, the rank is closer to 3 than 2.\nFrom your data I get:</p>\n\n<pre><code>octave:9&gt; tx = [ 0 -T(3) T(2)\n&gt; T(3) 0 -T(1)\n&gt; -T(2) T(1) 0]\ntx =\n\n   0.000000   0.028545   0.041492\n  -0.028545   0.000000  -0.000165\n  -0.041492   0.000165   0.000000\n\noctave:11&gt; E= R* tx\nE =\n\n  -2.1792e-04   2.8546e-02   4.1491e-02\n  -4.8255e-02   4.6088e-05  -2.1160e-04\n   1.4415e-02   1.1148e-04   2.4526e-04\n\noctave:12&gt; F=inv(M1')*E*inv(M2)\nF =\n\n  -3.6731e-10   4.8113e-08   2.4320e-05\n  -8.1333e-08   7.7681e-11   6.7289e-05\n   7.0206e-05  -3.7128e-05  -7.6583e-02\n\noctave:14&gt; rank(F)\nans =  2\n</code></pre>\n\n<p>Which seems to make more sense. Can you try that F matrix in your plotting code?</p>\n",
        "system": ""
    },
    {
        "instruction": "Libsvm java training testing example(also in real time)",
        "input": "",
        "output": "<p>It is because your featuresTesting is never used, <code>HashMap&lt;Integer, Double&gt; tmp=new HashMap&lt;Integer, Double&gt;();</code> should be <code>HashMap&lt;Integer, Double&gt; tmp=featuresTesting.get(testInstance);</code></p>\n",
        "system": ""
    },
    {
        "instruction": "Both SVM and Clustering ? Confusion",
        "input": "",
        "output": "<p>(i) They extracted the low-level shape features (triple adjacent contour segments (TAS)) from the image with the help of image processing algorithms such as edge/contour detection,  (ii) they defined feature dissimilarity, (iii) they defined a kernel function, and used the function as a standard for the clustering, (iv) they used SVM to learn which label(language) corresponds to a given clustering.</p>\n\n<p>The unsupervised clustering method mainly helps different languages be classified into different groups based on the shape feature, yet given the fact that we already know several handwritings represent a same language, it is still unknown which language they are. As a result, a supervised learning is needed.</p>\n",
        "system": ""
    },
    {
        "instruction": "Fingerprint Image Enhancement",
        "input": "",
        "output": "<p>You should use orientation filters e.g Gabor filter.</p>\n\n<p>search it on Google (filter ensamble, fingerprint enhacemeNt)</p>\n\n<p>how it works:\n    1) create an ensemble of Gabor filters (different orientations , different scales...)\n    2) convolve an image with each filter in ensemble\n    3) take maximum response from images (for each pixel choose filter that gives highest score)</p>\n\n<p>now you will know what orientation and what filter size suits the best for each pixel (line segment + orientation) and in the same time you will discard noisy data.</p>\n\n<p>yes it is going to be slow, but the results are very nice.</p>\n\n<p>look at:\n<a href=\"http://www.cse.iitk.ac.in/users/biometrics/pages/111.JPG\" rel=\"nofollow\">http://www.cse.iitk.ac.in/users/biometrics/pages/111.JPG</a></p>\n",
        "system": ""
    },
    {
        "instruction": "Iterating through a hsv image keeps crashin",
        "input": "",
        "output": "<p>It's possibly crashing because you have a typo in the inner loop, where you compare with <code>i</code> instead of <code>j</code> in the termination condition. </p>\n\n<pre><code>for(int j=0; i&lt;hsvimage2.cols; j++)\n</code></pre>\n\n<p>Also, if you have a BGR image going in (3 channels), you'll get a HSV image out (3 channels), but you are accessing the pixels as if they were single channel. Try something like this for your loop to dump the H, S and V values:</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>for(int i=0; i&lt;hsvimage2.rows; i++)\n{\n    for(int j=0; j&lt;hsvimage2.cols; j++)   // original error was on this line\n    { \n        Vec3b pHSV = hsvimage2.at&lt;Vec3b&gt;(i, j);\n        std::cout &lt;&lt; pHSV.val[0] &lt;&lt; \" \" \n                  &lt;&lt; pHSV.val[1] &lt;&lt; \" \" \n                  &lt;&lt; pHSV.val[2] &lt;&lt; std::endl;\n    }\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "Building a rectangle with a group of points in opencv",
        "input": "",
        "output": "<p><strong>Method 1</strong></p>\n\n<p>This method will be useful when your image contains contour which not represent your  rectangle sides</p>\n\n<ol>\n<li>Here first thing you need to do is find the centre of each contour,\nyou may proceed with find   <a href=\"http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/moments/moments.html\" rel=\"nofollow noreferrer\">OpenCV moment</a> or\n<a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#minenclosingcircle\" rel=\"nofollow noreferrer\">minEnclosingCircle</a> after <a href=\"http://docs.opencv.org/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#findcontours\" rel=\"nofollow noreferrer\">findcontour</a>. Now you have set of\npoints representing your rectangle.</li>\n<li>Next step is classify the points to sides of rectangle like top, bottom, left and right. That is find the points which are lying on the same line  these <a href=\"http://www.mathsisfun.com/equation_of_line.html\" rel=\"nofollow noreferrer\">link</a> and <a href=\"https://stackoverflow.com/questions/7740507/extend-a-line-segment-a-specific-distance\">discussion</a>  might be helpful.</li>\n<li>After sorting(classify the points which lies on same line) you can easily find out top, bottom, right and left by extending these the lines and find four <a href=\"http://www.wikihow.com/Algebraically-Find-the-Intersection-of-Two-Lines\" rel=\"nofollow noreferrer\">intersection</a>  of each line, where the minimum y-value stand for top, minimum x stand for left, maximum x stand for right and maximum y stand of bottom.  </li>\n</ol>\n\n<p><strong>Edit:</strong></p>\n\n<p><strong>Method 2</strong></p>\n\n<p>Instead of doing all above step you can simply find out four corners as described below.</p>\n\n<ul>\n<li>Find centre points of all contour.</li>\n<li>Find  points with minimum x and maximum x which will represent two corner.   </li>\n<li>Find points with minimum y and maximum y which will represent the other two corner.  </li>\n<li><p>Now you can decide which point top left, top right, bottom left and bottom right by looking on these values. </p>\n\n<p>-> From set of four points consider set of two points with minimum y-value. Now consider these two points and your top left corner will be point with minimum x value and top right corner will be the point with maximum x.</p>\n\n<p>-> Similarly  from the remaining two points(Set of points with maximum y values) find the point with  minimum x value which will be bottom left and points with maximum x will be  bottom right corner.</p></li>\n</ul>\n\n<p><strong>Code for method 2</strong></p>\n\n<pre><code>Mat src=imread(\"src.png\",0);\n    vector&lt; vector &lt;Point&gt; &gt; contours; // Vector for storing contour\n    vector&lt; Vec4i &gt; hierarchy;\n    findContours( src, contours, hierarchy,CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE ); // Find the contours in the image\n\n    vector&lt;Point2f&gt;center( contours.size() );\n    vector&lt;float&gt;radius( contours.size() );\n    for( int i = 0; i&lt; contours.size(); i++ ){\n        minEnclosingCircle(contours[i], center[i], radius[i] );\n        circle(src,center[i],radius[i], Scalar(255),1,8,0);\n    }\n\nfloat top_left=0, top_right=0, bot_left=0,bot_right=0;\nfloat idx_min_x=0,idx_min_y=0,idx_max_x=0,idx_max_y=0;\n\nfor( int i = 0; i&lt; contours.size(); i++ ){\n\n   if(center[idx_max_x].x&lt;center[i].x) idx_max_x=i;\n   if(center[idx_min_x].x&gt;center[i].x) idx_min_x=i;\n\n   if(center[idx_max_y].y&lt;center[i].y) idx_max_y=i;\n   if(center[idx_max_y].y&gt;center[i].y) idx_min_y=i;\n  }\n\n\nvector&lt;Point2f&gt;corners;\ncorners.push_back (center[idx_max_x]);\ncorners.push_back (center[idx_min_x]);\ncorners.push_back (center[idx_max_y]);\ncorners.push_back (center[idx_min_y]);\n\nPoint tmp;\n\nfor( int i = 0; i&lt; corners.size(); i++ ) {\n for( int j = 0; j&lt; corners.size()-1; j++ ) {\n  if(corners[j].y&gt;corners[j+1].y){\n  tmp=corners[j+1];\n  corners[j+1]=corners[j];\n  corners[j]=tmp;\n  }\n}\n}\n\nif(corners[0].x&gt;corners[1].x){ top_left=1; top_right=0;}\nelse { top_left=0; top_right=1;}\n\nif(corners[2].x&gt;corners[3].x){ bot_left=3; bot_right=2;}\nelse { bot_left=2; bot_right=3;}\n\n line(src,corners[top_left],corners[top_right], Scalar(255),1,8,0);\n line(src,corners[bot_left],corners[bot_right], Scalar(255),1,8,0);\n\n line(src,corners[top_left],corners[bot_left], Scalar(255),1,8,0);\n line(src,corners[top_right],corners[bot_right], Scalar(255),1,8,0);\nimshow(\"src\",src);\n\nwaitKey();\n</code></pre>\n\n<p><strong>Result:</strong>\n<img src=\"https://i.sstatic.net/K171I.jpg\" alt=\"enter image description here\"></p>\n",
        "system": ""
    },
    {
        "instruction": "How do I test an OpenCV SVM?",
        "input": "",
        "output": "<p>It seems that your problem lies in the detection of \"true\" and \"false\", in fact your code should not even compile (1 opening bracet and 2 closing ones)</p>\n\n<pre><code>if (p &gt; 0.0) || p&lt;0.0) {\n        t++;\n    }\n    else {\n        f++;\n    }\n</code></pre>\n\n<p>you are giving your model a \"point\" iff <code>p!=0.0</code>, so you aleays get a 100%. You should compare the prediction of your SVM with the <strong>true</strong> value of this point. And if they are the same -- increase the <code>t</code> counter, and increase <code>f</code> otherwise.</p>\n",
        "system": ""
    },
    {
        "instruction": "Why do the weights of my iterative reweighted least square algorithm for logistic regression always end up with NaN?",
        "input": "",
        "output": "<p>The <code>NaN</code> result occurs in your <code>inv(phi' * R * phi)</code>. Did you check your <code>phi</code>? Try <code>cond(phi)</code> to check whether it is very large. That may cause that inverse operation provides a huge elements.</p>\n\n<p>By the way, I am trying to understand why it is not <code>w_new = w_old - inv(phi' * R * phi) * phi' * R * (y - t);</code> in your iterative reweighted least square algorithm implementation?</p>\n",
        "system": ""
    },
    {
        "instruction": "Error -215 trainDescCollection[iIdx] Python OpenCV",
        "input": "",
        "output": "<p>This is an open bug in opencv. \n<a href=\"https://github.com/Itseez/opencv/issues/5700\" rel=\"noreferrer\">https://github.com/Itseez/opencv/issues/5700</a></p>\n\n<p>Try using smaller images (for example, take a subset and/or down-sample). It looks as though the implementation involves a tricky (in the pejorative sense) optimisation which assumed that the number of features (in one of the images) is less than some magic power-of-two. </p>\n",
        "system": ""
    },
    {
        "instruction": "Template Matching with template update",
        "input": "",
        "output": "<p>The problem is in this section:</p>\n\n<pre><code>if (select_flag)\n{\n    roiImg.copyTo(mytemplate);\n    // select_flag = false;   //select_flag is kept false so that new template can\n    // ^^^^^^^^^^^ WRONG\n    go_fast = true;           //copied to 'mytemplate' for each iteration\n}\n</code></pre>\n\n<p>You actually need to set <code>select_flag</code> to be <code>false</code> on the first iteration. Otherwise, you're just copying what's in the current image on that frame into your template, and of course you find it in exactly the same place!</p>\n\n<p>Once that's done, make sure you move your template update to <em>after</em> the tracking is done on that frame. I'd also recommend not drawing on the original image (your <code>rectangle</code>) until after all image accesses are done. You were actually drawing the rectangle into your image before copying out of it. Here's my adjusted function with template update:</p>\n\n<pre><code>void track()\n{\n  std::cout &lt;&lt; select_flag &lt;&lt; std::endl;\n\n    if (select_flag)\n    {\n        roiImg.copyTo(mytemplate);\n        select_flag = false;   //select_flag is kept false so that new template can\n        go_fast = true;           //copied to 'mytemplate' for each iteration\n\n    }\n\n//     imshow( \"mytemplate\", mytemplate ); waitKey(0);\n\n    Mat result  =  TplMatch( img, mytemplate );\n\n    imshow(\"match\", result);\n\n    Point match =  minmax( result );  //PROBLEM: \"match\" always returning same value!!!\n\n    std::cout &lt;&lt; \"match: \" &lt;&lt; match &lt;&lt; endl;\n\n    /// template update step\n    Rect ROI = cv::Rect( match.x, match.y, mytemplate.cols, mytemplate.rows );\n\n    std::cout &lt;&lt; ROI &lt;&lt; std::endl;\n\n    roiImg = img( ROI );\n    imshow( \"roiImg\", roiImg ); //waitKey(0);\n\n    // Update the template AFTER tracking has occurred to carry it over to the next frame\n    roiImg.copyTo(mytemplate);\n    imshow(\"mytemplate\", mytemplate);\n\n    // Draw onto the image AFTER all accesses are performed\n    rectangle( img, match, Point( match.x + mytemplate.cols , match.y + mytemplate.rows ), CV_RGB(255, 255, 255), 0.5 );\n}\n</code></pre>\n",
        "system": ""
    },
    {
        "instruction": "crowd tracking with opencv",
        "input": "",
        "output": "<p>One of my former colleagues implemented something similar (controlling a few motors according to crowd movement) using optical flow. You can analyze the frames of video from a camera, calculate optical flow between frames, and use the values to estimate the crowd movement.</p>\n\n<p>OpenCV has support to perform the above tasks, and comes with good code samples. A desktop should be able to do this in real-time (you might have to tweak with image resolution). </p>\n\n<p>I am not exactly sure how to interface between a C++ program and a sound system. Pure Data (PD) is an alternative, but it might not have much support for motion analysis.</p>\n",
        "system": ""
    },
    {
        "instruction": "How do I get the vertices from an edge image",
        "input": "",
        "output": "<p>Please see my answer here: <a href=\"https://stackoverflow.com/questions/7263621/how-to-find-corners-on-a-image-using-opencv/7263794#7263794\">How to find corners on a Image using OpenCv</a></p>\n\n<p>As for step 7: cvApproxPoly returns a CvSeq*.  <a href=\"http://www.aishack.in/2010/01/an-introduction-to-contours/\" rel=\"nofollow noreferrer\">This link</a> explains it well.  As shown <a href=\"http://physics.nyu.edu/grierlab/manuals/opencv/core_2include_2opencv2_2core_2types__c_8h_source.html\" rel=\"nofollow noreferrer\">here</a>, a CvSeq struct contains a <em>total</em> member that contains the number of elements in the sequence.  In the case of a true quadrilateral, <em>total</em> should equal 4. If the quadrilateral is a square (or rectangle), angles between adjacent vertices should be ~90 degrees.</p>\n",
        "system": ""
    },
    {
        "instruction": "Homography for set of trainimages",
        "input": "",
        "output": "<p>I have done this before and I do not see a better solution than to put most of it in a for loop. You should just take whatever is constant outside of the loop, and try to reuse data instead of recreating everything for every loop.</p>\n",
        "system": ""
    },
    {
        "instruction": "Random tree CvRTrees persistence issue opencv",
        "input": "",
        "output": "<p>It was an error due to file mistake!</p>\n\n<p>So the persistency is working!</p>\n\n<p>But I leave the post as sample if someone needs to implement it!</p>\n",
        "system": ""
    }
]